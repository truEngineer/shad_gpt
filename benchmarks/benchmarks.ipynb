{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d165662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59171d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Model:\n",
    "    model: AutoModelForCausalLM\n",
    "    tokenizer: AutoTokenizer\n",
    "    device: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf2b78",
   "metadata": {},
   "source": [
    "## Загружаем Mistral 7B из HF\n",
    "\n",
    "- Блогпост: https://mistral.ai/news/announcing-mistral-7b/\n",
    "- Модель на HF: https://huggingface.co/mistralai/Mistral-7B-v0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d0eed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928f3425ea9c43eba820519b2d68365e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-18:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-23:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-12:\n",
      "Process ForkProcess-21:\n",
      "Process ForkProcess-31:\n",
      "Process ForkProcess-32:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-27:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-26:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-28:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-16:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-14:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkProcess-15:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "Process ForkProcess-6:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "mistral_device = 'cuda:0'\n",
    "mistral = Model(\n",
    "    model=AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-v0.1\",\n",
    "        device_map=mistral_device,\n",
    "        torch_dtype=\"auto\",\n",
    "        use_flash_attention_2=True,\n",
    "    ), \n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\"),\n",
    "    device=mistral_device\n",
    "    \n",
    ")\n",
    "\n",
    "mistral.tokenizer.pad_token = mistral.tokenizer.eos_token\n",
    "mistral.tokenizer.padding_side = \"left\"\n",
    "mistral.tokenizer.model_max_length = mistral.model.config.max_position_embeddings\n",
    "print('max_length', mistral.model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08fcb4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> In order to prepare pasta, first you need to get your ingredients ready. In addition to the flour and the egg, you will also need some salt, olive oil, and sometimes water. Mix them together to make the dough for pasta. You may add some herbs and vegetables as a variation. Mix them in a bowl or in a dough machine first, and kneed them into shape. You could use your hands or a rolling pin to spread and thin the dough evenly. After you have added the right amount of liquid you will be'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"In order to prepare pasta, first you need to\"\n",
    "\n",
    "model_inputs = mistral.tokenizer([prompt], return_tensors=\"pt\").to(mistral_device)\n",
    "\n",
    "generated_ids = mistral.model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "mistral.tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edd28f",
   "metadata": {},
   "source": [
    "## Загружаем Llama 7B из локального чекпоинта\n",
    "- Модель на HF (нужно запрашивать доступ) https://huggingface.co/meta-llama/Llama-2-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b2de9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a7cf3a26e74f5fb6a786a99a04118d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length 2048\n"
     ]
    }
   ],
   "source": [
    "llama_device = \"cuda:1\"\n",
    "llama = Model(\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"llama-7b-hf-model\",\n",
    "        device_map=llama_device,\n",
    "        torch_dtype=\"auto\",\n",
    "        use_flash_attention_2=True,\n",
    "\n",
    "    ),\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"llama-7b-hf-model\"),\n",
    "    device=llama_device\n",
    ")\n",
    "\n",
    "llama.tokenizer.pad_token = llama.tokenizer.eos_token\n",
    "llama.tokenizer.padding_side = \"left\"\n",
    "llama.tokenizer.model_max_length = llama.model.config.max_position_embeddings\n",
    "print('max_length', llama.model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc392d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> In order to prepare pasta, first you need to wash them well, then bring in water with a lot of salt. That is the key to maintaining its original flavor. First, the pasta must be softened and then fry it for a few minutes over high heat.\\nAfter that, put the pasta into a different bowl. If pasta is cooked too much, then it is likely that the cooking process will not be very successful. If it lacks salt, the pasta will be so hard'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"In order to prepare pasta, first you need to\"\n",
    "\n",
    "model_inputs = llama.tokenizer([prompt], return_tensors=\"pt\").to(llama_device)\n",
    "\n",
    "generated_ids = llama.model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "llama.tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b1e45",
   "metadata": {},
   "source": [
    "### Функция для оценки вероятности продолжения при условии префикса\n",
    "Вычисляем $\\log P\\left(\\texttt{suffix} | \\texttt{prefix}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2514c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "def score_completion(model_obj, prefix, suffix):\n",
    "    model = model_obj.model\n",
    "    tokenizer = model_obj.tokenizer\n",
    "    device = model_obj.device\n",
    "    max_length = tokenizer.model_max_length\n",
    "        \n",
    "    # Encode input sequence, preserving preffix-suffix split\n",
    "    tokenizer.truncation_size = \"left\"\n",
    "    prefix_encoded = tokenizer(\n",
    "        [prefix], \n",
    "        add_special_tokens=True, \n",
    "        max_length=max_length, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    tokenizer.truncation_size = \"right\"\n",
    "    suffix_encoded = tokenizer(\n",
    "        [suffix], \n",
    "        add_special_tokens=False, \n",
    "        max_length=max_length, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    input_ids = torch.concat([prefix_encoded.input_ids, suffix_encoded.input_ids], dim=-1).to(device)\n",
    "    attention_mask = torch.concat([prefix_encoded.attention_mask, suffix_encoded.attention_mask], dim=-1).to(device)\n",
    "    \n",
    "    # Apply model\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # shape: (1, seqlen + 1, vocab_size)\n",
    "    log_probs = torch.log_softmax(outputs.logits, dim=-1)  # shape: (1, seqlen + 1, vocab_size)\n",
    "    \n",
    "    # Collect logprobs of correspondent tokens\n",
    "    # shape: (1, seqlen + 1)\n",
    "    scores = -torch.gather(log_probs, dim=2, index=input_ids[:, 1:, None]).squeeze(-1)\n",
    "    scores = scores[0].detach().cpu().numpy()\n",
    "    \n",
    "    # Ignore <s> in the beginning\n",
    "    prefix_len = prefix_encoded.input_ids.shape[-1] - 1\n",
    "    \n",
    "    # Split logprobs into prefix and suffix\n",
    "    return {\n",
    "        'prefix': scores[:prefix_len],\n",
    "        'suffix': scores[prefix_len:]\n",
    "    }\n",
    "    \n",
    "def toprob(x):\n",
    "    return np.exp(-np.sum(x))\n",
    "\n",
    "def score_suffix(model, prefix, suffix):\n",
    "    scores = score_completion(model, prefix, suffix)\n",
    "    return toprob(scores['prefix']), toprob(scores['suffix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d468d",
   "metadata": {},
   "source": [
    "**Аккуратно с токенизатором** - между префиксом и суффиксом не должно быть пробелов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faf9dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1 [ 4.402521    1.4552702  10.406805    0.5393598   0.4020606   0.43647316] [0.25132608 0.7172515  4.507877  ]\n",
      "s2 [ 4.402521    1.4552702  10.406805    0.5393598   0.4020606   0.43647316\n",
      "  0.25132608  0.7172515 ] [6.789127]\n"
     ]
    }
   ],
   "source": [
    "s1 = score_completion(mistral, \"2 + 2 =\", \"4 \")\n",
    "s2 = score_completion(mistral, \"2 + 2 = 4\", \" \")\n",
    "\n",
    "print('s1', s1['prefix'], s1['suffix'])\n",
    "print('s2', s2['prefix'], s2['suffix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906122ed",
   "metadata": {},
   "source": [
    "**Вероятности предложений совпали** при разном разбиении на префикс и суффикс\n",
    "$$P(S) = P(\\texttt{suff} | \\texttt{pref}) \\cdot P(\\texttt{pref})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32d5d78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6335237e-11, 1.6335254e-11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = score_suffix(mistral, \"2 + 2 =\", \"4 .\")\n",
    "b = score_suffix(mistral, \"2 + 2 = 4\", \".\")\n",
    "\n",
    "a[0] * a[1], b[0] * b[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d704e93",
   "metadata": {},
   "source": [
    "# OpenBook QA\n",
    "- Загружаем скачанный локально датасет\n",
    "- https://allenai.org/data/open-book-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b7ebf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbook_en_jsonlines.json\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls bench_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518a2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bench_data/openbook_en_jsonlines.json', 'r') as fin:\n",
    "    open_book_dataset = [json.loads(x) for x in fin.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a4627c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'options': ['puppies learning new tricks',\n",
       "    'children growing up and getting old',\n",
       "    'flowers wilting in a vase',\n",
       "    'plants sprouting, blooming and wilting'],\n",
       "   'task': 'The sun is responsible for',\n",
       "   'true_answer_id': 3,\n",
       "   'true_answer_text': 'plants sprouting, blooming and wilting'},\n",
       "  {'options': ['the mountains seem very close',\n",
       "    'the mountains are boring',\n",
       "    'the mountains look the same as from up close',\n",
       "    'the mountains seem smaller than in photographs'],\n",
       "   'task': 'When standing miles away from Mount Rushmore',\n",
       "   'true_answer_id': 3,\n",
       "   'true_answer_text': 'the mountains seem smaller than in photographs'}],\n",
       " 5957)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_book_dataset[:2], len(open_book_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4620ee8",
   "metadata": {},
   "source": [
    "Пошафлим датасет, чтобы удобно замеряться на сулчайном подсемлпе для скорости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25f0a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(open_book_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75484291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'options': ['Venus', 'Mars', 'Neptune', 'our Moon'],\n",
       "  'task': 'This white object is visible due to being close to earth',\n",
       "  'true_answer_id': 3,\n",
       "  'true_answer_text': 'our Moon'},\n",
       " {'options': ['yellow', 'gold', 'blue', 'gray'],\n",
       "  'task': 'A sky that is mostly this color will likely be precipitating:',\n",
       "  'true_answer_id': 3,\n",
       "  'true_answer_text': 'gray'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_book_dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc7e35",
   "metadata": {},
   "source": [
    "## Замерим мистраль и ламу в разных режимах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7cc649",
   "metadata": {},
   "source": [
    "### Скоринг без нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adc83e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_on_dataset_simple(model, dataset):\n",
    "    results = []\n",
    "    for row in tqdm(dataset, position=0):\n",
    "        \n",
    "        # Score simple argmax [P(option | task)]\n",
    "        prefix = row['task']\n",
    "        option_scores = np.array([\n",
    "            score_suffix(model, prefix, option)[1]\n",
    "            for option in row['options']\n",
    "        ])\n",
    "        \n",
    "        selected_option = np.argmax(option_scores)\n",
    "        \n",
    "        results.append(selected_option == row['true_answer_id'])\n",
    "\n",
    "    return np.array(results, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccd16dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:03<00:00,  5.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_opbqa_simple_20 = apply_on_dataset_simple(mistral, open_book_dataset[:20])\n",
    "mistral_opbqa_simple_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e61d757b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mistral_opbqa_simple_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62d96e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:33<00:00,  5.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_opbqa_simple_200 = apply_on_dataset_simple(mistral, open_book_dataset[:200])\n",
    "np.mean(mistral_opbqa_simple_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef6617a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:31<00:00,  6.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_opbqa_simple_200 = apply_on_dataset_simple(llama, open_book_dataset[:200])\n",
    "np.mean(llama_opbqa_simple_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545446f1",
   "metadata": {},
   "source": [
    "### Добавим нормализацию на длину"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8870acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_on_dataset_simple_len_norm(model, dataset):\n",
    "    results = []\n",
    "    for row in tqdm(dataset, position=0):\n",
    "        \n",
    "        # Score simple argmax [P(option | task)]\n",
    "        prefix = row['task']\n",
    "        option_scores = np.array([\n",
    "            score_suffix(model, prefix, option)[1]\n",
    "            for option in row['options']\n",
    "        ])\n",
    "\n",
    "        # Add len normalization\n",
    "        option_scores /= np.array([len(r) for r in row['options']])\n",
    "        \n",
    "        selected_option = np.argmax(option_scores)\n",
    "        results.append(selected_option == row['true_answer_id'])\n",
    "\n",
    "    return np.array(results, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a26cc9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:35<00:00,  5.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_opbqa_simple_len_norm_200 = apply_on_dataset_simple_len_norm(mistral, open_book_dataset[:200])\n",
    "np.mean(mistral_opbqa_simple_len_norm_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa1c7eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:31<00:00,  6.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.345"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_opbqa_simple_len_norm_200 = apply_on_dataset_simple_len_norm(llama, open_book_dataset[:200])\n",
    "np.mean(llama_opbqa_simple_len_norm_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbad893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_on_dataset_simple_norm_on_prior(model, dataset, prior_prefix):\n",
    "    results = []\n",
    "    for row in tqdm(dataset, position=0):\n",
    "        \n",
    "        # Score simple argmax [P(option | task)]\n",
    "        prefix = row['task']\n",
    "        option_scores = np.array([\n",
    "            score_suffix(model, prefix, option)[1]\n",
    "            for option in row['options']\n",
    "        ])\n",
    "\n",
    "        assert isinstance(prior_prefix, str)\n",
    "        priors = np.array([\n",
    "            score_suffix(model, prior_prefix, option)[1]\n",
    "            for option in row['options']\n",
    "        ])\n",
    "        option_scores /= priors\n",
    "\n",
    "        selected_option = np.argmax(option_scores)\n",
    "        results.append(selected_option == row['true_answer_id'])\n",
    "\n",
    "    return np.array(results, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4689308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:10<00:00,  2.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_opbqa_simple_norm_on_prior_200 = apply_on_dataset_simple_norm_on_prior(\n",
    "    mistral, open_book_dataset[:200], \n",
    "    prior_prefix=\"Answer:\"\n",
    ")\n",
    "np.mean(mistral_opbqa_simple_norm_on_prior_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe633b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cache/mistral_opbqa_simple_norm_on_prior_200.npy', mistral_opbqa_simple_norm_on_prior_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30feb1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:02<00:00,  3.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.485"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_opbqa_simple_norm_on_prior_200 = apply_on_dataset_simple_norm_on_prior(\n",
    "    llama, open_book_dataset[:200], \n",
    "    prior_prefix=\"Answer:\"\n",
    ")\n",
    "np.mean(llama_opbqa_simple_norm_on_prior_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f01cc702",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cache/llama_opbqa_simple_norm_on_prior_200.npy', llama_opbqa_simple_norm_on_prior_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63cbfdf",
   "metadata": {},
   "source": [
    "### Превратим задачу в MCQ (multiple choice question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6388e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mcq(row):\n",
    "    options = row['options']\n",
    "    task = row['task']\n",
    "    prompt = \"\\n\".join([f\"{i + 1}. {o}\" for i, o in enumerate(options)])\n",
    "    return {\n",
    "        'task': task + '\\n' + prompt + '\\nAnswer:',\n",
    "        'options': [f'{i + 1}' for i in range(len(options))],\n",
    "        'true_answer_id': row['true_answer_id']\n",
    "    }\n",
    "\n",
    "def apply_on_dataset_with_fromat_fn(model, dataset, normalization=None, format_fn=None):\n",
    "    results = []\n",
    "    if format_fn is not None:\n",
    "        dataset = [format_fn(x) for x in dataset]\n",
    "        \n",
    "    for row in tqdm(dataset, position=0):\n",
    "        prefix = row['task']\n",
    "        option_scores = np.array([\n",
    "            score_suffix(model, prefix, option)[1]\n",
    "            for option in row['options']\n",
    "        ])            \n",
    "        results.append(np.argmax(option_scores) == row['true_answer_id'])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "baaa327c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'This white object is visible due to being close to earth\\n1. Venus\\n2. Mars\\n3. Neptune\\n4. our Moon\\nAnswer:',\n",
       " 'options': ['1', '2', '3', '4'],\n",
       " 'true_answer_id': 3}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_mcq(open_book_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0792922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:35<00:00,  5.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.585"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_opbqa_mcq_200 = apply_on_dataset_with_fromat_fn(\n",
    "    mistral, open_book_dataset[:200], \n",
    "    format_fn=make_mcq\n",
    ")\n",
    "np.mean(mistral_opbqa_mcq_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f69d48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:32<00:00,  6.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_opbqa_mcq_200 = apply_on_dataset_with_fromat_fn(\n",
    "    llama, open_book_dataset[:200], \n",
    "    format_fn=make_mcq\n",
    ")\n",
    "np.mean(llama_opbqa_mcq_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc74cb",
   "metadata": {},
   "source": [
    "### Соберем few-shot подводку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f8bb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water\\n1. scalds\\n2. cools\\n3. toasts\\n4. freezes\\nAnswer: 1\\n\\nPasta may be cooked in water when\\n1. the water is warm\\n2. the water is on the stove\\n3. water is bubbling from applied warmth\\n4. the pasta is very fresh\\nAnswer: 3\\n\\nA decrease in diseases\\n1. has no impact on a population\\n2. leads to more sick people\\n3. leads to less sick people\\n4. leads to an uptick in emergency room visits\\nAnswer: 3\\n\\nWhen soil is viewed in a scientific way, what is seen and viewed is actually\\n1. insects like big beetles\\n2. tiny lifeforms in dirt\\n3. small mammals living there\\n4. a lot of tiny pebbles\\nAnswer: 2\\n\\nSome animals use a liquid coming from their skin to adjust to\\n1. cold\\n2. water\\n3. heat\\n4. humidity\\nAnswer: 3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshot_rows = [make_mcq(r) for r in open_book_dataset[-5:]]\n",
    "fewshot_prompt = \"\\n\\n\".join([f\"{r['task']} {r['options'][r['true_answer_id']]}\" for r in fewshot_rows])\n",
    "fewshot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15da9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_on_dataset_with_fromat_fn_and_prompt(model, dataset, normalization=None, format_fn=None, prompt=None):\n",
    "    results = []\n",
    "    if format_fn is not None:\n",
    "        dataset = [format_fn(x) for x in dataset]\n",
    "        \n",
    "    for row in tqdm(dataset, position=0):\n",
    "        prefix = row['task']\n",
    "        if prompt:\n",
    "            prefix = prompt + \"\\n\\n\" + prefix\n",
    "            \n",
    "        option_scores = np.array([\n",
    "            score_suffix(model, prefix, option)[1]\n",
    "            for option in row['options']\n",
    "        ])            \n",
    "        results.append(np.argmax(option_scores) == row['true_answer_id'])\n",
    "        \n",
    "    return np.array(results, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9aaca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:47<00:00,  4.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_opbqa_mcq_fewshot_200 = apply_on_dataset_with_fromat_fn_and_prompt(\n",
    "    mistral, open_book_dataset[:200], \n",
    "    format_fn=make_mcq,\n",
    "    prompt=fewshot_prompt\n",
    ")\n",
    "np.mean(mistral_opbqa_mcq_fewshot_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7732652",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cache/mistral_opbqa_mcq_fewshot_200.npy', mistral_opbqa_mcq_fewshot_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6ae56c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:34<00:00,  5.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_opbqa_mcq_fewshot_200 = apply_on_dataset_with_fromat_fn_and_prompt(\n",
    "    llama, open_book_dataset[:200], \n",
    "    format_fn=make_mcq,\n",
    "    prompt=fewshot_prompt\n",
    ")\n",
    "np.mean(llama_opbqa_mcq_fewshot_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80c60bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cache/llama_opbqa_mcq_fewshot_200.npy', llama_opbqa_mcq_fewshot_200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a246f1a",
   "metadata": {},
   "source": [
    "## Статзначимость\n",
    "- Проверим бутстрап и альтернативные способы оценки статзначимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ef872cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_test(baseline, test, n_samples=500000, paired=True):\n",
    "    baseline = np.array(baseline, dtype=np.float32)\n",
    "    test = np.array(test, dtype=np.float32)\n",
    "    assert len(baseline) == len(test)\n",
    "    indices = np.random.randint(0, len(test), (n_samples, len(test)))\n",
    "    if paired:\n",
    "        indices2 = indices\n",
    "    else:\n",
    "        indices2 = np.random.randint(0, len(test), (n_samples, len(test)))\n",
    "    \n",
    "    test_sampled_scores = np.mean(test[indices], axis=-1)\n",
    "    baseline_sampled_scores = np.mean(baseline[indices2], axis=-1)\n",
    "\n",
    "    diff = test_sampled_scores - baseline_sampled_scores\n",
    "    \n",
    "    same_cnt = np.sum(diff == 0)\n",
    "    test_better_cnt = np.sum(diff > 0)\n",
    "    test_worse_cnt = np.sum(diff < 0)\n",
    "    pval_left = 1.0 - test_better_cnt / n_samples\n",
    "#     return pval_left\n",
    "    \n",
    "    pval_right = 1.0 - test_worse_cnt / n_samples\n",
    "    pval_twosided = min(min(pval_left, pval_right) * 2.0, 1.0)\n",
    "    return pval_twosided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09cba54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mistral_opbqa_mcq_fewshot_200), len(llama_opbqa_mcq_fewshot_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31b17d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama 0.76\n",
      "mistral 0.51\n",
      "pvalue 0.0\n"
     ]
    }
   ],
   "source": [
    "print('llama', np.mean(mistral_opbqa_mcq_fewshot_200))\n",
    "print('mistral', np.mean(llama_opbqa_mcq_fewshot_200))\n",
    "print('pvalue', bootstrap_test(baseline=mistral_opbqa_mcq_fewshot_200, test=llama_opbqa_mcq_fewshot_200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "badba940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, ttest_ind, wilcoxon, ttest_rel\n",
    "def all_tests(A, B):\n",
    "    tests = [\n",
    "        (mannwhitneyu, \"mannwhitneyu\"), \n",
    "        (ttest_ind, \"ttest_ind\"), \n",
    "        (wilcoxon, \"wilcoxon\"), \n",
    "        (ttest_rel, \"ttest_rel\")\n",
    "    ]\n",
    "    for fn, name in tests:\n",
    "        print(f\"{name}: {fn(A, B).pvalue}\")\n",
    "    print(f\"bootstrap: {bootstrap_test(A, B, paired=True)}\")\n",
    "    print(f\"bootstrap-unpaired: {bootstrap_test(A, B, paired=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a44503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mannwhitneyu: 2.1503481599517891e-07\n",
      "ttest_ind: 1.3851098841591184e-07\n",
      "wilcoxon: 6.1590119706630185e-09\n",
      "ttest_rel: 1.3570433906500782e-09\n",
      "bootstrap: 0.0\n",
      "bootstrap-unpaired: 0.0\n"
     ]
    }
   ],
   "source": [
    "all_tests(mistral_opbqa_mcq_fewshot_200, llama_opbqa_mcq_fewshot_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f492ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mannwhitneyu: 0.4847809081775155\n",
      "ttest_ind: 0.4851576177708934\n",
      "wilcoxon: 0.32698934959801507\n",
      "ttest_rel: 0.32822703308821277\n",
      "bootstrap: 0.35995599999999994\n",
      "bootstrap-unpaired: 0.517272\n"
     ]
    }
   ],
   "source": [
    "all_tests(llama_opbqa_simple_norm_on_prior_200, mistral_opbqa_simple_norm_on_prior_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31868a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mannwhitneyu: 0.3783468435580267\n",
      "ttest_ind: 0.4851384255369875\n",
      "wilcoxon: 0.21781019839200122\n",
      "ttest_rel: 0.32820996272670244\n",
      "bootstrap: 0.3153760000000001\n",
      "bootstrap-unpaired: 0.47297199999999995\n"
     ]
    }
   ],
   "source": [
    "add_1 = (np.random.rand(len(llama_opbqa_simple_norm_on_prior_200)) - 0.5) * 0.0001\n",
    "add_2 = (np.random.rand(len(llama_opbqa_simple_norm_on_prior_200)) - 0.5) * 0.0001\n",
    "\n",
    "all_tests(\n",
    "    llama_opbqa_simple_norm_on_prior_200 + add_1,\n",
    "    mistral_opbqa_simple_norm_on_prior_200 + add_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a586930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mannwhitneyu: 3.330958373896012e-05\n",
      "ttest_ind: 1.3853408041564848e-07\n",
      "wilcoxon: 4.945252167103879e-06\n",
      "ttest_rel: 1.3572904826876695e-09\n",
      "bootstrap: 0.0\n",
      "bootstrap-unpaired: 0.0\n"
     ]
    }
   ],
   "source": [
    "all_tests(mistral_opbqa_mcq_fewshot_200 + add_1, llama_opbqa_mcq_fewshot_200 + add_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce2ce0",
   "metadata": {},
   "source": [
    "### Замерим непрокрасившийся сетап на семпле побольше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac8fe38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [12:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5655"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_opbqa_simple_norm_on_prior_2000 = apply_on_dataset_simple_norm_on_prior(\n",
    "    mistral, open_book_dataset[:2000], \n",
    "    prior_prefix=\"Answer:\"\n",
    ")\n",
    "np.save('cache/mistral_opbqa_simple_norm_on_prior_2000.npy', mistral_opbqa_simple_norm_on_prior_2000)\n",
    "np.mean(mistral_opbqa_simple_norm_on_prior_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0dcb2a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [10:52<00:00,  3.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5225"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_opbqa_simple_norm_on_prior_2000 = apply_on_dataset_simple_norm_on_prior(\n",
    "    llama, open_book_dataset[:2000], \n",
    "    prior_prefix=\"Answer:\"\n",
    ")\n",
    "np.save('cache/mistral_opbqa_simple_norm_on_prior_2000.npy', llama_opbqa_simple_norm_on_prior_2000)\n",
    "np.mean(llama_opbqa_simple_norm_on_prior_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b7e47f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral_opbqa_simple_norm_on_prior_2000 0.5655\n",
      "llama_opbqa_simple_norm_on_prior_2000 0.5225\n",
      "-------------\n",
      "mannwhitneyu: 0.006337404430601847\n",
      "ttest_ind: 0.006322441753302127\n",
      "wilcoxon: 1.880218804695599e-05\n",
      "ttest_rel: 1.8119041325330304e-05\n",
      "bootstrap: 1.6000000000016e-05\n",
      "bootstrap-unpaired: 0.006783999999999901\n"
     ]
    }
   ],
   "source": [
    "print('mistral_opbqa_simple_norm_on_prior_2000', np.mean(mistral_opbqa_simple_norm_on_prior_2000))\n",
    "print('llama_opbqa_simple_norm_on_prior_2000', np.mean(llama_opbqa_simple_norm_on_prior_2000))\n",
    "print('-------------')\n",
    "\n",
    "all_tests(mistral_opbqa_simple_norm_on_prior_2000, llama_opbqa_simple_norm_on_prior_2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f639a8d",
   "metadata": {},
   "source": [
    "## Посмотрим на распределение скоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "33f1d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(task, options, option_scores, true_answer_id):\n",
    "    scored_options_str = \"\\n\".join([f\"{s} - {o}{' [TRUE]' if i == true_answer_id else ''}\" for i, (o, s) in enumerate(zip(options, option_scores))])\n",
    "    print(f\"{task}\\nOptions:\\n{scored_options_str}\")\n",
    "    \n",
    "def apply_on_dataset_with_all_tools(model, dataset, normalization=None, format_fn=None, prompt=None):\n",
    "    results = []\n",
    "    if format_fn is not None:\n",
    "        dataset = [format_fn(x) for x in dataset]\n",
    "        \n",
    "    for i, row in tqdm(enumerate(dataset), position=0):\n",
    "        prefix = row['task']\n",
    "\n",
    "        if prompt:\n",
    "            prefix = prompt + \"\\n\\n\" + prefix\n",
    "            \n",
    "        option_scores = np.array([\n",
    "            score_suffix(model, prefix, option)[1]\n",
    "            for option in row['options']\n",
    "        ])  \n",
    "\n",
    "        if normalization == 'length':\n",
    "            option_scores /= np.array([len(r) for r in row['options']])\n",
    "        elif normalization is not None:\n",
    "            prior_prefix = normalization\n",
    "            priors = np.array([\n",
    "                score_suffix(model, prior_prefix, option)[1]\n",
    "                for option in row['options']\n",
    "            ])\n",
    "            option_scores /= priors\n",
    "        \n",
    "#         if i in (0, 10, 20):\n",
    "        visualize(row['task'], row['options'], option_scores, row['true_answer_id'])\n",
    "        \n",
    "        results.append(np.argmax(option_scores) == row['true_answer_id'])\n",
    "        \n",
    "    return np.array(results, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb56d8d",
   "metadata": {},
   "source": [
    "### MCQ + Fewshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f29dc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for\n",
      "1. puppies learning new tricks\n",
      "2. children growing up and getting old\n",
      "3. flowers wilting in a vase\n",
      "4. plants sprouting, blooming and wilting\n",
      "Answer:\n",
      "Options:\n",
      "0.1635913848876953 - 1\n",
      "0.14436888694763184 - 2\n",
      "0.18537330627441406 - 3\n",
      "0.503896951675415 - 4 [TRUE]\n",
      "When standing miles away from Mount Rushmore\n",
      "1. the mountains seem very close\n",
      "2. the mountains are boring\n",
      "3. the mountains look the same as from up close\n",
      "4. the mountains seem smaller than in photographs\n",
      "Answer:\n",
      "Options:\n",
      "0.3910847008228302 - 1\n",
      "0.18473531305789948 - 2\n",
      "0.18473531305789948 - 3\n",
      "0.23720481991767883 - 4 [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When food is reduced in the stomach\n",
      "1. the mind needs time to digest\n",
      "2. take a second to digest what I said\n",
      "3. nutrients are being deconstructed\n",
      "4. reader's digest is a body of works\n",
      "Answer:\n",
      "Options:\n",
      "0.18901626765727997 - 1\n",
      "0.14720602333545685 - 2\n",
      "0.5822111368179321 - 3 [TRUE]\n",
      "0.07879370450973511 - 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_on_dataset_with_all_tools(\n",
    "    llama, open_book_dataset[:3], \n",
    "    format_fn=make_mcq,\n",
    "    prompt=fewshot_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e1c068a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for\n",
      "1. puppies learning new tricks\n",
      "2. children growing up and getting old\n",
      "3. flowers wilting in a vase\n",
      "4. plants sprouting, blooming and wilting\n",
      "Answer:\n",
      "Options:\n",
      "0.014951090328395367 - 1\n",
      "0.03165145590901375 - 2\n",
      "0.024650176987051964 - 3\n",
      "0.9249911904335022 - 4 [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When standing miles away from Mount Rushmore\n",
      "1. the mountains seem very close\n",
      "2. the mountains are boring\n",
      "3. the mountains look the same as from up close\n",
      "4. the mountains seem smaller than in photographs\n",
      "Answer:\n",
      "Options:\n",
      "0.20462031662464142 - 1\n",
      "0.15935847163200378 - 2\n",
      "0.07527562230825424 - 3\n",
      "0.5562157034873962 - 4 [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When food is reduced in the stomach\n",
      "1. the mind needs time to digest\n",
      "2. take a second to digest what I said\n",
      "3. nutrients are being deconstructed\n",
      "4. reader's digest is a body of works\n",
      "Answer:\n",
      "Options:\n",
      "0.011937608011066914 - 1\n",
      "0.028636841103434563 - 2\n",
      "0.9483218789100647 - 3 [TRUE]\n",
      "0.007240525912493467 - 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_on_dataset_with_all_tools(\n",
    "    mistral, open_book_dataset[:3], \n",
    "    format_fn=make_mcq,\n",
    "    prompt=fewshot_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4fc02",
   "metadata": {},
   "source": [
    "### MCQ + 0-Shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c23b063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for\n",
      "1. puppies learning new tricks\n",
      "2. children growing up and getting old\n",
      "3. flowers wilting in a vase\n",
      "4. plants sprouting, blooming and wilting\n",
      "Answer:\n",
      "Options:\n",
      "0.11462289839982986 - 1\n",
      "0.042167410254478455 - 2\n",
      "0.042167410254478455 - 3\n",
      "0.05414402484893799 - 4 [TRUE]\n",
      "When standing miles away from Mount Rushmore\n",
      "1. the mountains seem very close\n",
      "2. the mountains are boring\n",
      "3. the mountains look the same as from up close\n",
      "4. the mountains seem smaller than in photographs\n",
      "Answer:\n",
      "Options:\n",
      "0.14511708915233612 - 1\n",
      "0.12806537747383118 - 2\n",
      "0.12806537747383118 - 3\n",
      "0.12806537747383118 - 4 [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When food is reduced in the stomach\n",
      "1. the mind needs time to digest\n",
      "2. take a second to digest what I said\n",
      "3. nutrients are being deconstructed\n",
      "4. reader's digest is a body of works\n",
      "Answer:\n",
      "Options:\n",
      "0.1293146163225174 - 1\n",
      "0.07368124276399612 - 2\n",
      "0.07843327522277832 - 3 [TRUE]\n",
      "0.1293146163225174 - 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_on_dataset_with_all_tools(\n",
    "    llama, open_book_dataset[:3], \n",
    "    format_fn=make_mcq,\n",
    "    prompt=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6305decc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for\n",
      "1. puppies learning new tricks\n",
      "2. children growing up and getting old\n",
      "3. flowers wilting in a vase\n",
      "4. plants sprouting, blooming and wilting\n",
      "Answer:\n",
      "Options:\n",
      "0.04783881828188896 - 1\n",
      "0.054208479821681976 - 2\n",
      "0.061426255851984024 - 3\n",
      "0.07887287437915802 - 4 [TRUE]\n",
      "When standing miles away from Mount Rushmore\n",
      "1. the mountains seem very close\n",
      "2. the mountains are boring\n",
      "3. the mountains look the same as from up close\n",
      "4. the mountains seem smaller than in photographs\n",
      "Answer:\n",
      "Options:\n",
      "0.06996961683034897 - 1\n",
      "0.048089370131492615 - 2\n",
      "0.05119086056947708 - 3\n",
      "0.0744822695851326 - 4 [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When food is reduced in the stomach\n",
      "1. the mind needs time to digest\n",
      "2. take a second to digest what I said\n",
      "3. nutrients are being deconstructed\n",
      "4. reader's digest is a body of works\n",
      "Answer:\n",
      "Options:\n",
      "0.2256922572851181 - 1\n",
      "0.10660947114229202 - 2\n",
      "0.06074425205588341 - 3 [TRUE]\n",
      "0.0325140617787838 - 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_on_dataset_with_all_tools(\n",
    "    mistral, open_book_dataset[:3], \n",
    "    format_fn=make_mcq,\n",
    "    prompt=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03970a",
   "metadata": {},
   "source": [
    "### Скоринг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2c9d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for\n",
      "Options:\n",
      "5.724991336032714e-14 - puppies learning new tricks\n",
      "1.0741314088180687e-11 - children growing up and getting old\n",
      "6.48289364899024e-12 - flowers wilting in a vase\n",
      "2.2158153201276738e-14 - plants sprouting, blooming and wilting [TRUE]\n",
      "When standing miles away from Mount Rushmore\n",
      "Options:\n",
      "1.2116071701484543e-08 - the mountains seem very close\n",
      "6.121588791430099e-10 - the mountains are boring\n",
      "1.011798934566488e-12 - the mountains look the same as from up close\n",
      "2.0233129408020467e-12 - the mountains seem smaller than in photographs [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When food is reduced in the stomach\n",
      "Options:\n",
      "5.628290368486777e-12 - the mind needs time to digest\n",
      "7.758056158474412e-17 - take a second to digest what I said\n",
      "6.080840692090916e-12 - nutrients are being deconstructed [TRUE]\n",
      "6.066636316517671e-20 - reader's digest is a body of works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_on_dataset_with_all_tools(\n",
    "    llama, open_book_dataset[:3], \n",
    "    format_fn=None,\n",
    "    prompt=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dcc72511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for\n",
      "Options:\n",
      "5.811069941213942e-13 - puppies learning new tricks\n",
      "1.394638744767196e-12 - children growing up and getting old\n",
      "2.326297574711811e-12 - flowers wilting in a vase\n",
      "7.426364211082781e-13 - plants sprouting, blooming and wilting [TRUE]\n",
      "When standing miles away from Mount Rushmore\n",
      "Options:\n",
      "1.0746776801795477e-09 - the mountains seem very close\n",
      "4.674062178966487e-11 - the mountains are boring\n",
      "4.422852871704269e-13 - the mountains look the same as from up close\n",
      "2.4759210306979362e-11 - the mountains seem smaller than in photographs [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When food is reduced in the stomach\n",
      "Options:\n",
      "4.944046907984223e-12 - the mind needs time to digest\n",
      "3.686249749732844e-18 - take a second to digest what I said\n",
      "8.703655743173833e-13 - nutrients are being deconstructed [TRUE]\n",
      "6.309999773249721e-24 - reader's digest is a body of works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_on_dataset_with_all_tools(\n",
    "    mistral, open_book_dataset[:3], \n",
    "    format_fn=None,\n",
    "    prompt=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5d304",
   "metadata": {},
   "source": [
    "### Скоринг с нормализацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9be28ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for\n",
      "Options:\n",
      "0.4955384135246277 - puppies learning new tricks\n",
      "118.44405364990234 - children growing up and getting old\n",
      "20.253536224365234 - flowers wilting in a vase\n",
      "11538.96875 - plants sprouting, blooming and wilting [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When standing miles away from Mount Rushmore\n",
      "Options:\n",
      "71569.8203125 - the mountains seem very close\n",
      "132.78067016601562 - the mountains are boring\n",
      "627455.625 - the mountains look the same as from up close\n",
      "422023.375 - the mountains seem smaller than in photographs [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When food is reduced in the stomach\n",
      "Options:\n",
      "3.0362017154693604 - the mind needs time to digest\n",
      "0.004176686517894268 - take a second to digest what I said\n",
      "4901.15771484375 - nutrients are being deconstructed [TRUE]\n",
      "0.007643749471753836 - reader's digest is a body of works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_on_dataset_with_all_tools(\n",
    "    llama, open_book_dataset[:3], \n",
    "    format_fn=None,\n",
    "    prompt=None,\n",
    "    normalization='Answer:'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "024a2ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is responsible for\n",
      "Options:\n",
      "19.41446876525879 - puppies learning new tricks\n",
      "53.42073440551758 - children growing up and getting old\n",
      "7.065333843231201 - flowers wilting in a vase\n",
      "255618.84375 - plants sprouting, blooming and wilting [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When standing miles away from Mount Rushmore\n",
      "Options:\n",
      "2011.4620361328125 - the mountains seem very close\n",
      "16.4169921875 - the mountains are boring\n",
      "130829.78125 - the mountains look the same as from up close\n",
      "731890.0 - the mountains seem smaller than in photographs [TRUE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When food is reduced in the stomach\n",
      "Options:\n",
      "4.504613399505615 - the mind needs time to digest\n",
      "0.0017585433088243008 - take a second to digest what I said\n",
      "9947.365234375 - nutrients are being deconstructed [TRUE]\n",
      "4.1989492274296936e-06 - reader's digest is a body of works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_on_dataset_with_all_tools(\n",
    "    mistral, open_book_dataset[:3], \n",
    "    format_fn=None,\n",
    "    prompt=None,\n",
    "    normalization='Answer:'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
