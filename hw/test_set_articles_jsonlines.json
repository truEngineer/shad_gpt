{"Host":"https://habr.com","Path":"/en/company/cvetmir3d/blog/373323/?mobile=no","Text":"Интервью с главой ЦМИТ Иваном Мошкиным / Habr            \n\n03-04-2017\nНа днях мы побывали в гостях у наших друзей и партнёров в Центре молодежного инновационного творчества «LAB3DPrint», это один из старейших ЦМИТов в России, который представляет собой площадку для обучения взрослых и детей цифровому производству, 3D-моделированию, конструированию. Помимо интересного и полезного времяпрепровождения, учащиеся получают уникальный опыт и навыки в таких инновационных направлениях, как 3D печать, 3D моделирование, робототехника, 2D черчение и лазерной резке. Центр прекрасно оснащен современным оборудованием для производства 3D-прототипов, лазерной резки, гравировки, вакуумного литья и многого другого. В конце прошлого года, наша компания начала поставки 3D-принтеров Raise3D в Россию, подписав эксклюзивный контракт с этим производителем и один из первых 3D-принтеров Raise3D N2 Plus был установлен в Лаборатории трёхмерной печати. Мы обсудили с Иваном Мошкиным, создателем и руководителем Лаборатории трёхмерной печати, чем сейчас живет компания, каких успехов удалось достичь, ну и, конечно его впечатлениях о Raise3D. Иван, нас, как официальных дистрибьюторов Raise3D, в первую очередь, интересуют ваши впечатления о модели N2 Plus Dual. Расскажите, почему вы выбрали именно эту модель? Мы тщательно подходим к выбору оборудования для своих нужд, и еще в конце прошлого года у нас появилась потребность в FDM 3D-принтере с большой камерой печати, такой принтер позволил бы нам печатать изделия больших размеров целиком, а не частями. Мы выбирали из нескольких моделей, а в Raise3D N2 Plus Dual привлекло несколько ключевых моментов: Два экструдера, которые очень нужны, при печати моделей с растворимыми поддержками Большая областью построения 305*305*610 мм и при этом закрытый корпус Сменные сопла большого диаметра 0,8 мм., что ускоряет печать больших моделей Конструкция экструдера, которая дает возможность печатать различными материалами Ну и наконец, не маловажным фактором стала цена на принтер, которая примерно в два раза ниже аналогичных моделей из США и Европы. Принтер находится у вас в эксплуатации примерно 1,5 месяца, оправдал ли он ваши ожидания? Сразу могу выделить несколько неоспоримых преимуществ принтера. Наличие собственной аккумуляторной батареи позволяет возобновить печать в случае проблем с электричеством, когда модель печатается сутками – это очень полезная функция. При отключении питания принтер запомнит место остановки и продолжит печать именно с него. Принтер очень тихо работает, это оказалось очень важным преимуществом, с учетом того, что на нём постоянно что-то печатается. Еще один плюс — возможность изменения ряда параметров печати уже во время печати. Также недавно нас порадовало обновленное ПО iDeaMaker, в котором появился русский язык. Возможно успели выявить какие-то недостатки? Очень не хватает датчика окончания подачи пластика. Также мы считаем, что колесики не самое удачное решение, хотелось бы иметь альтернативу в виде ножек, у нас принтер постоянно стоит на одном месте, мы бы предпочли стандартную подставку под принтер. По поводу окончания подачи пластика – скоро появится такая опция. Замечательно. Мы обязательно приобретем это обновление. Для каких целей вы используете Raise3D N2 Plus Dual? В основном для прототипирования крупных моделей и объектов пром.дизайна. К примеру, вес этой детали больше килограмма, её Raise3D напечатал за 44 часа, с учетом размера и сложности вполне неплохой результат. Также мы используем принтер для изготовления протезов, ортезов, черепов для медицинских учреждений. Если говорить о материалах для печати то мы уже использовали PLA, ABS, HIPS, а также прозрачный пластик SBS. Иван, расскажите об основных направлениях деятельности вашего ЦМИТа? В данный момент можно выделить 3 основных направления: оказание услуг по прототипированию, 3D-печати, 3D-сканированию; лазерная резка; обучение взрослых и детей работе на цифровом оборудовании: 3D-принтерах, сканерах, лазерных станках. На сегодняшний день, в нашем центре проходят следующие курсы занятий: 3D-прототипирование для взрослых, основы работы на 3D-принтере, моделирование в Fusion 360, основы работы с нестандартными материалами и постобработки, лазерная резка и гравировка, инженерный дизайн CAD для взрослых. Кстати наши ученики на днях завоевали первое место по лазерным технологиям на московском чемпионате Junior Skills. От всей души поздравляем вас и ваших учеником с упехом и желаем дальнейших свершений и побед, надеемся, что оборудование Raise3D будет вам в этом помогать. Команда компании Цветной Мир По традиции минутка рекламы ) Покупая оборудование в нашей компании, вы получаете 10 преимуществ: 1. Возможность воспользоваться программой Trade In. 2. Гарантия — 12 месяцев 3. Инструкция на русском языке 4. Техническая поддержка в течении всего срока эксплуатации 5. Вы покупаете 3D-принтер у официального дистрибьютора в России 6. Бесплатная доставка. 7. Бесплатное обучение в нашем офисе. 8. Возможность купить принтер в кредит через банки ТКС, ОТП, Ренессанс. 9. 10% скидка на пластик навсегда. 10. Возможность ознакомиться с принтером в нашем демо-зале. Подписывайтесь на наши группы в Facebook, VK и YouTube, чтобы быть в курсе последних событий.\n","id":1}
{"Host":"https://habr.com","Path":"/en/company/vk/blog/475684/","Text":"Подборка @pythonetc, октябрь 2019 / Habr                         \n\n13-11-2019\nНовая подборка советов про Python и программирование из моего авторского канала @pythonetc. ← Предыдущие подборки Если хотите итерировать сразу несколько итерируемых объектов, то можете использовать функцию zip (не имеет никакого отношения к файловому формату ZIP): from datetime import timedelta names = [ 'Eleven. Return and Revert', 'Wilderness', 'The Menagerie Inside', 'Evaporate', ] years = [ 2010, 2013, 2015, 2018, ] durations = [ timedelta(minutes=57, seconds=38), timedelta(minutes=48, seconds=5), timedelta(minutes=46, seconds=34), timedelta(minutes=43, seconds=25), ] print('Midas Fall LPs:') for name, year, duration in zip( names, years, durations ): print(f' * {name} ({year}) — {duration}') Результат: Midas Fall LPs: * Eleven. Return and Revert (2010) — 0:57:38 * Wilderness (2013) — 0:48:05 * The Menagerie Inside (2015) — 0:46:34 * Evaporate (2018) — 0:43:25 Генератор можно остановить явным вызовом g.close(), но чаще всего сборщик мусора делает это за вас. После вызова close, в точке, где генерирующая функция была поставлена на паузу, инициируется GeneratorExit: def gen(): try: yield 1 yield 2 finally: print('END') g = gen() print(next(g)) # prints '1' g.close() # prints 'END' Не забывайте о трёх аспектах. Во-первых, вы не можете продолжать генерировать значения при обработке GeneratorExit: def gen(): try: yield 1 finally: yield 3 g = gen() next(g) g.close() # RuntimeError Во-вторых, если генератор ещё не запущен, то исключение не будет брошено, но генератор все равно перейдет в состояние «остановлен»: def gen(): try: yield 1 finally: print('END') g = gen() g.close() # nothing print(list(g)) # prints '[]' В-третьих, close ничего не делает, если генератор уже закончил работу: def gen(): try: yield 1 yield 2 finally: print('END') g = gen() print(list(g)) print('Closing now') g.close() # END # [1, 2] # Closing now f-строки позволяют задавать ширину выводимого значения, а также другие форматирующие спецификаторы: >>> x = 42 >>> f'{x:5}+{x:15f}' ' 42+ 42.000000' Ещё они могут содержать вычисленные выражения, что бывает полезно, когда ширина не известна заранее: def print_table(matrix): cols_width = [ max(len(str(row[col])) for row in matrix) for col in range(len(matrix[0])) ] for row in matrix: for i, cell in enumerate(row): print( f'{cell:{cols_width[i]}} ', end='' ) print() albums = [ ['Eleven. Return and Revert', 2010], ['Wilderness', 2013], ['The Menagerie Inside', 2015], ['Evaporate', 2018], ] print_table(albums) Результат: Eleven. Return and Revert 2010 Wilderness 2013 The Menagerie Inside 2015 Evaporate 2018 Если ваш класс является производным от другого, то метакласс вашего класса тоже должен быть производным от метакласса того класса: from collections import UserDict from abc import ABCMeta # ABCMeta is a metaclass of UserDict class MyDictMeta(ABCMeta): def __new__(cls, name, bases, dct): return super().__new__(cls, name, bases, dct) class MyDict(UserDict, metaclass=MyDictMeta): pass Может быть целесообразно автоматически получать метакласс этого другого класса: def create_my_dict_class(parents): class MyDictMeta(*[type(c) for c in parents]): def __new__(cls, name, bases, dct): return super().__new__(cls, name, bases, dct) class MyDict(*parents, metaclass=MyDictMeta): pass MyDict = create_my_dict_class((UserDict,)) __init__ позволяет модифицировать объект сразу после его создания. Если хотите контролировать созданное, то используйте __new__: from typing import Tuple, Dict from cached_property import cached_property class Numbers: _LOADED: Dict[Tuple[int, ...], 'Numbers'] = {} def __new__(cls, ints: Tuple[int, ...]): if ints not in cls._LOADED: obj = super().__new__(cls) cls._LOADED[ints] = obj return cls._LOADED[ints] def __init__(self, ints: Tuple[int, ...]): self._ints = ints @cached_property def biggest(self): print('calculating...') return max(self._ints) print(Numbers((4, 3, 5)).biggest) print(Numbers((4, 3, 5)).biggest) print(Numbers((4, 3, 6)).biggest)\n","id":2}
{"Host":"https://habr.com","Path":"/ru/post/4174/?mobile=yes","Text":"Владислав Улендеев: «Мы создаём отдельный холдинг, в котором будет два владельца: eHouse и Rambler» / Хабр                \n\n19-07-2006\nВ интервью «Хабрахабру» Владислав Улендеев, исполнительный директор холдинга eHouse, раскрывает некоторые детали сделки с Rambler Media о продаже 51% акций социальной сети Damochka.Ru и баннерообменной системы BannerBank. Кто был инициатором сделки? После пресс-релиза Rambler Media я прочитал мнения участников рынка. В частности, о том, что мы делали предложения о покупке «Дамочки» и «БаннерБанка» всем крупным участникам рынка. Это неправда. Мы никому не обращались с таким предложением. Все потенциальные покупатели этих проектов приходили к нам сами. Да, мы вели переговоры с несколькими компаниями о продаже этих проектов. В основном, с инвесторами, но не игроками рынка. Какова сумма сделки? По мнению участников рынка, она может составить до $1 млн. Участники рынка ошибаются в разы. Вы избавляетесь от непрофильных активов? Мы не избавляемся от активов. Мы занимаемся их развитием. Наверно, вы просто неправильно поняли смысл сделки. Мы продали половину компании. Мы создаём отдельный холдинг, в котором будет два владельца: eHouse и Rambler. Каждый акционер внесёт свои возможности и знания в новую компанию. Вы считаете, это «избавление» от проектов? Теперь не считаю, если речь о совместном холдинге… Но раз уж вы о нем сказали, поясните, пожалуйста, какой это будет холдинг, какие еще компании в него войдут, и кто будет им управлять? На первом этапе в холдинг войдут проекты «Дамочка» и «БаннерБанк». Но уже в ближайшее время мы планируем присоединение новых компаний к холдингу. К сожалению, я не могу пока раскрыть наши планы, но могу сказать: холдингом будет управлять команда менеджеров, которая будет создана совместно. Вы делаете ставку на трафик сетей семейства «БаннерБанк», но теперь, судя по всему, решили изменить положение дел, и «Рамблер» будет вашим донором, или, скажем, подносчиком снарядов? Это не так. Мы не делаем ставку. Мы используем трафик, который нам достаётся почти бесплатно. Это очень эффективно. Но теперь придется делиться почти бесплатным трафиком с «Рамблером»? Как будете компенсировать потери? Когда в 2001 году мы купили «БаннерБанк», все сети, построенные на этой технологии, показывали 2 млн показов в день. Наша комиссия за использование технологии «БаннерБанк» составляла 10% или 200 тыс показов в день. Тогда нам этого не хватало. Сейчас технологию «БаннерБанк» используют более 1200 сетей, которые объединяют более 90 000 сайтов. Эти сети на сегодняшний день показывают 150-170 млн баннеров в день, ежедневная комиссия eHouse составляет 17-20 млн баннеров! По нашим оценкам, «БанерБанк» покрывает более 40% рунета. Количество показов баннеров в системе растёт с каждым днём. Этого количества более чем достаточно для рекламы всех наших проектов. И поэтому мы приняли решение использовать «излишки» показов для продажи. Сейчас мы начали просто продавать свободную комиссию. Но уже в ближайшее время совместно с «Рамблером» будет построена система контекстной рекламы на основе «БаннерБанка». Клиенты смогут показывать баннеры в сетях «БаннерБанка» лишь на тех страницах, тематика которых их интересует. То есть, в будущем можно будет создавать виртуальные частные контекстные сети на базе «БаннерБанка»? Да. Плюс к этому пользователи «БаннерБанка» смогут пользоваться единой системой контекстного показа во всех сетях «Баннербанка». Когда планируете выпустить технологию на рынок? В ближайшее время. Это не так сложно, как кажется на первый взгляд. Основные движки и технологии у нас есть, и они работают. Вы читали комментарии участников рынка в прессе о сделке с «Рамблером»? Я читал некоторые комментарии «специалистов» о нашей сделке с «Рамблером». Двойственные ощущения. С одной стороны приятно, что менеджеры конкурентов плохо ориентируются в том, что происходит. С другой, обидно за рынок. И за издания, которые ориентируются на таких «экспертов». Особенно был удивлён комментарием менеджера из «Адривера». Он ничего не слышал о «БаннерБанке» уже очень долго. «Адривер» использует сети РЛЕ. Наверное, они — наш главный конкурент на рынке баннерных сетей. Но РЛЕ показывает баннеров в 3 раза меньше, чем мы! Реплика этого менеджера напомнила мне слова из басни: «слона, то, я и не приметил». Более 500 системных администраторов используют «БаннерБанк» для построения баннерно-обменных сетей. Они строят на нём свой бизнес. Пару слов хотел сказать о «Мамбе». Приятно, что у менеджеров «Финама» завышенная оценка их проекта и места на рынке. Но в тоже время пугает абсолютное непонимание, о каком рынке они говорят. Мы не являемся конкурентом «Мамбы». Мы строим сообщество. Мы — сайт social network, а «Мамба» — сайт dating service. Мы не занимаемся сводничеством. И анкеты проституток с сайта удаляются. На «Мамбе» же каждая десятая анкета предлагает интим-услуги. Да, мы понимаем, что жрицам древнейшей профессии нужно где-то рекламироваться, но это не наш бизнес. В силу своих принципов построения бизнеса, «Мамба» не может делать сервисы, которые развиваем мы. Например, на «Дамочке» более 250 тыс дневников. Самые популярные имеют более 2 тыс читателей. И здесь наш конкурент не «Мамба», а Livejournal. Мы строим сообщество. Каждую неделю «дамочадцы» встречаются в Москве в «живую» и устраивают большие вечеринки на 400-500 человек. Мы снимаем большие клубы и люди общаются, тусуются до утра. Летом мы проводим open-air в парках отдыха. В Питере вечеринки проходят раз в 2 недели. Тусовки проходят в Лондоне, Таллине, Нью-Йорке, Баку. Раз в год мы устраиваем большие слёты – «Мегадамы», на которые съезжаются «дамочадцы» из разных городов, более 3 тыс человек. У «Дамочки» есть свои футбольные команды — Дамка Москва и Дамка Питер, которые играют в лигах Москвы и Питера. У «Дамочки» есть своя команда КВН. Ещё раз: мы строим сообщество. Мы не тупой сайт знакомств. Мы не рекламируем проституток и трансвеститов. Мы не строим бизнес по принципу «деньги не пахнут». Да, посетителей на всех сайтах «Мамбы» в разы больше, чем на «Дамочке». Но это другой бизнес. И мы не претендуем занять какую-либо долю в этом бизнесе. Поэтому оценка менеджера «Мамбы» о нашей доле на этом рынке не верна. Мы на разных рынках. В прошлом вы регулярно публиковали финансовые итоги за год, однако вот уже два года ничего не публикуете. Почему? Нам кажется это бесполезным. Наши конкуренты публикуют завышенные отчёты, добавляя туда цифры других бизнесов. Компании типа НАУЭТ проводят исследования, в которых также публикуют нереальные оценки, часто основываясь на телефонных опросах. Смешно, но в одном из рейтингов по электронной коммерции НАУЭТ мы увидели знакомую компанию с достаточно большим оборотом. В момент составления рейтинга мы вели переговоры с этой компанией о создании совместного интернет-магазина, и было доподлинно известно, что на момент переговоров они не продавали через интернет ничего. Зачем соревноваться с цифрами, высосанными из пальца? По нашей оценке, мы по-прежнему самые большие продавцы в интернете. Мы опережаем ближайших конкурентов в разы. Публиковать отчёты мы начнём, когда станем публичной компанией. Когда вы планируете стать публичной компанией? Вы проведете IPO в одиночестве, либо выйдете на биржу совместно с другим крупным игроком? Я не хочу называть конкретные сроки. Выходить мы планируем с группой инвестиционных компаний. Крупных игроков на нашем рынке мало и, как правило, их интернет-бизнесы отягощены офлайновыми структурами. Мы не хотим мешать интернет-торговлю с другими бизнесами, поэтому, скорее всего, крупных парнёров не будет. Мы планируем в ближайшее время ряд покупок, но не крупных, а нишевых игроков. Какой эпитет можно подобрать, глядя на вашу дружбу с «Рамблером» сейчас? Можно сказать, что компания стала вашим стратегическим партнером? Мы партнёры. Это долгосрочное партнерство? Конечно.\n","id":3}
{"Host":"https://habr.com","Path":"/en/company/lamptest/blog/401209/?hl=ru_RU&fl=ru","Text":"Уникальный стабилизатор Штиль Инстаб 500 / Habr                 \n\n03-02-2017\nУ меня в руках стабилизатор «Штиль Инстаб 500» , изготовленный специально для проекта Lamptest.ru. Он выдаёт стабильное переменное напряжение 230 В с точностью ±0.6 В и его выходное напряжение не зависит от входного. Стабилизаторы сетевого напряжения ( моя статья о стабилизаторах ) используются прежде всего там, где качество сетевого напряжения невысокое и в сети часто бывает пониженное или повышенное напряжение. Почти все имеющиеся на рынке стабилизаторы относятся к одному из двух типов: ступенчатые (реле или твердотельные переключатели подключают нагрузку к одной из нескольких обмоток автотрансформатора) и электромеханические (токосъёмник перемещается по обмотке автотрансформатора с помощью мотора). У всех стабилизаторов этих типов выходное напряжение зависит от входного и если через стабилизатор подключены обычные лампы накаливания, яркость их света будет «дёргаться» при скачках входного напряжения, несмотря на наличие стабилизатора. Российская компания Штиль выпускает уникальные стабилизаторы, работающие по другому принципу. Это стабилизаторы с двойным преобразованием: сначала сетевое напряжение выпрямляется, а потом постоянное напряжение с помощью инвертора преобразуется в синусоидальное переменное напряжение, уровень которого абсолютно не зависит от входного напряжения, кроме того на выход принципиально не могут попасть никакие помехи из сети. Согласно инструкции, стабилизатор имеет КПД 96%, коэффициент нелинейных искажений при линейной нагрузке 1.5%. Заодно он является «улучшителем» коэффициента мощности — независимо от коэффициента мощности (power factor) нагрузки, его коэффициент мощности для питающей сети всегда 0.99. Для проверки работы стабилизатора я подключил его к выходу ЛАТРа — лабораторного автотрансформатора, позволяющего плавно менять переменное напряжение в широких пределах ( моя статья о нём ). Напряжение на входе и выходе стабилизатора контролируется одновременно цифровым и стрелочным прибором (по отклонению стрелки нагляднее видно колебание напряжения). К выходу стабилизатора подключена обычная лампа накаливания. Стабилизатор работает при входном напряжении 90-310 В. При любых изменениях входного напряжения в этом диапазоне (как плавных, так и скачкообразных) напряжение на выходе абсолютно не изменяется. Стрелка вольтметра стоит, как вкопанная, яркость лампы не меняется. К примеру, на входе стабилизатора 95 В, а на выходе 230.2 В. Напряжение на выходе немного меняется, но это зависит не от входного напряжения, а от процессов в самом стабилизаторе. Производитель перестраховывается и указывает в технических характеристиках точность стабилизации ±2% (а при выходном напряжении 220 В это ±4.4 В), фактически напряжение отклоняется от номинального не более, чем на 0.6 В (±0.3%). Если входное напряжение оказывается за пределами стабилизации, стабилизатор отключает нагрузку и на нём загорается один из индикаторов Uвх < 90В или Uвх > 310 В. После возвращения входного напряжения в допустимые пределы, стабилизатор включается вновь. Upd.: Снял осциллограмму на выходе стабилизатора: красивая синусоида. От изменений входного напряжения её форма никак не меняется. При подключении и отключении реактивной нагрузки иногда на доли секунды синусоида искажается (на ней появляются короткие импульсы). Зафиксировать помехи в виде скриншота не удалось. Так выглядит стабилизатор со снятым кожухом. Работой инвертора управляет плата процессора, защищённая от помех металлическим экраном. Напряжение на выходе стабилизатора регулируется только параметрами в прошивке. Сейчас выпускаются стабилизаторы с выходным напряжением 220 В, а мой стабилизатор с напряжением 230 В был сделан в единственном экземпляре. Ближе к лету планируется наладить выпуск стабилизаторов с возможностью выбора выходного напряжения 220/230 В. Я снял видео, наглядно демонстрирующее работу стабилизатора. В серии Инстаб четыре стабилизатора с выходной мощностью 500, 1000, 1500 и 3500 ВА, но только Инстаб 500 не имеет вентилятора и почти бесшумен. Почти, потому, что стабилизатор издаёт тихий зудящий звук, почти не слышный на расстоянии метра от корпуса стабилизатора. Инстаб 500 стоит 6396 рублей, Инстаб 1000 — 10956 рублей, Инстаб 1500 — 12949 руб, Инстаб 3500 — 22649 руб. Штиль производит и автомобильные инверторы , мощностью 300 Вт/450 Ва, которые по своим размерам и весу раза в три больше китайских инверторов с аналогичной заявленной мощностью. И это неспроста. Инверторы Штиль PS12-300А и PS24-300А дают чистую синусоиду, выдерживают перегрузку до 105% — длительно, 105-125% — 60 с, 125-170 % — 8с, >170% — 3с и имеют все возможные защиты, правда стоят при этом 7500 рублей. Вообще, основной бизнес компании — производство промышленных инверторов, ИБП и установок электропитания. Их оборудование стоит на множестве базовых станций операторов сотовой связи. Судя по всему, Штиль — единственный в мире производитель бытовых инверторных стабилизаторов. Возможно, что-то подобное есть для профессионального применения, но других бытовых стабилизаторов, работающих по такому принципу, мне найти не удалось. Стабилизаторы Штиль разработаны и производятся в России на заводе в Туле. Спасибо компании Штиль и лично Генеральному директору ООО «Штиль» Сергею Павлову за стабилизатор, благодаря которому измерения Lamptest.ru стали ещё точнее. © 2017, Алексей Надёжин\n","id":4}
{"Host":"https://habr.com","Path":"/ru/post/105035/?mobile=yes","Text":"7 советов о том, как получить больше от Google AdWords / Хабр                 \n\n27-09-2010\nВсем известно, что службы контекстной рекламы, такие как Google AdWords, основываются на принципах аукциона. Но на самом деле все не так просто. Повышение вашего показателя качества (Quality Score) означает, что вы будете тратить меньше, а получать больше посетителей на ваш сайт. Допустим, что ваш Google AdWords бюджет составляет 50 000$ в год. Вы конкурируете с гораздо более крупными компаниями, имеющими рекламные бюджеты гораздо выше вашего. Зачастую это означает, что они могут купить гораздо больше ключевых слов из тех, которые вам нужны, чем вы. Но это не всегда так. Торги по ключевым словам действуют не совсем по принципам обычного аукциона. Google присваивает каждому рекламному объявлению показатель качества – число от 1 до 10, который отражает по мнению поискового гиганта, понравится ли реклама и тот сайт, на который она ведет посетителям или нет. Это число влияет на ранг вашего объявления в аукционе. Высокий показатель качества позволит вам опередить конкурентов с более высокими ставками и получить больше с каждого вашего рекламного доллара. Удвоение показателя качества означает, что вы будете платить вдвое меньше за клик. Вот несколько советов как улучшить свой показатель качества: 1. Не показывайте объявления, по которым не будут кликать Не надо действовать по принципу «мы повесим тут это объявление и посмотрим, будет ли кто-то переходить по нему». Это может показаться логичным, но зачастую является ошибкой. Соотношение кликов к показам (CTR), является одним из наиболее важных составляющих показателя качества, поэтому, если ваше рекламное объявление показывается, но пользователи не щелкают по нему, показатель качества сильно снижается, и стоимость повторного запуска этого же объявления и даже других в будущем увеличится. Google присваивает показатель качества группам объявлений и самим рекламодателям, так что плохой CTR может повредить вам во многих случаях. Поэтому стоит постоянно тестировать ключевые фразы в разных формулировках, чтобы найти сочетания с наивысшим CTR. Это не только приведет к вам больше клиентов, но и снизит цену за клик. 2. Разделяй и влавствуй Можно использовать тысячи ключевых слов в одной группе объявлений, которые ведут на одну и ту же страницу. Но оказывается, результат получается гораздо лучше, если разделить их на группы по нескольку слов в каждой, связанных общим смыслом, и которые будут вести на соответствующие страницы. Например, если вы продаете комплектующие для фотоаппаратов не надо помещать «Nikon», «Canon», «навести и снять», «SLR» в одну группу объявлений. Их стоит разбить. К примеру создать группу объявлений в которой рекламируются только батареи для Canon PowerShot и клик по которым приводил бы именно на страницу с этими батареями. И так далее. Такое поведение может стать козырем небольшой компании против крупных конкурентов. 3. Избавьтесь от ключевых слов, которые не работают Вместо длинного списка ключевых словосочетаний, которые характеризуют все ваши предложения. Какие-то из них могут оказаться более популярными из-за того, что характеризуют общие понятия, но рекламная кампания будет размыта и часть клиентов, которые искали одно, а получили другое, будут уходить разочарованными. Лучшими ключевыми словами будут те, которые не носят общий характер. Выбирайте слова и словосочетания, которые будут наиболее частными в нише, которую вы представляете. 4. На целевой странице и в целом на сайте должен быть нормальный контент Наличие полезной информации, является одной из важнейших вещей, которые улучшают ваш показатель качества. Не стоит использовать содержимое других сайтов. Вам потребуется уникальное, разнообразное, часто обновляемое содержимое. Это приводит к лучшему отклику пользователей. Сделайте так, чтобы пользователи могли доверять вам и вашему сайту, люди ищут именно это. 5. Предоставьте пользователям выбор Вы можете на своих целевых страницах пытаться призывать пользователей к конкретным действиям. Например, «нажмите здесь, чтобы узнать о нашей продукции» или «подпишитесь на нашу бесплатную рассылку». Но Google хочет, чтобы у пользователей был широкий спектр вариантов и меню навигации на вашей целевой странице может повысить ваши показатели качества. После того как пользователь пришел на целевую страницу, он хочет получить больше информации. Так дайте ему ее. Сделайте так, чтобы им захотелось изучать ваш сайт. Иначе они разочаруются, что скажется на показателе качества не в лучшую сторону. 6. Расскажите о себе Если на вашем сайте нет хорошо заметных ссылок на раздел «о нас» или «о компании», нет данных или раздела о правилах использования личных данных, которые вы просите у пользователя, таких как адрес электронной почты, то ваш показатель качества будет страдать. Google заинтересован в «прозрачности» рекламодателей. И если пользователям приходится доверять свои данные, компания хочет быть уверена, что они будут использованы по назначению и не окажутся в лапах спамеров. 7. Сделайте так, чтобы страницы загружались быстрее Убедитесь, что ваши страницы грузятся быстро. Если они будут делать это слишком медленно, пользователям это не понравится. Поменяйте хостинг или уменьшите количество или размер изображений и интерактивных элементов на странице, чтобы добиться приемлемой скорости загрузки. Зачастую этот показатель очень критичен. Даже небольшие простои могут быть замечены автоматической проверкой качества Google. Очевидно, что столкнувшись с тем, что целевая страница недоступна, программа уменьшит показатель качества. Постоянно следите за этими показателями вашего сайта. via Inc.\n","id":5}
{"Host":"https://habr.com","Path":"/ru/company/jugru/blog/501922/?amp&amp","Text":"IoT там, где вы не ждали. Разработка и тестирование (часть 1) / Хабр                                                \n\n18-05-2020\nПрименением IoT в фитнес-трекерах, колонках, пылесосах уже никого не удивишь. Думали ли вы, что различные датчики можно установить в мусорные контейнеры и мусоровозы для контроля вывоза мусора? Звучит странно и вызывает вопрос: «А зачем?». Анатолию Коровину случилось поработать на таком проекте, который они делали с нуля. А в этой статье разбор его доклада с конференции Heisenbug, где он подробно рассказал, почему нужно было делать «Умный мусоровоз» и как велись разработка и тестирование этого проекта. Общая структура доклада показана на картинке ниже. В этой статье мы рассмотрим только то, что закрашено зеленым на схеме. Проблемы в отрасли Основные действующие лица: мусорные контейнеры (ну тут все понятно, мы каждый день видим мусорные баки); жильцы многоквартирных домов; региональный оператор (организация отвечающая за работу с твердыми коммунальными отходами (ТКО) в регионе); компании перевозчики мусора (транспортные компании, спецтехника для вывоза ТКО); мусорные полигоны (это и мусорные котлованы, и мусоросжигатели, и заводы по переработке мусора). Когда мы платим за вывоз мусора, то оценить качество услуги проще всего на основании факта вывоза мусора со двора. Но если погрузиться глубже в этот вопрос и попытаться выяснить, куда перевозчик везет мусор (до полигона или в ближайшую канаву), то возникают сложности. Поэтому одной из задач нашей системы стал контроль добросовестности исполнителей. Бумажная отчетность В большинстве регионов, с которыми мы работали, региональные операторы собирали отчеты от перевозчиков в бумажном виде. В отчете фиксируется номер автомобиля перевозчика, его маршрут, время загрузки баков и объем мусора. И сдают эти отчеты один раз (или два раза) в год, так что вы можете представить какой это объем данных. Сложно проверить Учитывая, что нет никакого механизма для проверки достоверности этих данных, то во многом отчет принимается «на веру». Для решения проблемы операторы в некоторых регионах установили GPS-трекеры на машины перевозчика и стали отслеживать маршрут транспорта. Хоть операторы и задали вектор решения верно, целиком задача не была решена — отчеты все также представляли собой кипу бумаг на полках в офисе операторов. Сложно представить, что кто-то станет проверять все руками. Из этой кучи бумаги нужно достать какую-нибудь случайно, открыть программу трекера, сверить время в отчете и на трекере, местоположение машины. И на проверяющего падает слишком большой объем данных. Помимо этого, есть недобросовестные перевозчики, использующие уязвимости в этой системе. Они постепенно осознали, какие показатели у них проверяют, и начали выстраивать «серые» схемы. Например, перевозчик подъезжает к мусорному контейнеру во дворе, водитель выходит из машины, курит пару минут и уезжает, так и не забрав мусор. Затем он приезжает на ближайшую стройку и загружает себя коммерческим мусором, утилизация которого должна оплачиваться отдельно, потому что это отходы производств и других промышленных предприятий. После этого перевозчик добирается до полигона, где уверенно говорит, что весь этот мусор забрал у подъездов жилых домов. В таком сценарии маршрут, полученный с GPS-трекера, будет показывать те же самые точки, что фигурируют в отчете перевозчика, но мусор никто не вывез. И это только один из множества сценариев. Решаем проблемы при помощи IoT Чтобы покрыть наибольшее число сценариев, была построена IoT-система, включающая автоматизацию мусорных баков, транспорта перевозчика и мусорные полигоны. Умные мусорные баки Для начала мы оборудовали мусорные контейнеры несколькими типами датчиков: Обычный ультразвуковой датчик — определяет уровень заполненности контейнера. Использовать дорогие навороченные системы не имело смысла, поскольку такой девайс по цене iPhone не поставишь в мусорку где-то в удаленном районе, и наверняка его кто-то захочет унести домой. Датчик температуры — телеметрия с него дает понять, не горит ли мусор; акселерометр. Он определяет угол наклона контейнера, и так мы фиксируем в истории, когда приезжал перевозчик и выгружал мусор. А в качестве приятного бонуса мы можем узнать, не перевернули ли вандалы мусорку. RFID-метка. С ее помощью можно однозначно сопоставить, какая машина перевозчика выгружает контейнер. Все данные передавались через GSM/GPRS, так что никаких проводов, просто прицепили на стенку контейнера, и девайс измеряет уровень мусора перед собой. Следим за транспортом перевозчика Для контроля транспорта мы использовали: GPS-трекеры; датчик подъема крана, чтобы отслеживать передвижение арматуры; RFID-считыватель для определения, какой контейнер прицеплен к машине; камера на случай каких-либо разбирательств. Как вы думаете, какой датчик оказался самым бесполезным? Да, снимки с камеры не принесли никакой пользы. Из них не то что гламурный аккаунт в Instagram не соберешь, там даже непонятно, на что смотрим-то: Камеру довольно сложно установить на спец-транспорте перевозчика так, чтобы фотографии были наглядными. В итоге на фото были: дорога, шлагбаум, лужи и т.д., и изредка контейнерные площадки с не самых удачных ракурсов. Ожидали увидеть различные нарушения перевозки мусора, а на деле мусор — сами фото. Контролируем утилизацию мусора Контейнеры и транспорт подключили, осталось получить телеметрию от конечной точки — мусорного полигона. Для этого на въезде устанавливается огромный весовой стенд. Когда автомобиль заезжает, мы распознаем номер машины и фиксируем данные в базе. На выезде транспорт проходит аналогичную процедуру, и на разнице этих двух значений мы и работаем. Что дало внедрение системы? Перевозчики от этой системы получают довольно большой профит, поскольку теперь каждое утро они знают оптимальный маршрут следования. Машины теперь могут отправиться сначала к точкам, где контейнеры полностью заполнены, и затем двигаться по точкам по убыванию уровня заполненности. Такие треки разительно отличаются от обычных маршрутов перевозчиков, поскольку без телеметрии предугадать оптимальный вариант довольно сложно. Также бывает и такой сценарий: большинство площадок для вывоза мусора скомпоновано в достаточно малой области, но есть одна точка, которая располагается на значительном расстоянии от других. Без телеметрии трудно узнать, нужно ли забирать мусор с этой площадки, и водитель приедет к ней, увидит, что контейнеры пустые, и уедет, потратив время и топливо. В продолжении этой статьи расскажем, как устроен бэкенд этого проекта и какую специфику привнес IoT в подходы к тестированию. Анатолий Коровин выступит на ближайшей конференции Heisenbug 2020 Piter, которая пройдет 15-18 июня в онлайне. Этим летом возможно получить знания не только в области тестирования, но и в распределенных вычислениях, фронтенде, бэкенде, DevОps, мобильных проектах, по билету-абонементу.\n","id":6}
{"Host":"https://habr.com","Path":"/ru/post/555502/?mobile=yes","Text":"Влияние протокола языкового сервера (LSP) на будущее IDE / Хабр                                                  \n\n02-05-2021\nПеревод статьи How the Language Server Protocol Affects the Future of IDEs Автор оригинала Mehul Mohan С момента своего появления Visual Studio Code в одиночку так сильно повлиял на экосистему разработчиков, что возврата назад уже не будет. Это общедоступный бесплатный инструмент с открытым исходным кодом и это очень мощный инструмент. Но, благодаря VSCode, Microsoft в 2016 году дала жизнь еще одной суперзначимой вещи, которая менее известна. Это Language Server Protocol - Протокол языкового сервера. Что такое Протокол языкового сервера? Протокол языкового сервера (Language Server Protocol - LSP) - это способ общения с языковыми серверами (такой же как HTTP или FTP). Языковые серверы - это специальные программы, работающие на обычных серверах. Они принимают мета-состояние редактора, в котором вы пишете код (например, текущая позиция курсора внутри редактора, токен над которым находится курсор), и возвращают набор действий или инструкций - какой токен должен быть следующим, что должно произойти, когда вы нажимаете CMD/Ctrl-клик на этом токене, и так далее. Это взаимодействие происходит с помощью набора правил, заданных протоколом. Протокол языкового сервера можно рассматривать как урезанную модификацию HTTP взаимодействующую только по JSON-RPC протоколу. Зачем нужен LSP? Вы заметили, что в VSCode постоянно появляются изобретательные сообщения об ошибках и предложения автоподстановок? И как же легко, просто установив расширение из магазина VSCode, вы получаете всю мощь IntelliSense для совершенно разных языков, таких как C, Python, Java и т.д.? Все это происходит благодаря LSP. Поддержка автозавершения и IntelliSense для HTML/CSS/JavaScript идет сразу вместе с VSCode (так же, как PyCharm идет сразу с поддержкой Python). Однако такая же поддержка других языков может быть реализована с помощью протокола LSP для этих языков. Что такое JSON-RPC? JSON-RPC означает удаленный вызов процедуры JSON (Remote Procedure Call). Это архитектура (подобно тому, как REST является архитектурой), но основная единица - это вызов процедуры, а не конечная точка API в случае REST. Это простой пример для JSON-RPC: // Request curl -X POST —data '{ \"jsonrpc\": \"2.0\", \"method\": \"runThisFunction\", \"params\": [ \"some-param\", 2 ], \"id\": 1 }' // Response { \"jsonrpc\": \"2.0\", \"result\": \"codedamn\", \"id\": 1 } В этом примере мы посылаем запрос в кодировке JSON в соответствии со спецификацией RPC. Если сервер настроен на корректную работу с JSON-RPC, то он выполнит метод runThisFunction с переданными параметрами и вернет результат в том виде, как показано выше. LSP + JSON-RPC LSP использует JSON-RPC для связи с удаленным сервером. Для этого используется следующий формат: Content-Length: <bytes of JSON>\\r\\n\\r\\n<json-payload> Пример запроса: Content-Length: 78 {\"jsonrpc\":\"2.0\",\"method\":\"runThisFunction\",\"params\":[\"some-param\",2],\"id\":1} LSP требует, чтобы вы передали заголовок Content-Length, за которым следуют два CRLF токена \\r\\n. Когда запущенные языковые серверы, такие как ccls, получат это сообщение, они ответят соответствующим сообщением: Конечно, в приведенном выше примере видно, что ccls говорит о том, что не существует метода, названного runThisFunction. Но можно заметить, что удаленный сервер также отвечает заголовком Content-Length со спецификацией JSON-RPC. Почему все это так важно? С введением формального протокола LSP, Microsoft свела знаменитую проблему M x N к проблеме M + N. M = Различные языки (C, C++, PHP, Python, Node, Swift, Go и т.д.). N = Различные редакторы кода (VSCode, Eclipse, Notepad++, Sublime Text и т.д.). Раньше для того, чтобы M редакторов поддерживали N языков, вам нужно было иметь M*N решений. То есть каждый редактор кода должен был реализовать поддержку каждого языка самостоятельно. С появлением LSP в редактор оставалось лишь внедрить поддержку протокола языкового сервера. После этого любой, кто делает языковой сервер (следуя стандартам LSP), может легко интегрироваться в редактор кода, при этом редактор никогда не будет \"знать\", с каким языком он работает! Будущее IDE По мере того, как языковые сервера реализуются для различных языков программирования, у разработчиков появляется возможность выбирать редактор на свой вкус. Без привязки к конкретному языку. Нет больше необходимости ограничивать себя, например только XCode для разработки на Swift или PyCharm для Python. И не только это, LSP можно внедрить прямо в JavaScript для поддержки IntelliSense в браузере! Настало потрясающее время для программистов!\n","id":7}
{"Host":"https://habr.com","Path":"/en/company/spbifmo/blog/462393/?mobile=no","Text":"Как начать карьеру еще в вузе: рассказывают выпускники пяти профильных магистратур / Habr                        \n\n03-08-2019\nНа этой неделе в нашем блоге на Хабре вышла целая серия материалов о том, как проходит обучение и практика в магистратуре Университета ИТМО: Магистранты факультета ИТ и программирования делятся опытом Образовательный процесс и работа со светом в нашей магистратуре Учеба и практический опыт на факультете фотоники и оптоинформатики Фото Университета ИТМО Сегодня на очереди сразу несколько направлений «Национального центра когнитивных разработок». Рассказываем, как здесь все устроено с точки зрения учебы и практики. Первая работа — в лаборатории вуза Магистранты могут попробовать свои силы в выбранной ими профессии еще во время обучения в вузе. Студенты проходят практику и начинают работать в лабораториях Университета ИТМО. Как говорит Александр Валерьевич Бухановский, директор «Национального центра когнитивных разработок», студенты с первых дней погружаются в научно-исследовательские и опытно-конструкторские разработки по заказу индустриальных партнеров. Таким образом, учащиеся применяют получаемые знания в реальных проектах. Такой подход оказывает влияние и на индивидуальную образовательную траекторию. Студент имеет возможность выбирать образовательные модули из магистерских программ в соответствии с потребностями реализуемого им проекта. — Александр Валерьевич Бухановский Дополнительное преимущество такого подхода заключается в том, что студенты могут трудиться практически без отрыва от учебы. Проекты, над которыми магистранты работают в лаборатории, часто становятся частью выпускной или курсовой работы. Многие выпускники даже решают строить свою карьеру в лабораториях Университета ИТМО. В качестве примера можно привести историю трудоустройства Анастасии Функнер. Она включилась в работу над проектами уже в процессе поступления на направление «Цифровое здравоохранение» и продолжает работать в вузе после выпуска в 2018 году. В настоящий момент Анастасия готовится к поступлению в аспирантуру. Трудоустройство по специальности в компаниях-партнерах Практиковаться во время учебы в магистратуре можно не только в университетских структурах, но и в компаниях, которые сотрудничают с кафедрами «Национального центра когнитивных разработок». Среди них: «Газпром нефть», ООО «Инфотехника», МТС. Еще — банки и финтех-компании: Среди индустриальных партнеров программы числятся крупные интернациональные финтех-компании, например ITIVITI. Они обеспечивают как практику (в виде решения совершенно конкретных «боевых» кейсов), так и трудоустройство выпускников. Таким образом, возможности для студентов есть и на внутреннем, и на международном рынке труда. — Боченина Клавдия Олеговна, руководитель программы «Финансовые технологии больших данных» Больше компаний-партнеров Комитет по информатизации и связи Санкт-Петербурга Комитет цифрового развития Ленинградской области Санкт-Петербургский информационно-аналитический центр Компания «Нетрика» OOO «Агентство электронного тестирования» СПб Государственное бюджетное учреждение здравоохранения «Медицинский информационно-аналитический центр» Государственное бюджетное учреждение Ленинградской области «Оператор электронного правительства» В случае с компаниями-партнёрами не возникает противоречий между учебной деятельностью и задачами магистранта на рабочем месте. Темы семестровых НИР и финальная магистерская диссертация ориентированы на задачи, поставленные работодателем. Работа являлась и научной деятельностью, а фактический работодатель — научным руководителем. Занимались АИС «Антинар», ИС ИАО. Фактически все одногруппники работали, некоторые были даже коллегами, а студенты на курс старше работали в соседних отделах. — Сергей Моисеенко, работает в Комитете цифрового развития Ленинградской области Помимо Сергея Моисеенко, карьеру в компании-партнере строит еще одна выпускница кафедры «Управления государственными информационными системами» — Ксения Воронина (эту кафедру реорганизовали в «Цифровые технологии умного города»). Она работает в Комитете по информатизации и связи Санкт-Петербурга и руководит командой разработчиков. На момент начала работы в Комитете по информатизации, я училась на втором курсе магистратуры. Знания, приобретенные в процессе обучения, оказались более чем актуальными и необходимыми специалисту, работающему в данной сфере, еще и по такому специфическому направлению, как информатизация. — Ксения Воронина Желание студентов совмещать работу и учебу поддерживают преподаватели вуза. Боченина Клавдия Олеговна считает, что работа студента в компании, совпадающей по профилю с профилем программы, это не просто допустимый, а желательный сценарий. Магистрант получает возможность специализироваться на тех профессиональных задачах, с решением которых ему предстоит сталкиваться по окончании обучения. Фото Университета ИТМО При этом мы и компании партнеры делаем все, чтобы магистрантам было комфортно и работать, и учиться одновременно. В качестве иллюстрации хорошо подойдет опыт Руслана Вергунова, выпускника направления «Финансовые технологии больших данных». Со второго курса магистратуры он работает специалистом по анализу и обработке данных в финансовой компании. Многие занятия проходили в вечернее время или в субботу — совмещать их с работой было легко. Предметы, которые проходили в дневное время, посещать труднее, однако можно было приходить на самые важные темы, а остальные изучать самостоятельно. Также мне удалось договориться о четырехдневном рабочем дне, когда была необходимость посещать военную кафедру. Все относились к этому с пониманием и шли навстречу. — Руслан Вергунов Также показательным можно назвать опыт Лилии Шандалюк, трудоустроенной в группе компаний GOST. Лилия половину дня работала, а другую половину — посещала занятия. По её словам, работодатель понимал, что деятельность в рамках университета важна для реализации проектов компании. Поэтому проблем с допуском до учебы не возникало. Работа в сторонних организациях Знания, полученные в вузе, помогают выпускникам устроиться на работу в ведущие российские компании. Например, Руслан Вергунов говорит, что для прохождения технического интервью ему хватило учебной программы университета. Некоторые выпускники находят работу за рубежом. Например, так поступил Степан Ракитин, завершивший программу «Большие данные и машинное обучение». Сегодня он работает как Machine Learning Engineer в немецкой компании HelloFresh. В его задачи входит разработка инфраструктуры для машинного обучения. В ближайшее время Степан планирует перейти в другую берлинскую компанию — Anaconda Inc. По его словам, знания, полученные в стенах университета, очень помогли на собеседованиях и помогают во время работы. Я считаю, что мне очень пригодились знания и практический опыт, полученные в Университете ИТМО. Там я получил представление о том, что такое машинное обучение и data science в целом, научился разрабатывать системы обработки больших объёмов данных. — Степан Ракитин Фото Университета ИТМО Степан также отметил, что в поиске работы ему помогает имя вуза. Есть два «универа», к которым я вижу особое отношение — ИТМО и МФТИ. Обычно кандидаты из них рассматриваются в приоритете, даже если вы учились там давным-давно. Также будьте готовы к повышенным ожиданиям, потому что при упоминании ИТМО — ты сразу олимпиадник и ездил на ICPC. — Степан Ракитин Бывают и случаи, когда студенты открывают свои компании. Боченина Клавдия Олеговна рассказывает, что однажды прямо на защите ВКР представитель бизнеса решил помочь превратить одну из защищенных работ магистранта в стартап. Он выразил готовность найти инвестора для развития этого проекта. Преподаватели, руководители и партнеры Университета ИТМО всегда готовы пойти навстречу студентам, которые хотят стать высококвалифицированными специалистами и развиваться в выбранной области. Для этого магистрантам предоставляются все необходимые инструменты — теория, практика, стажировки и полноценный рабочий опыт. P.S. Прием документов на магистерские программы «Большие данные и МО», «Цифровое здравоохранение», «Технологии разработки компьютерных игр», «Финансовые технологии больших данных» и «Цифровые технологии умного города» продолжается до 5 августа.\n","id":8}
{"Host":"https://habr.com","Path":"/en/post/257549/?mobile=no","Text":"Как я в облака ходил / Habr             \n\n08-05-2015\nПонадобилось не так давно в целях развертывания сервиса для тестирования получить в свое распоряжение виртуальную машину в облаке. Желательно бесплатно. И надолго. Пошерстил множество ресурсов, в том числе Хабр. И не нашел подробного обзора «чего, как и почем». Так родилась идея написания этой статьи по результатам самостоятельного поиска. Многие провайдеры облачных вычислений (cloud computing) предоставляют бесплатные «пробники» на определенный период. По крупицам отыскивая информацию в поисковых выдачах, нашел и опробовал нескольких из провайдеров. В статье речь пойдет о том, как получить в свое распоряжение виртуальную машину (или несколько) от таких провайдеров как Google, Microsoft, HP и в перспективе другие. Будет также небольшое сравнение. Обладая уровнем паранойи выше среднего, я совсем не хотел вводить данные своей реальной банковской карточки на каких-либо сайтах. А большинство (нет, то есть ВСЕ) провайдеры требуют именно их, прежде чем позволят попробовать свои сервисы. В итоге я решил пользоваться для этих целей виртуальной картой одной известной платежной системы — и это получилось, ну почти. Итак, поехали. 1) Microsoft Начал с Microsoft Azure. Зайдя первым делом на сайт live.com, зарегистрировал почтовый ящик outlook.com. Забавная штука: регистрация проходит мягко и ненавязчиво, но вот после попытки отправить любой email выводится требование подтвердить аккаунт через SMS, привязав, таким образом, мобильный телефон. Ну что ж, практика известная. Мобильник специально для таких целей у меня уже заготовлен: Итак, аккаунт live.com получен. Переходим на account.microsoftazure.com и кликаем по Get Free Trial. Регистрация подразумевает 4 пункта, каждый из которых – мини квест. Наша задача в том, чтобы все стали отмечены зелененькой галочкой Complete. Вводим информацию о себе. Потом переходим к верификации телефона (что поделать, недостаточно им того, что телефон уже привязан при регистрации почты). После успешного ввода смс-кода напротив пункта два появляется заветное complete и открывается форма для ввода данных карты. Вводим. Нажимаем на Sign up и получаем complete, после чего нас просят подождать: После некоторого ожидания сайт просит еще раз перелогиниться и, наконец, открывается заветная консоль управления инстансами: Создание инстансов (т.е. виртуальных машин) интуитивно понятно. Можно выбрать образ для развертывания из тех, что заботливо подготовили для нас специалисты Microsoft. А затем нажать «Create a virtual machine». Ждем, ждем и еще раз ждем, пока образ развернется — и, вуаля! Можем ходить на него по rdp. Единственное НО, если вы, как и я, приверженец старых версий виндоуз — придется немного помучиться. Т.к. на инстансе для RDP сессий по умолчанию включена проверка Network Level Authentication, причем в обязательном порядке. Это значит, что если у вас клиент старых версий (даже WinXP SP3), то подключиться у вас не выйдет, сервер просто выдаст ошибку. Лично я вышел из этой ситуации, временно задействовав RDP клиент с доступной мне Win7. Затем перенастроил инстанс так, чтобы проверка NLA не была обязательной. После этого можете делать с инстансом все, что пожелаете! В законных пределах, конечно. В вашем распоряжении 300$ на расходы и месяц на тесты. Ну а дальше – платить (или проходить весь квест с фейковой регистрацией по новой). 2) Google cloud Заходим на cloud.google.com. Нажимаем Free Trial и попадаем на страничку регистрации. Куда любезно вводим свои данные: После чего нас сразу же пропускают в создание инстансов. Вбиваем имя, зону размещения виртуальной машины (я выбрал поближе: europe-west1-b). Ну и тип машины, мне для моих скромных нужд больше, чем n1-standard и не нужно. А вот дальше нужно решить, с какого имиджа будет создана виртуалка. Захотел я посмотреть, как у гугла будет работать виндоус сервер. В наличии есть только server-2008-r2. Его и выбрал. Дальше вбиваем username, password админской учетки. И переходим к заветному Networking. Тут нам предлагают статический IP, белый, пушистый, который доступен отовсюду. И это очень круто. Но если вы его отвяжете от конкретного инстанса и он будет просто закреплен за вашим аккаунтом, за это берут мзду, небольшую. Да и не так это важно, гугл дает нам аж 300$ и целых 60 дней на попробовать. После нажатия на кнопочку Create и некоторого ожидания, наша машина доступна для подключения по RDP, и таких сложностей, как в MS с NLA, уже нет – коннектимся отовсюду и без проблем. Проблемы возникли позже. Когда я развернул внутри инстанса свой проект (vpn), то удаленные машины по неизвестным для меня причинам не могли даже ходить друг к другу по ssh, пинги пропускались, но в остальном сеть подмораживало. Экспериментировал с настройками виртуального сетевого адаптера гугловского, там он зовется Red Hat Networking. Но так и не добился успеха. В итоге всё решил переходом на юниксовый инстанс виртуалки от «Гугла» же. При этом подобной проблемы не возникало с облачным сервисом от Microsoft, у них винда работала как часы. Плюс «Гугла» же в том, что за все 60 дней я не припомню ни одного ребута. То есть аптайм шикарный. У «Майкрософта» же за 30 дней был как минимум раз когда пришлось все переконфигурировать. В общем, резюмируя, могу сказать что облачный бесплатный (триальный) хостинг ваших сервисов, в тестовых целях конечно, имеет право на жизнь! Да и пощупать как оно в реальности работает было приятно и полезно.\n","id":9}
{"Host":"https://habr.com","Path":"/en/company/BrandMaker/blog/137523/","Text":"Дневники внедрения: работа модуля «Медиа Ресурсы» на примере банка / Habr             \n\n03-02-2012\nДоброго времени суток, хабрачитатели! Одна из главных трудностей, возникающих при продвижении на рынок нового бизнес-софта – это отсутствие практических сведений об эффективности его работы в условиях реальных бизнес-процессов. У головного офиса BrandMaker в Германии имеется огромный багаж подобной информации, но относится она сугубо к европейским клиентам компании, и в России мало применима. Европа – Европой, но клиентам хочется убедиться в том, что программа будет столь же эффективна и в российских реалиях. Чтобы наработать необходимый объем этих данных, BrandMaker-Russia решила предоставлять клиентам возможность протестировать продукт перед покупкой, оплачивая при этом только операционную работу по развертыванию и администрированию системы. Одним из условий такого сотрудничества является то, что в ходе тестирования пользователи системы самостоятельно оценивают изменения в бизнес-процессах, экономию временных и финансовых ресурсов. Нам важно, чтобы этот анализ происходил именно на стороне клиента – это, безусловно, делает его результаты более весомыми при принятии окончательного решения. Сегодня хочу представить результаты одного из таких исследований. Итак, вводные данные: • клиент: средних размеров банк с федеральной сетью (представлен во всех крупных городах РФ) • пользователей, участвующих в тестировании системы – 50 • начало тестирования – октябрь 2011 года. • конфигурация системы: стартовали с модуля «Медиа Ресурсы», с февраля планируется подключение модуля «Web-to-Print». • в головном офисе банка работает штатный дизайнер, выполняющий основную часть работы по разработке и индивидуализации дизайна. В конце года были подведены промежуточные итоги внедрения «Меди Ресурсов». Суть анализа состояла в определении временных затрат по ключевым рабочим процессам отделов маркетинга. Оценивалась продолжительность этих процессов до внедрения модуля и после. Сам по себе документ получился достаточно объемным, поэтому ниже приведу только самые основные показатели. В ходе анализа рассматривались в основном процессы, связанные с разработкой рекламных материалов для продуктов/услуг банка. Всего здесь сотрудники банка выделили семь основных процессов. По каждому из них оценивалась общая продолжительность и точные затраты времени в человеко/часах, а также давались комментарии по поводу типичных проблем. Конечные данные по экономии времени представлены в итоговой таблице. Расчеты временных затрат одного сотрудника Управления рекламы Головного банка при работе с рекламными макетами (из расчета 5 макетов в течение 1 месяца). Результаты оказались куда лучше, чем мы ожидали, и это нас несколько озадачило. Во время презентаций продукта мы обычно говорим о сокращении затрат времени в среднем на 60% — в соответствии с практикой немецких коллег. В данном случае, видимо, дело в высоком исходном уровне потерь времени на все эти процессы. Причем это, по нашим оценкам, вполне показательный пример для большинства российских компаний. За счет чего происходит экономия времени? Было Больше всего потерь времени обнаружилось на этапах пересылки макетов и их адаптации на местах. Из-за большого объема файлов пересылать их по электронной почте невозможно, поэтому приходится использовать файлообменники. Недостатки: • Ограниченный «срок жизни» файла (обычно не более месяца). • Найти его можно только по прямой ссылке. • Отсутствие функции предпросмотра. Конкретно у данного клиента ситуация усугубляется тем, что из соображений безопасности основная масса компьютеров работает в локальной сети, без доступа в Интернет, и есть ограничения по скорости трафика. Довольно типично для банковской сферы. Из-за этого все участники процесса в цепочке «дизайнер — головной офис банка — региональные офисы банка — типографии на местах» стараются перестраховаться и «на всякий случай» скачивают все рабочие файлы каждый себе на компьютер. Причем не только конечные согласованные варианты, но и кучу ненужных потом файлов (промежуточные и альтернативные варианты, превью и т.д.). В итоге обилие разных версий файла, сохраненных на компьютерах разных пользователей потом затрудняет поиск актуальной версии. Особенно если она понадобится не сразу, а через какое-то время. Тут, конечно, все очень зависит от того, как каждый конкретный сотрудник поддерживает порядок в своих файлах. «Человеческий фактор» в классическом виде. Стало Последние пару месяцев, после внедрения BrandMaker, штатный дизайнер загружает макет непосредственно в модуль «Медиа Ресурсы», и там уже идет вся последующая работа с ним – согласование, адаптация, и т.п. Есть возможность дать прямую ссылку на скачивание стороннему пользователю – например, в типографию. Для того, чтобы просмотреть файл, не нужно выгружать его себе на компьютер – просмотр осуществляется прямо в системе, с помощью обычного браузера. Таким образом, необходимость в пересылках и скачиваниях файла отпадает полностью. (Единственная инстанция, которой необходимо будет выкачать макет из системы — это непосредственно типография, если речь идет о полиграфических макетах). Варианты, адаптированные для разных филиалов, дизайнер также выкладывает в общий доступ. После внедрения Web-to-Print функции адаптации макетов вообще снимут с дизайнера – менеджеры на местах будут сами работать с готовым шаблоном, подставляя в него свои данные. В феврале 2012 в банке планируется протестировать еще два кейса по применению BrandMaker: организовать на базе «Медиа Ресурсов» федеральную систему мониторинга активности конкурентов, а на базе Web-to-Print – механизм для генерирования персонализированных писем. Что из этого будет получаться – обязательно расскажем. Спасибо за внимание!\n","id":10}
{"Host":"https://habr.com","Path":"/en/post/466765/?mobile=no","Text":"Разработка монолитной Unix подобной OS — Системный журнал ядра (3) / Habr                       \n\n09-09-2019\nВ предыдущей второй по счету статье мы с вами разработали необходимые функции для работы со строками из библиотеки С. В этом уроке мы реализуем полноценный отладочный вывод на экран — системный журнал ядра. Оглавление Система сборки (make, gcc, gas). Первоначальная загрузка (multiboot). Запуск (qemu). Библиотека C (strcpy, memcpy, strext). Библиотека C (sprintf, strcpy, strcmp, strtok, va_list ...). Сборка библиотеки в режиме ядра и в режиме пользовательского приложения. Системный журнал ядра. Видеопамять. Вывод на терминал (kprintf, kpanic, kassert). Динамическая память, куча (kmalloc, kfree). Организация памяти и обработка прерываний (GDT, IDT, PIC, syscall). Исключения. Виртуальная память (каталог страниц и таблица страниц). Процесс. Планировщик. Многозадачность. Системные вызовы (kill, exit, ps). Файловая система ядра (initrd), elf и его внутренности. Системные вызовы (exec). Драйверы символьных устройств. Системные вызовы (ioctl, fopen, fread, fwrite). Библиотека C (fopen, fclose, fprintf, fscanf). Оболочка как полноценная программа для ядра. Пользовательский режим защиты (ring3). Сегмент состояния задачи (tss). Системный журнал ядра Перед тем как начать, нам понадобится ввести несколько полезных функций для работы с портами ввода-вывода. Порты ввода-вывода для программиста ничем не отличаются от обычных ячеек в памяти, за исключением того что для оперирования ими существуют отдельные команды. Устройства, оперирующие этими портами, подключены к шине памяти. Также для них существует выделенное адресное пространство. Нам потребуется две ассемблерные функции для работы с портами ввода-вывода, ведь как вы уже помните, я не терплю ассемблерных вставок. extern u_char asm_read_port(u_char port); extern void asm_write_port(u_int port, u_char data); Аналогично две команды для управления маскируемыми прерываниями процессора. extern void asm_lock(); extern void asm_unlock(); Ну и для экономии электроэнергии после неисправимых ошибок нужна команда остановки процессора. extern void asm_hlt(); Как ты помнишь, видеопамять начинается по адресу 0xB8000, но я предлагаю писать сообщения сначала в буфер в обычном текстовом формате. А потом просто этот буфер копировать в видеопамять с учетом аттрибутов цвета. Для этого я реализовал несколько утилит для работы с тамими буферами. Один буфер будет для системного журнала ядра, и остальные для виртуальных терминалов. Прокрутка экрана также будет выполняться над буфером. И только функция video_flush будет копировать буффер в видеопамять, расширяя его аттрибутами. extern void video_init(); extern void video_disable_cursor(); extern void* video_scroll(char const* video_buff, char* pos); extern char* video_clear(char const* video_buff); extern void video_flush(char const* video_buff); Теперь самое время ввести самые часто используемые функции. Две последних будут использоваться для отладки ядра, когда лень отлаживать дебаггером. Поверь, я не разу не юзал дебаггер когда писал это ядро. extern void kpanic(char* message, ...); extern void kassert(const char* file, u_int line, bool expr); extern void kunreachable(const char* file, u_int line); Ну и собственно, функции для работы с системным журналом ядра. Для того чтобы управлять тем, что на экран выводится, системный журнал или пользовательская консоль я ввел функцию kmode. А для чтения системного журнала в буффер нужна будет функция klog, ибо у пользовательских процессов не будет доступа к ядру кроме как через системные вызовы. extern void kclear(); extern void kprintf(const char* format, ...); extern void kvprintf(const char* format, va_list list); extern void kmode(bool is_early); extern void klog(char* buf, u_int n); Наиболее интересные функции привожу тут: /* * Api - Scroll video buffer up * Returns new position */ extern void* video_scroll(char const* video_buff, char* pos) { char* ptr = (void*)video_buff; /* scroll up */ for (int i = 1; i < VIDEO_SCREEN_HEIGHT; ++i) { for (int j = 0; j < VIDEO_SCREEN_WIDTH; ++j) { ptr[(i - 1) * VIDEO_SCREEN_WIDTH + j] = ptr[i * VIDEO_SCREEN_WIDTH + j]; } } /* empty last line */ for (int j = 0; j < VIDEO_SCREEN_WIDTH; ++j) { ptr[(VIDEO_SCREEN_HEIGHT - 1) * VIDEO_SCREEN_WIDTH + j] = ' '; } /* move position up */ pos -= VIDEO_SCREEN_WIDTH; return pos; } /* * Api - Print kernel message */ extern void kvprintf(const char* format, va_list list) { char buff[VIDEO_SCREEN_WIDTH]; int len = vsprintf(buff, format, list); for (int i = 0; i < len; ++i) { if (buff[i] != '\\n') { kputc(buff[i]); } else { int line_pos = (syslog_pos - syslog) % VIDEO_SCREEN_WIDTH; for (int j = 0; j < VIDEO_SCREEN_WIDTH - line_pos; ++j) { kputc(' '); } } } kflush(); } /* * Put character to syslog */ static void kputc(char ch) { if ((size_t)syslog_pos - (size_t)syslog + 1 < VIDEO_SCREEN_SIZE) { *syslog_pos++ = ch; } else { syslog_pos = video_scroll(syslog, syslog_pos); kputc(ch); } } Подробный туториал смотри в видеоуроке. Ссылки → Видеоурок к этой статье → Исходный код (тебе нужна ветка lesson3) Список литературы James Molloy. Roll your own toy UNIX-clone OS. Зубков. Ассемблер для DOS, Windows, Unix Калашников. Ассемблер — это просто! Таненбаум. Операционные системы. Реализация и разработка. Роберт Лав. Ядро Linux. Описание процесса разработки.\n","id":11}
{"Host":"https://habr.com","Path":"/ru/post/241229/","Text":"habr.com\n== Pythonista. Пишем на Python для iOS\n\n\n22 окт 2014 в 15:31  Планшет iPad от компании Apple — всем известное и не нуждающееся в рекламе устройство. Но очень часто хочется использовать всю мощь этого устройства не только для игр и развлечения, а для серьёзной работы. Например для написания программ. Несмотря на 4-x летнею историю развития этого гаджета и наличие разных моделей удобных сред для программирования под iOS существует крайне мало. (Оговорюсь сразу, во избежании дальнейшей путаницы: программирование **на** iOS — значит написание кода и запуск программы на iPad или iPhone, а программирование **для** iOS — написание приложения, которое может быть выложено в App Store.)\n\n Недавно я наткнулся на великолепную программку [Pythonista](https://itunes.apple.com/ru/app/pythonista/id528579881?mt=8), которая позволяет писать **на** iOS **для** iOS.Краткое описание\n\n Как пишут создатели этой программы:> Pythonista brings the Zen of Python to your iPad or iPhone.\n\nИ это действительно так. Программа является лучшим компилятором для Python. \n\n На мой взгляд, лучшим это приложение делают 3 вещи:\n- Не нужно интернет-подключение для запуска программы. iPad действительно становиться рабочей станцией;\n- Есть всплывающие подсказки и встроенная документация (опять же без доступа к интернету);\n- И, конечно, самое главное, это возможность экспорта в XCode.\nОбзор\n\n Среда ориентирована на Python 2.7. Но есть и некоторые фишки из 3-й ветки. Например, сработает и такой код:\n\n print \"Hello, world\"\n и код print (\"Hello, world\")\n\n\n Кроме стандартных библиотек, есть [несколько библиотек](http://omz-software.com/pythonista/docs/ios/index.html) для непосредственной разработки для iOS. Остановлюсь на одной. Она называется **ui** и отвечает за GUI.\n\n Рассмотрим несколько примеров работы с этой библиотекой. Очень интересно, что в Pythonista графический интерфейс можно задавать программно, а можно нативно:\n\n import ui\n\ndef button_tapped(sender):\n    sender.title = 'Hello'\n\nview = ui.View()                                      # [1]\nview.name = 'Demo'                                    # [2]\nview.background_color = 'white'                       # [3]\nbutton = ui.Button(title='Tap me!')                   # [4]\nbutton.center = (view.width * 0.5, view.height * 0.5) # [5]\nbutton.action = button_tapped                         # [6]\nview.add_subview(button)                              # [7]\nview.present('sheet')                                 # [8]\n\n\n Это первый пример по работе с библиотекой ui. Разберём программу построчно:\n\n 1) Сначала создаём объект View; 2) Потом задаём имя этого объекта, оно будет отображаться в его заголовке; 3) Устанавливаем цвет фона объекта — белый, можно задавать словом, а можно при помощи RGB; 4) Создаём кнопку с надписью «Tap me!»; 5) Размещаем кнопку на объекте; 6) Задаём функцию, которая будет выполняться при нажатии на кнопку. (В данном случае изменится надпись на кнопке); 7) Уточняем, что «button» является наследником «view»; 8) Наконец, вызываем метод view.present() для отображения объекта на экране iOS устройства.\n\n Вот что будет происходить на айпаде:\n\n Но тоже самое можно сделать и нативно:\n\n 1) Создадим скрипт с UI:\n\n 2) Открыв UI, нажмем на кнопку \"+\" и выберем button:\n\n 3) Растянем кнопку и разместим её по центру экрана:\n\n 4) Откроем атрибуты кнопки и зададим функцию, срабатывающую при её нажатии:\n\n 4) Перейдём в редактор скрипта и напишем функцию:\n\n def button_tapped(sender):\n    sender.title = 'Hello'\n Скажем, к какому UI привязать данный скрипт:\n\n ui.load_view('My UI').present('sheet')\n\n\n 5) Запустим программу:В завершение\n\n В завершение хочу сказать, что представленный мною обзор далеко не полон и не раскрывает всех функций этой программы. Множество примеров, отличное описание библиотек — всё это позволит довольно быстро разобраться во всех свойствах этого приложения.\n\n Рекомендую посетить [сайт](http://omz-software.com/pythonista/) создателей Pythonista. На нём есть [документация ](http://omz-software.com/pythonista/docs/), к сожалению, только на английском языке.\n\n **UPD:** Читайте мою статью посвящённую этой программе и автоматизации iOS в февральском номере журнала ][акер","id":12}
{"Host":"https://habr.com","Path":"/en/company/plarium/blog/476890/?mobile=no","Text":"Для тех, кто работает в Houdini. О курсах Nature of Vex и Bites of Python / Habr             \n\n21-11-2019\nПод катом вы найдете отзыв специалистов из Houdini Team краснодарской студии Plarium о видеокурсах Nature of Vex и Bites of Python от Mix Training, посвященных работе с языками Python и Vex в графической программе Houdini. Также в этом посте ребята делятся подборкой материалов, которые будут полезны всем интересующимся. Немного вводной Языком Vex пугают начинающих пользователей Houdini. Во многом благодаря ему сложился стереотип о том, что в Houdini обязательно нужно кодить. На самом деле в Houdini можно кодить, и это как раз облегчает и ускоряет многие процессы, а не усложняет их. Например, помогает избежать вот таких жутковатых сетапов: Язык Vex создавался для написания шейдеров в рендере Mantra (встроенный рендер программы Houdini), но довольно быстро вышел за пределы изначального использования благодаря своей гибкости, простоте и скорости. Название языка происходит от сокращения Vector EXpressions, но с помощью него можно манипулировать совершенно разными типами данных. Так, Vex в основном используется для различного рода манипуляций составляющими геометрии (точками, полигонами), а также для процедурного создания геометрии. Язык Vex довольно нетребователен к синтаксису и форматированию кода, обладает не очень высоким порогом вхождения. Зачастую достаточно пары-тройки строчек, чтобы достичь желаемого результата. Среди его плюсов также многопоточность и, как следствие, хорошая скорость. Программирование на Vex нужно как для решения элементарных задач, так и для комплексных и сложных вычислений, и со всем этим язык справляется крайне быстро. С его помощью можно делать массу потрясающих вещей в процедурном моделинге, в анимации и симуляциях. Конечно, нам нравится, когда кто-то думает, что мы такие все из себя программеры, но на самом деле мы привыкли к функциональности и удобству (хотя многие, работая в Houdini первый раз, могут решить, что удобнее только спать на гвоздях). Если бы какой-то инструмент не делал нашу жизнь проще, мы бы им не пользовались. Поэтому не стоит воспринимать возможность программирования как то, что мешает начать изучать Houdini. Vex — это всего лишь еще один (пусть и очень хороший) инструмент среди множества других. Python, гораздо более известный в широких кругах, в представлении и подробном описании не нуждается. Расскажем, зачем он нам. В контексте Houdini Python используется для управления самой программой (создание нод в проекте, операции с файлами, автоматизация повторяющихся операций, воспроизведение сложных комбинаций действий и т. д.). Также программирование на Python нам нужно для создания красивых интерфейсов в инструментах и написания удобных команд, управляющих ассетами при нажатии на кнопку. Если бы в Houdini-ассете существовала кнопка «сделать красиво», она была бы написана на Python. Кроме того, иногда он применяется для манипуляции геометрией (как и язык Vex), но необходимо понимать, что Python менее интуитивен в настройке для таких целей и зачастую справляется с этой работой медленнее, чем Vex. Подробнее о курсах Разработчик Houdini — компания Side Effects Software — выпускает так много обновлений и предоставляет столько возможностей для пользователей, что официальная документация и официальные обучающие курсы просто не успевают актуализироваться. Поэтому мы по крупицам собираем информацию из разных источников (платных, бесплатных, официальных и не очень), чтобы в наиболее полном объеме овладеть этими гибкими и мощными инструментами — языками программирования Vex и Python (да и Houdini в целом). Наш выбор пал на курсы от Mix Training, так как они претендовали на широкий охват материала о Python и Vex в Houdini. У автора курсов есть канал на YouTube (неплохой ресурс для желающих начать обучение Houdini), отличающийся неформальной расслабленной подачей и большим количеством тем, от моушен-дизайна до геймдева. Помимо канала у него еще своя гаражная death-metal-группа. Мы решили, что автору стоит доверять, и приобрели Nature of Vex и Bites of Python, по 8 часов каждый курс (можно смотреть на скорости 1,5). Плюсы Полезность для специалистов разного уровня. Эти курсы можно сравнить с библиотекой, в которой лежат все самые важные аспекты Vex и Python в Houdini, от элементарных вещей до продвинутых и сложных сетапов. В Vex — от определения атрибутов и переменных до оригинальной реализации алгоритма Space Colonization. В Python — от простого автоматического создания нод в сцене и маленьких улучшений в самой программе Houdini до написанного с нуля менеджера атрибутов. Есть вся необходимая базовая информация по синтаксису этих двух языков и об их взаимодействии с Houdini. В курсе много всего для начинающих, но это нас совершенно не смутило. Пересматривая видеоуроки или перечитывая статьи о базовых вещах в Houdini, находишь что-то новое и по-новому понимаешь уже известное. Кроме того, в Houdini почти все можно делать разными способами, формируя со временем свой уникальный стиль, поэтому за работой мастера всегда наблюдать ценно и интересно. Даже то, как организованы ноды в проекте, может многое сказать о его создателе. Актуальность. Обширные и фундаментальные курсы редко бывают современными. Многие из них не угнались за развитием программы Houdini, которая за последние три года довольно сильно изменилась. На смену устоявшимся подходам пришли новые, более оптимизированные и удобные (старые никуда не делись, но перестали быть предпочтительными). В частности, увеличилась доля языка Vex в работе с Houdini. Изучая основы Houdini, важно знать, какие методы актуальны, чтобы, сталкиваясь с более старым (и зачастую более сложным) учебным материалом, понимать, как эффективно применить получаемую информацию на практике. И минусы… В курсах нет готовых решений для реального продакшена. Автор выбирает темы уроков и способы решения задач скорее для демонстрации возможного, чем для получения оптимизированного конечного результата. Эти решения не всегда самые эффективные, и не все из них подходят под определение «лучших практик». Если вы ищете пошаговую инструкцию, охватывающую все стадии производства от начала до финального рендера (как здесь, например), то эти курсы не совсем для вас. Автор предпочитает оставлять финал открытым, что может немного обескуражить начинающих пользователей Houdini. Побочные эффекты неформальной подачи и импровизации. Автор иногда делает ошибки (что может быть и плюсом) или тратит время урока на то, чтобы что-то вспомнить или сконцентрироваться. Учитывая, что информация в курсах во многом ознакомительного характера из-за широты охвата материала, возможности подробно останавливаться на каких-то моментах нет. Из-за этого заминки автора и его спонтанные решения могут вызывать еще больше вопросов. К счастью, у него есть бесплатные уроки о создании менеджера проектов в Houdini с помощью Python, и в некоторых аспектах они более практичны и подробны, чем информация по той же теме в курсах. На наш взгляд, плюсы сильно перевешивают минусы. Если у вас есть желание более или менее систематизированно изучать возможности программирования в Houdini (и саму Houdini), то с этих видеуроков можно начать. Они также будут хорошим дополнением к другим обучающим материалам и ресурсам — как обзор основных аспектов использования Vex и Python в Houdini или видеосправочник, в котором можно быстро что-то подсмотреть. Бонус: несколько вдохновляющих и обучающих ссылок Entagma — GreyScaleGorilla в мире Houdini (пользователи Cinema4d нас поймут). Очень широкий охват тем и отличная подача материала. Кстати, совсем недавно у них стартовал новый сезон. Simon Holmedal — человек-легенда в houdini-комьюнити. Он больше про вдохновение, чем про конкретные практические приемы. Вспомните о нем, если будет нужно увидеть и почувствовать, что можно делать в Houdini. Ben Watts — отличный дизайнер и преподаватель. Matt Estela — автор одного из самых значимых и популярных ресурсов для обучения Houdini — cgwiki. Ресурс, обновляющийся регулярно, просто ломится от количества полезной информации и готовых решений. Однозначно рекомендуем. Anastasia Opara — наша соотечественница, автор знакомого многим великолепного курса для Houdini Procedural Lake Houses. Осилить его полностью с первого и даже со второго раза вряд ли получится, но бросать точно не стоит: так много информации о продвинутых практиках применения Vex и процедурном моделинге найти трудно. Для вдохновения рекомендуем ознакомиться с презентацией автора Believability in Procedural Modelling. Houdini по-русски — канал с очень качественными уроками по Houdini на русском языке. Настолько качественными, что некоторые англоязычные пользователи даже хотели бы выучить русский, чтобы иметь возможность эти уроки смотреть. Учебные материалы разделены по плейлистам в зависимости от уровня сложности.\n","id":13}
{"Host":"https://habr.com","Path":"/en/company/croc/blog/474714/?mobile=no","Text":"Особенности национального ритейла, или Как я проверил на прочность французские технологии / Habr            \n\n12-11-2019\nНикто не будет спорить, что новые технологии сначала появляются и обкатываются в американском ритейле. Если Amazon и Walmart начинают что-то тестировать и внедрять, чаще всего это значит, что через пару-тройку лет у покупателей в российских торговых сетях будет аналогичный сервис. Российских — но не европейских: там ситуация с ИТ в ритейле совсем иная. Почему я так решил, расскажу. В сентябре ездил во Францию — заглянул на ключевую ритейл-выставку в Париж и в самый инновационный магазин одной крупной европейской сети. За несколько дней изучил разработки 100500 компаний, в основном французских, среди которых много стартапов. Темы все знакомые — использование данных, digital-маркетинг, управление цепочками поставок, омниканальность и маркетплейсы. Ожидал увидеть прорывные новинки, а встретил — местные ремейки. У многих мировых вендоров всё это давно есть в продуктовых линейках. Но, может, французские аналоги — качественнее, инновационнее, кастомнее? Чтобы разобраться, решил провести краш-тест этих поделок. №1. Управляемые полки Поехал на выставку. На умной полке лежат товары, а система их опознает и учитывает. Задумка классная, очень упрощает управление запасами, например, булок. На мой интерес: «Видеоаналитика? Компьютерное зрение? Машинное обучение?» — разработчики замотали головами. Сила тяжести! Наличие разных булок на полке проверяют по массе, то есть, по сути, это не полка — это весы. Беру товар, система измеряет оставшуюся массу и определяет, чем решил угоститься Retail_Tsar. Я решил представить себя французом после посещения магазина с умной витриной, взял с полки пирожок — а потом передумал и положил его назад. Но в другое место. Редкий покупатель будет заморачиваться и возвращать товар ровно в ту точку, откуда взял. Оказалось, что это не одни весы, а несколько, информация с которых неким образом агрегируется. Простая перестановка товаров в рамках одной категории — и система не смогла посчитать и определить вид пирожка. А уж когда я выстроил их поперек полки, а не один за другим, умная полка сбилась окончательно и, возможно, отправила разработчикам просьбу прогнать этого русского. Идея интересная, реализация вялая. № 2. Умный POS Автоматическая касса, которая распознает товары на подносе, — привлекательное решение для небольших кафе, нацеленных на ланчи офисных сотрудников. Обслуживать их побыстрее было бы здорово. Техническая начинка — уже поинтереснее: система распознавания, связанная с кассовой системой и — куда без них — с весами. Разработчики мне бодро рассказывали, что POS с легкостью отличает чашку кофе от круассана, а тот — от бутерброда. По плану система будет строить 3D-модель круассана, и если ты его обкусал или порвал, то поймет, что это было, даже если часть уже в животе. Ассортимент бизнес-ланчей обычно небольшой, зашить один раз информацию о продукции, конечно, потребует некоторого времени, но зато какой результат! Я положил круассан набок. Система икнула и отказалась распознавать такое святотатство. Прикрыл круассан бутербродом — аналогично. А уж когда я закопал под ними булочку, система потеряла вообще все. — Месье, — говорю, — у вас явные проблемы с распознаванием. — Нет-нет, — отвечают разработчики, — это просто система еще не научена на эти конкретные булки, мы их утром купили. Вот к вечеру она их запомнит, еще и инженер наш приедет, проконтролирует, вот как раз тогда и приходите. Представил, как в обед так отвечают в кафе, загрустил. №3. Умная витрина В топовом инновационном Парижском магазине. Умная витрина для вина, которую, как мне сказали, будут тиражировать на всю сеть. Изящная этажерка прикручена к стене, множество отсеков для бутылок. Хочешь сегодня французское, нажимаешь кнопку — подсвечивается. Хочешь полусухое — пожалуйста. Хочешь водку…, а нет у них водки! Думаю, вы уже догадываетесь, что было дальше. Я сказал «ага» и пошел переставлять бутылки. Но подсвечиваемые секции от этого не изменились: расположение в них было захардкорено. То есть рядом требуется сотрудник, который будет следить, чтобы все стояло строго на своих местах? И под чьим внимательным взглядом покупатели будут возвращать бутылку в строго отведенную секцию? Еще, не дай боже, кончится товар: пока не поколдует электрик, будет светиться пустое поле? Мои сопровождающие улыбнулись: «Димитри, лучше посмотрите наш проекционный стол». Стол прекрасный, белый, на него выводится проекция. Красота: ставишь бутылку, и перед тобой всплывает информация об урожае, цене, регионе, еще какие-то подробности. Понятно, что для массового сегмента или магазина у дома такая история — дорогая и бессмысленная, зато какой необычный клиентский опыт получат посетители алкогольных бутиков! Беру бутылку, ставлю. Ничего не происходит. Поворачиваю другой стороной. Аналогично. Ну, может, конкретно эта бутылка — не особо удачный урожай, не хочет стол отображать о нем данные. Ставлю другую — тот же результат. Проверяю, включен ли стол. Ларчик просто открывался: оказалось, работает система только на продукции определенного поставщика. Допустим, хотя решение странновато. Тогда покажите, где найти правильную продукцию. Но и это шампанское было встречено сначала игнором, а потом высветилась неверная информация. Хорош юзер экспириенс, 20 минут прикладываю к столу разные бутылки безо всякого результата. За это время опытный консультант мне бы уже продал их все, и мы бы уже выбирали сыр. *** Я не хочу сказать, что ни одна современная французская разработка в ритейле меня не впечатляет. Есть перспективные: например, автоматизированная подсобка-куб, которая будет очень удобна для небольших обувных магазинов. Система автоматических стеллажей, которая по запросу выдает нужную модель нужного размера и следит, чтобы консультанты убирали коробку на место, что позволит здорово сэкономить место и ускорить поиск товара. Другое решение – мобильное приложение дополненной реальности, которое позволяет «примерить», например, вендинговый автомат к пространству в магазине: встанет-не встанет, останется ли нормальный проход, как будет сочетаться с окружением… Привязка элементов дополненной реальности не к QR-коду или другой метке, как мы привыкли, а к изображению — например, к рекламной листовке или флаеру. Но все это, кроме куба — технологии, которые уже есть у нас и хорошо исследованы. Российский ритейл подхватывает их быстрее и более жаден до инноваций, чем традиционный французский — и европейский в целом. Легкий национальный колорит — распознавание круассанов или вюрстов — ничего принципиально не меняет. Дело в некой консервативности европейских покупателей, которые не пушат ритейлеров внедрять что-то новенькое, а годами ходят в один и тот же магазин. А магазины иногда даже не знают, что умеют: когда я был с семьей на Кипре и попытался расплатиться за коктейль телефоном, собралась целая толпа желающих на это посмотреть. Пришлось покупать несколько, чтобы продавцы поверили: их POS действительно умеет творить такие чудеса. Российский потребитель в этом плане гораздо избалованнее, ритейлеры — изощреннее, и технологии быстро охватывают всю страну. Зато у французов вино хорошее. Особенно если покупать без нашей торговой наценки. Захотите поговорить о вине и Париже — моя почта DSmirnov@croc.ru\n","id":14}
{"Host":"https://habr.com","Path":"/en/company/delta/blog/461361/?mobile=no","Text":"Больше и мощнее: как мы обеспечили работу нового оборудования в ЦОД MediaTek / Habr            \n\n25-07-2019\nНередко компании сталкиваются с необходимостью установить новое, более мощное оборудование в уже существующих помещениях. Иногда решить эту задачу бывает непросто, но существует ряд стандартных подходов, помогающих выполнить ее. О них мы сегодня расскажем на примере ЦОД Mediatek. Компания MediaTek, всемирно известный производитель микроэлектроники, решила построить новый центр обработки данных в своей штаб-квартире. Как обычно, проект нужно было реализовать в кратчайшие сроки, а также обеспечить совместимость нового решения со всем существующим оборудованием. Кроме этого, средства электропитания и охлаждения нужно было изначально адаптировать к условиям здания, в котором должен был начать работу новый ЦОД. От CIO компании поступил запрос на технологии автоматизации и мониторинга ЦОД, а также заказчиком приветствовалось внедрение энергоэффективных решений в области охлаждения и энергоснабжения. То есть на эти технологии был выделен дополнительный бюджет, что и позволило создать действительно высокопроизводительный ЦОД в заданных условиях. Большая нагрузка Прежде чем приступать к проекту, следовало хорошо изучить особенности размещаемого оборудования — а оно было действительно мощным. В новом ЦОД планировалось установить 80 стоек, причем некоторые подразумевали размещение нагрузки в 25 кВт. Было проведено моделирование размещения нагрузки и анализ возможных схем охлаждения, после чего принято решено разделить ЦОД на функциональные зоны. Зона высокой нагрузки, где размещается самое мощное оборудование, была отделена, а для охлаждения и электропитания в ней решено установить наиболее мощные и технологичные системы, включая внутрирядные кондиционеры RowCool. Зона со средней плотностью размещения, куда попали в основном сетевое коммутационное оборудование, системы хранения и вспомогательные серверы, также располагалась отдельно. Учитывая меньшее выделение энергии стойками, здесь удалось создать более длинный “горячий коридор”, а значит — сэкономить полезную площадь. Мы провели симуляцию движения воздуха и оценили допустимые параметры температуры для обеих зон, рассчитали мощность оборудования и допустимые размеры коридоров, а также параметры размещения оборудования в стойках. Симуляция движения воздуха помогла найти оптимальные точки для размещения внутрирядных кондиционеров RowCool, чтобы совместное использование активного охлаждения и системы разделения горячих и холодных коридоров давало максимальный эффект. Были спроектированы и установлены модульные системы разделения нагрузки для обеих зон. В результате зона с высокой нагрузкой получила более короткие коридоры и большее количество кондиционеров RowCool, чем зона средней нагрузки. Рядные кондиционеры были подключены к чиллерам при помощи водяного охлаждения. Чтобы обеспечить безопасность такой системы, в ЦОД были установлены десятки сенсоров, а также определены зоны обнаружения возможных протечек жидкости. В случае появления хоть одной капли воды система сразу же выдает уведомление и помогает исправить ситуацию. Более того, кондиционеры RowCool, расположенные в зоне высокой нагрузки, соединены в группы, а между ними настроено автономное взаимодействие. Это сделано для того, чтобы в случае выхода одного кондиционера из строя другие могли усилить свою работу и обеспечить достаточное охлаждение с учетом работы “холодного коридора”, пока кондиционер будут чинить или заменять. Для этого рядные кондиционеры также установлены по схеме N+1. ИБП и распределение электропитания Опираясь на проверенную практику, мы разместили резервные аккумуляторы, а также системы ИБП в отдельной зоне, чтобы потоки воздуха не смешивались и системы охлаждения не теряли мощность на тех нагрузках, которым особо и не требуется дополнительный холод. Учитывая, что суммарная мощность всего ЦОД превышает 1500 кВт, инфраструктуру питания и зону ИБП нужно было проектировать с особой тщательностью. Модульные ИБП были установлены с учетом резервирования по схеме N+1, а для каждой стойки было подведено кольцевое питание — то есть как минимум два кабеля с питанием. В системе мониторинга при этом одновременно отслеживалось энергопотребление, напряжение и ток, чтобы моментально заметить любое нештатное изменение. В зоне с высокой нагрузкой кабинеты распределения питания (PDU) были установлены с задней стороны от стоек Delta, а сверху были размещены дополнительные модули распределения на 60А. В зоне со средней нагрузкой удалось обойтись кабинетами распределения, установленными над стойками. Такой подход позволил сэкономить средства без ущерба для качества. Управление и DCIM В новом ЦОД были реализованы системы управления работой оборудования. Так, через систему DCIM InfraSuite можно отследить все оборудование и место его расположения в ЦОД, а также все параметры электропитания для каждой отдельно взятой стойки. В каждой стойке также был установлен датчик с индикатором EnviroProbe, данные с которых собираются на концентраторах EnviroStation по каждому ряду и передаются на центральный сервер управления. Благодаря этому диспетчеры ЦОД могут постоянно контролировать параметры температуры воздуха и влажности в каждой стойке. Кроме контроля электропитания система InfraSuite также позволяет планировать заполнение ЦОД, потому что в систему внесены данные о количестве и мощности установленного оборудования. Инженеры могут запланировать установку новых серверов или коммутационных систем, одновременно перераспределяя электропитание через интеллектуальные кабинеты PDU. Заключение Практика построения ЦОД для MediaTek была интересна тем, что нам пришлось разместить много высокопроизводительной нагрузки на достаточно небольшой площади. И вместо того чтобы распределять ее по всему помещению, эффективнее оказалось выделить высокомощные серверы в отдельную зону и обустроить там более мощное и технологичное охлаждение. Система комплексного контроля и управления позволяет постоянно следить за энергопотреблением высокомощных серверов, а резервные элементы охлаждения и электропитания помогают не допускать простоев, даже в случае выхода из строя оборудования. Именно такие ЦОД нужно строить под критически важные бизнес-процессы современных компаний.\n","id":15}
{"Host":"https://habr.com","Path":"/ru/companies/cloud_mts/articles/659037/","Text":"Аварийное восстановление и резервное копирование в облаке / Хабр                                                        \n\nРешения _Disaster Recovery_  восстанавливают работоспособность ИТ-инфраструктуры после сбоя или атаки. Объясняем, как использовать вместе с резервным копированием. Материал будет полезен специалистам, которые начинают работу с облаком и только знакомятся с его возможностями.\n## Почему мы об этом говорим ##\nТолько за прошлый год количество кибератак увеличилось более чем в пять раз, однако ситуация набирает обороты. Вероятность крупных утечек увеличивается, и на практике с ними сталкиваются даже крупные ИТ-компании. Как отмечают специалисты из Техасского университета, половина организаций, допустивших потерю данных, прекращает работу в течение двух лет. Для малого бизнеса, который, очевидно, обладает меньшим запасом прочности, эта цифра возрастает до 70%, а срок остановки деятельности сокращается до одного года.\nНо даже если компании удаётся выжить после утечки, она все равно несет репутационные риски. Затронутые аварией партнеры и клиенты начинают меньше доверять продуктам, которые предлагает пострадавший бизнес. Как показывает текущая ситуация, столкнуться с такими проблемами не так и сложно. Дело не только в киберпреступниках, которые постоянно разрабатывают новые инструменты и способы взлома — в том числе на базе систем искусственного интеллекта. Но и в том, что фреймворки вроде ISO 27001, NIST Cybersecurity Framework и другие методологии, связанные с информационной безопасностью, зачастую не успевают за изменениями и теряют актуальность для определенных организаций. Они отдаляются от реальных кейсов, с которыми приходится сталкиваться ИБ-специалистам. Плюс — на внедрение и реализацию подобных практик уходят достаточно ощутимые объемы корпоративных ресурсов.\nПри этом существенная часть компаний, конечно же, считает киберриски главной угрозой для бизнеса и вкладывается в кадры. Ресурсы направляют и на закупку систем киберзащиты. В 2021-м этот рынок оценивали в \\$165 млрд, но аналитики ожидают, что к 2028 году он вырастет до \\$366 млрд. Таким темпам роста способствует разнообразие аппаратных и программных средств борьбы с кражей данных и других атак: от инструментов защиты сетевого периметра  до антивирусного программного обеспечения . Сегодня мы хотели бы поговорить о конкретных решениях.\n## Суть Disaster Recovery ##\nСреднестатистическая компания тратит до двух месяцев на закрытие уязвимости в инфраструктуре. В условиях текущего кризиса это время может увеличиваться, а в результате интенсивных атак — организации с большей вероятностью действуют в спешке и сталкиваются с серьезными сбоями. Минимизировать ущерб помогают бэкапы и решения Disaster Recovery. С их помощью можно восстановить данные и инфраструктуру при взломе, ошибке сотрудника или проблемах с железом.\nНа первый взгляд, эти инструменты решают одну и ту же задачу — позволяют бизнесу продолжить работу, если инфраструктура вышла из строя или из БД пропали данные. Но между ними все же есть отличия.\n**Частота копирования данных** . BaaS подразумевает отправку копий образов виртуальных машин по расписанию — например, раз в день — в холодное хранилище облачного провайдера. Когда происходит потеря данных, они просто восстанавливаются из сохраненной копии. Если говорить о системах Disaster Recovery, то в облаке формируется дополнительная площадка с постоянно обновляемыми репликами виртуальных машин, корпоративных приложений и сервисов — при этом репликация происходит регулярно на протяжении дня. Так, при возникновении сложностей нагрузка переключается на «запасной аэродром».\n**Актуальность данных**  — второе отличие, связанное с предыдущим пунктом. Дело в том, что бэкап подразумевает хранение множественных копий данных, и восстановить их можно до любой сохраненной версии. Но очевидно, что в этот момент часть информации будет утеряна — её объем зависит от расписания резервного копирования, определяющего параметр RPO (recovery point objective). В то же время восстановление данных из холодного хранилища занимает достаточно много времени, которое определяется параметром RTO (recovery time objective). На эту задачу может уйти от нескольких часов до дней — в зависимости от объемов информации. Все это время корпоративные сервисы простаивают.\nЧто касается систем аварийного восстановления, то их показатели RPO и RTO сильно меньше. Репликация проходит в фоновом режиме на протяжении дня, поэтому на резервной площадке всегда находится практически полная копия основной инфраструктуры, а переключение на «запасной аэродром» после ЧП происходит в течение нескольких минут. В результате бизнес не замечает последствий сбоя. Так, среди наших клиентов есть логистическая компания, для которой решение Disaster Recovery сократило время простоя систем на 20%.\n\nОднако стоит заметить, что системы аварийного восстановления не позволяют «откатить» инфраструктуру к более ранним состояниям, так как на резервной площадке находится полная копия приложений и БД. В этом контексте имеет смысл использовать системы резервного копирования и аварийного восстановления в паре, чтобы исключить больший спектр рисков.\n**Отказоустойчивость** . Наконец, резервное копирование не является инструментом обеспечения непрерывной работы. Но аварийное восстановление дает более устойчивый к различным рискам фундамент, дополнительно повышает безопасность инфраструктуры, помогает избежать убытков из-за простоя.\nВ этом ключе работа Disaster Recovery подразумевает два сценария. Первый — когда корпоративные сервисы запущены on-premise, а в качестве резерва выступает облако. Во втором случае роль основной и запасной площадки играет геораспределенная облачная инфраструктура провайдера. Какой вариант выбрать — зависит от нужд организации и необходимого плана восстановления.\n\n## Что еще можно сделать ##\nНачиная работу с бэкапами и системами аварийного восстановления, следует помнить о ряде моментов. Нужно регулярно оценивать работоспособность бэкапов и инфраструктуры на резервных площадках, чтобы не оказаться в ситуации, когда восстановить систему после сбоя невозможно.\nДля этого достаточно проводить регулярные выборочные проверки и тестовые миграции на резервную площадку. Параллельно имеет смысл делать ревизию сохраняемых данных с целью исключить неактуальную информацию. Такой подход позволит сэкономить ресурсы и место на дисках с копиями.\nЭти очевидные рекомендации позволят серьезно повысить отказоустойчивость инфраструктуры. Но к сожалению, следуют им далеко не все — по статистике, больше половины бэкапов находится в неработоспособном состоянии.\nЧтобы повысить надежность инфраструктуры, стоит обращать не только на резервные копии и реплики, но и на сами приложения, которые подлежат защите. Во время разработки эксперты рекомендуют следовать практикам graceful degradation. Они определяют способность системы продолжать работу даже при отказе некоторых ее компонентов — это особенно важно в контексте инфраструктуры корпораций, включающей тысячи взаимосвязанных сервисов.\n## Что запомнить ##\nБэкапы позволяют сохранить копию данных, а затем восстановить в случае утери. Но Disaster Recovery создает работающую копию сервисов и восстанавливает их работоспособность при серьезных сбоях в течение нескольких минут — столько нужно на переключение трафика. Мы в #CloudMTS также предлагаем провести тестовое восстановление , чтобы оценить решение на практике.","id":16}
{"Host":"https://habr.com","Path":"/ru/post/112185/?mobile=no","Text":"Вопросы по продвижению приложений для Android / Хабр               \n\n20-01-2011\nНедавно мой знакомый попросил меня ответить на несколько вопросов по продвижению приложений для Android. Мы договорлись, что я напишу пост. За последний год у меня сложилось определенное впечатление о работе на рынке Android Market. С удовольствием поделюсь своим мнением. Надеюсь, что мой опыт будет полезен разработчикам. Буду рад, если пост породит конструктивную дискуссию. По Андроид Маркету: 1. Как часто выкладывают обновления, есть ли в этих обновлениях какие-то реальные баг-фиксы или пользователи «верят и молча обновляют»? Как пользователь я либо “верю и молча обновляю” приложение, либо не обновляю совсем если мне приложение не интересно. Все люди разные и есть пользователи, которые всегда следят за текущими изменениями в апдейте, мое ИМХО – преобладают люди, которые устанавливают апдейты, вникая в детали редко. Также есть категория приложений, в которых каждый апдейт приносит новые фичи. Как правило, лояльные пользователи такого приложения всегда ожидают от обновления нового функционала или контента. Как разработчик считаю, что приложение нужно обновлять регулярно один раз в одну-две недели. На это есть как минимум две причины: во-первых регулярные апдейты с новыми фичами и контентом положительно влияют на формирование лояльной аудитории проекта, во-вторых при обновлении приложение попадает в категорию 'Just in', что положительно сказывается на количестве закачек. Если приложение имеет позитивные комментарии и высокие оценки объем закачек при апдейте значительно больше. Стоит отметить, что если апдейтить чаще, чем раз в неделю, то скачка по загрузкам не будет, поскольку приложение не будет попадать в ‘Just in’. Пустые апдейты без изменений лучше избегать. Почти всегда (все мы не без греха!) в обновлении стремлюсь сделать какую-нибудь новую фичу или исправить баги. Без багов бывает только hello word, поэтому всегда есть что улучшать/исправлять, особенно, если речь идет о программах для мобильных устройств. Скорее всего, частые пустые обновления могут негативно сказаться на лояльности аудитории и репутации разработчика. Кроме того, насыщенный апдейт, как правило, оказывается весьма эффективным. ИМХО идеальный вариант – заранее девелопить обновления перед выпуском приложения, даже если первая версия окажется слегка урезанной. Как показывает практика, регулярные обновления – это очень важный и эффективный инструмент в продвижении приложений, но чтобы достичь приличных результатов одних апдейтов недостаточно. 2. Какая динамика закачек нужна и за какой период, чтобы попасть в ТОП? Однозначно на этот вопрос ответить сложно, поскольку рынок быстро меняется. На сегодняшний день в TOP Free войти гораздо сложнее, чем в TOP Paid. Мой опыт показывает, что если есть 5-10 закачек в течение одного-двух месяцев, то можно войти в TOP Paid-100 в своей категории. С TOP Free немного сложнее. В разное время я доходил до 30 места в TOP Free в своей категории. Мое самое успешное приложение имеет 700 000 закачек с апреля прошлого года, причем первые 50 000 закачек давались сложно, дальше шло легче. Сейчас эта программа держится в TOP Free 50 в своей категории с динамикой 1000-5000 закачек в день. В выходные закачек больше на 20-30%. 3. Если приложение бесплатно, но в нем есть внутренние платежи, – их обязательно нужно проводить через Google Checkout или они закрывают на это глаза? Внутренние платежи – очень выгодная модель, однако, на Android я ее так и не попробовал. Учитывая подавляющее большинство бесплатных приложений, я опасался, что данная модель будет воспринята ‘в штыки’ пользователями. Слышал, что у PayPal есть своя In-App Purchase библиотека для Android, но опыта работы с ней нет. По вопросам внутренних платежей есть несколько интересных веток на StackOverflow. 4. Чем собирается статистика – закачки, количество пользователей по версиям, что делают в приложении и т.д. Поскольку я стараюсь делать приложения максимально ‘легкими’, я не использую специальной библиотеки для сбора статистики. Статистика в моих приложениях собирается библиотекой для размещения рекламы от Mobclix. В админке у меня есть возможность посмотреть ‘App Opens’ и ‘New Users’ по часам, кроме непосредственно рекламных показателей (impressions, eCPM, eCPC и т.д.). Соглашусь, что сбор статистики – это очень важно, но я пока довольствуюсь ограниченным набором параметров. По продвижению: 5. Работали ли с сайтами обзоров и каталогами? Есть явный лидер, который дает больше всего загрузок? В том случае, если обзор и каталог дает ссылку на приложение в Андроид Маркете, есть ли возможность отследить какой именно ресурс дал эти загрузки на Маркете? С сайтами обзоров работал. Не считаю, что есть явный лидер, вроде TouchArcade для iOS. Думаю, что неплохой эффект будет иметь обзор на Pocket Gamer. Идеальный вариант – в гугле найти как можно больше обзорных сайтов, отсортировать их алексой и разослать всем просьбу сделать ревью. Чем больше обзоров – тем лучше! Отследить количество загрузок с конкретного обзора мне не представляется возможным на данный момент. Также можно заказать баннеры на тематических сайтах. Существует ресурс BuySellAds.com, где можно отфильтровать и отсортировать рекламные места на android сайтах по показам и цене. Кроме того, можно обратиться напрямую к рекламщикам на популярных сайтах. Наверняка, популярные сайты принадлежат медиа-холдингам, с помощью которых можно при желании освоить часть рекламного бюджета. Что касается сторонних, альтернативных каталогов для Android софта, у меня не получилось добиться количества загрузок, сравнимое с Android Market. Основная причина моей неудачи – моя лень. Поскольку каталогов много и я делаю обновления почти каждую неделю, мне очень неудобно сабмитить апдейты регулярно. Поэтому я на каталоги пока забил. Возможно, я просто не вижу достойной альтернативы Android Market. Допускаю мысль, что вернусь к теме каталогов позже. 6. Работали ли Вы с кем-то из рекламных сеток типа Tapjoy.com, Mdotm.com, Smaato.com? Я имею в виду, как рекламодатель. По показам? По загрузкам? Как рекламодатель в основном работал с AdMob и Mobclix. Бюджеты контекстной рекламы были не очень большими, поэтому объективно сравнивать сложно. По моим ощущениям, для бесплатного приложения на 10 кликов приходится один инстал. Опять же многое зависит от содержания рекламы, описания приложения и скриншотов. 7. Размещали ли рекламу в контекстной рекламе Google? Частенько рекламирую приложения с помощью AdMob, $200 долларов расходятся в течение 6-7 часов. По моей статистике на 237 показов приходится один клик (не лучший показатель), за $200 можно получить примерно 5000-10000 кликов и, по моим ощущениям, 500-1000 скачиваний. Возможно, я ошибаюсь в расчетах. 8. Аналогичный вопрос по тем же сеткам – с кем лучше работать на показ чужой рекламы у себя, почему. Как продавец рекламы плотно поработал с AdMob, Quattro Wirelless и Mobclix. Сейчас я использую в основном Mobclix. На сегодняшний день для моей самой популярной программы генерируется в среднем 600-700 тысяч показов в день. Следует отметить, что если программа популярная, то не всегда у рекламной сети хватает рекламы для того, чтобы обслужить все запросы приложения. Если это происходит, то можно потерять до 90% своих доходов. Для подобных случаев существуют решения, которые позволяют обеспечивать максимальное заполнение рекламой приложения за счет автоматической замены рекламных сетей исходя из наличия объявлений. На сегодняшний день я знаю 2 решения, позволяющие это делать автоматически – AdMob+AdWirl и Mobclix. Мне больше нравится Mobclix, поскольку Mobclix собирает со всех рекламных сетей деньги, аккумулирует их у себя и делает один платеж разработчику со всех сетей. В случае с AdWirl с каждой рекламной сети нужно получать деньги отдельно. К плюсам Mobclix также можно отнести возможность использовать AdMob как альтернативу. Outro По итогам 2010 года я убедился, что разрабатывать под Android не только интересно, но и выгодно. На рекламной модели можно зарабатывать и достаточно неплохо. В Android Market сейчас существует много свободных ниш, которые можно и нужно занять. Уверен, что есть достаточно много способов продвижения своего софта для Android. Главное – не бояться экспериментировать! Ни в коем случае не расстраиваться если что-то не получается и никогда не останавливаться. Спасибо. Буду очень рад Вашим комментариям. UPD. В комментариях для сбора статистики приложения советуют использовать Flurry или Google Analytics для Android\n","id":17}
{"Host":"https://habr.com","Path":"/ru/companies/otus/articles/710124/","Text":"Go 1.20 и арена памяти / Хабр                                                        \n\nОдной из революционных особенностей Go в сравнении с другими компилируемыми языками стало автоматическое управление освобождением памяти от неиспользуемых объектов (сборка мусора). В то же время она может привести к потере производительности при передаче контроля процессу управления памятью, но альтернативного механизма в Go представлено не было. Начиная с Go 1.20 появляется поддержка экспериментального решения для управления памятью, которое позволяет совместить безопасное выделение динамической памяти и уменьшить влияние интегрированного в скомпилированный код управления памятью на производительность приложения. В этой статье мы рассмотрим основные аспекты использования Memory Arena в Go 1.20.\n\nДля запуска кода будем использовать актуальную на данный момент версию Go 1.20rc2, которая может быть получена из установочного пакета или через go install  golang.org/dl/go1.20rc2@latest\n\nДля включения поддержки нового механизма управления памятью добавим переменную окружения:\n\n```\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\texport GOEXPERIMENT=arenas\n\n```\n\nтеперь для выделения памяти будем использовать новый модуль arena:\n\n```\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpackage  main\n\nimport \"arena\" type  Person struct {\n  Lastname string\n  Firstname string\n}\n\nfunc main ()  {\n  mem := arena.NewArena()\n  defer  mem.Free()\n\n  for  i:= 0 ; i< 10 ; i++ {\n    obj := arena.NewPerson\n    print (obj)\n  }\n}\n\n```\n\nКак можно увидеть при запуске, адреса объектов будут выделяться последовательно из единой области памяти и (после вызова free) вся выделенная арена будет освобождаться. При правильном использовании это улучшает производительность кода, поскольку для арены не будет вызываться автоматическая сборка мусора. При необходимости копирования данных в обычный heap, может использоваться метод Clone, который создает копию структуры из арены в обычную динамическую память (например, при необходимости возврата результата обработки в основное приложение). Также в арене можно создавать слайсы с указанием начального размера и потенциальной емкости  `arena.MakeSlice(mem, initial, capacity)`  .\n\nДля выделения памяти на основе типа из  `reflect`  также можно использовать новый метод  `reflect.ArenaNew(mem, typ)`  , который возвращает указатель на объект заданного типа, выделенный в арене, сохраненной в mem.\n\nОбнаруживать ошибки при использовании арены (например, чтение или запись значения в структуру после освобождения арены) можно механизмами  `go run -asan`  (Address Sanitizer) или  `go run -msan`  (Memory Sanitizer), например:\n\n```\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpackage  main\n\nimport \"arena\" type  Data struct  {\n  value int32\n}\n\nfunc main ()  {\n  mem := arena.NewArena()\n  v := arena.NewData\n  mem.Free()\n  v.value = 1\n}\n\n```\n\nпри запуске с asan/msan покажет ошибку некорректного использования указателя после освобождения арены.\n\nДля хранения строк в арене можно использовать создание области памяти из последовательности байт и копировать в нее содержимое строки, например так:\n\n```\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsrc := \"original\"\n\nmem := arena.NewArena()\ndefer  mem.Free()\n\nbs := arena.MakeSlice[ byte ](mem, len (src), len (src))\ncopy (bs, src)\nstr := unsafe.String(&bs[ 0 ], len (bs))\n\n```\n\nАрена также может использоваться не только для хранения структур, но и для примитивных типов данных (или их последовательностей), в этом случае взаимодействие ничем не отличается от работы с указателем на переменную:\n\n```\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpackage  main\n\nimport \"arena\" func main ()  {\n  mem := arena.NewArena()\n  defer  mem.Free()\n  v := arena.New int32 \n  *v = 10 println (*v)\n}\n\n```\n\nАналогично поведение слайсов в арене не отличается от обычных слайсов в Go:\n\n```\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpackage  main\n\nimport \"arena\" func main ()  {\n  mem := arena.NewArena()\n  defer  mem.Free()\n  v := arena.MakeSlice[ int32 ](mem, 50 , 100 )\n  v[ 49 ] = 10 ;\n  v = append (v, 20 )\n  println (v[ 49 ])\t\t//10 println (v[ 50 ])\t\t//20 println ( len (v))\t\t//51 println ( cap (v))\t\t//100\n}\n\n```\n\nДля обнаружения утечек памяти при использовании арены можно использовать обычные механизмы профилирования в Go (go tool pprof для визуализации сэмплирования выделения памяти, которое может быть сохранено через функции модуля runtime/pprof). С точки зрения выделения памяти работа с ареной похожа на выделение одного блока памяти (который может увеличиваться в размере) и при освобождении арены все выделенные в ней объекты становятся недоступными.\n\nПовышение производительности можно ожидать в тех случаях, когда приложение интенсивно выделяет память (например, при хранении двоичных деревьев или иных связанных структур данных), но при этом предполагается что выделенные структуры данных являются долгоживущими и существуют до момента освобождения арены целиком (сборщик мусора для арены не применяется и выделенные объекты в последовательной области памяти не очищаются).\n\nСтатья подготовлена в преддверии старта курса  Golang Developer. Professional.  Приглашаю всех на бесплатный вебинар, где руководитель курса проведет собеседование выпускника программы. Реальные вопросы, комментарии по ответам, советы. Будет интересно.\n\nЗарегистрироваться на бесплатный вебинар","id":18}
{"Host":"https://habr.com","Path":"/en/post/239363/?mobile=no","Text":"Интересный способ запуска Selenium тестов на заднем плане / Habr                         \n\n06-10-2014\nВ последнее время у нас на проекте остро стал вопрос о запуске Selenium тестов на заднем плане. Одно из правил в нашей команде гласит, что мы не комитим код, пока не прошли все авто-тесты. Для проекта нашего размера это вполне реально. Проекты у нас меняются каждые 2-3 месяца и обычно занимает от 5 до 15 минут прогнать все тесты. Операционная система, в которой мы работаем — Ubuntu, у всех по два монитора (спасибо заказчику). Поэтому, поглядывая на тесты, удобно поделать что-то еще. Поревьювить таски, например, или начать делать новую задачу. Главная проблема заключается в том, что во время прохождения тестов на компьютере ничего сделать невозможно. А именно, каждое действие веб-драйвера делает его окно активным. Таким образом, можно только нормально послушать музыку. В лучшем случае — почитать или посмотреть видео. А вот если печатать — то почти стопроцентно завалишь какой-нибудь тест. Из-за этой проблемы каждый team member тратил кучу драгоценного времени впустую. Так я начал инвистигейшн данной проблемы. Изначально все советовали перейти с хрома на phantomJs. Мы попробовали это, но одна проблема переросла в другую. Многие элементы перестали находиться на странице. Было замечено еще пару минусов: он медленнее и занимает много памяти в репозитории. И вообще phantomJs не совсем соответствовал требованиям, все-таки Хром самый популярный браузер и нам следовало тестировать на нем. На моем прошлом проекте мы тоже использовали хром драйвер, но поведение у него существенно отличалось. Во время запуска теста хром открывался и его окно становилось активным, но достаточно было кликнуть мышкой один раз в другом месте и после этого можно было нормально продолжать работу. Даже можно было свернуть окно. Тест продолжал работать, скриншоты делались и репорты генерились… Я хотел настроить настроить поведение драйвера в новом проекте именно так. Я задал вопрос по данной проблеме на многих площадках. Написал письма разным экспертам, в том числе и тем, кто комитит непосредственно в Selenium. По сути, мне так никто и не ответил. В основном были предложения использовать phantomJs и настроить локальный дженкинс… Собирая информацию, я пришел к выводу, что такое поведение связано с операционной системой. А именно — на Windows хром забирает фокус только на старте, а в Ubuntu и Mac OS на каждое действие, будь-то клик или проверка значения элемента. Успеха настроить драйвер таким образом, чтобы после запуска он больше не делал окно активным, я так и не добился. Поэтому, если у вас есть решение по этому вопросу, обязательно поделитесь, много людей будут очень благодарны вам. В какой-то момент я совсем отчаялся и подумал, что так и не смогу решить эту проблему. И тут совсем случайно я решил посоветоваться со знакомым из другой команды, и он мне рассказал, что они запускают тесты через vncserver. Он мне обьяснил, как это настроить, а потом в процессе использования я придумал еще несколько деталей, которые облегчили процесс и сделали его более быстрым и удобным. Этим способом я и хочу сегодня с вами поделиться. Немного отклоняясь от темы, хочу сообщить, что мы используем thucydides репорты и поэтому мы видим красивый результат всех тестов после их прохождения. Это делает процесс наблюдения за тестами ненужным. Эта инструкция для Ubuntu и конкретно для java проектов, использующих maven. Кто работает на Mac, думаю, данный подход должен работать, поэтому у кого получится настроить — поделитесь инструкцией, чтобы дополнить статью. Советую вначале полностью прочитать всю инструкцию, а только потом приступать к настройке: 1. В первую очередь нужно установить vncserver, вводим в терминале команду: sudo apt-get install vnc4server 2. Нужно запустить сервер, под каким-то номером дисплея, я выбрал номер 7. Когда вы запускаете сервер в первый раз, вас попросят ввести пароль. Запомните пароль, он нужен будет в будущем для подключения. Вводим: vncserver :7 3. Сервер может запуститься с маленьким разрешением дисплея, с помощью этой команды (сразу без выполнения второго шага) вы можете сами задать нужное разрешение для вас: vncserver :7 -geometry 1920-1080 4. Для подключения к серверу можно использовать любой vncviewer, я предпочитаю Real VNC. Скачиваете, устанавливаете… 5. Открываем viewer. Нажимаем кнопку «начать новое подключение» и вводим в строке сервер: localhost:7, а в строке Encryption: Let VNC Server choose 6. Жмем «подключиться» и вводим пароль, который мы указали в пункте два. 7. Должно открыться окно, дублируещее ваш десктоп. Запустите терминал в этом окне. 8. В моем случае переходим в папку проекта и вводим команду: mvn clean install 9. Тесты запускаются, не мешая остальным программам. Можно спокойно свернуть окно или даже закрыть программу vnc viewer — на прохождение тестов это никак не повлияет. Дополнительные шаги: Если вы желаете запускать тесты без vncviewer, то существует замечательная команда, переходим в папку проекта в теминале (можно и в терминале IDE) и вводим: xterm -display localhost:7 -e mvn clean install Недостаток такого способа в том, что вы не видите вывод консоли, а только окончание процесса. Если же у вас имеются репорты, то для вас это не особо важно. Если вы хотите остановить vncserver, то введите: vncserver -kill :7 После перезагрузки компьютера обычно нужно повторить шаг 2 или 3. А потом запускаете тесты через viewer или напрямую в терминале командой, написанной выше. Как вы уже поняли, этот способ совсем не идеальный, но, несомненно, он намного лучше, чем просто запускать тесты, как мы делали это раньше. Тесты проходят так же стабильно и быстро, как и без vnc сервера. Сейчас для меня самое удобное — это стартовать сервер и сразу напрямую стартовать тесты из консоли IDE. Я также пробовал открывать IDE прямо в vncviewer(для этого вначале нужно ее закрыть в обычном окне). Работать и запускать тесты стандартным способом, по одному, например. Это тоже довольно удобно, особенно если дебажишь один длинный тест и запускаешь его по много раз. Надеюсь, данный материал поможет вам сохранить много времени. Если у вас есть еще способы запуска тестов, не мешая работе, то обязательно поделитесь ими. А также, если у вас есть идеи, как улучшить данный подход. UPD: Совершенно случайно, я нашел причину(или одну из причин) такого поведения вебдрайвера. Наши тесты проходили очень долго и мы приняли решение отключить скриншоты на каждый шаг. Теперь скриншот делается только на зафейлиный шаг, перед тем как тест упадет. И неожиданно тесты начали нормально проходить на заднем плане. Теперь интересно, если тесты со скриншотыми на каждый шаг не дают вебдрайверу работать на заднем плане на Mac OS X и Windows?\n","id":19}
{"Host":"https://habr.com","Path":"/ru/post/111847/?fl=ru,en","Text":"Расширяем возможности StyleCop / Хабр               \n\n18-01-2011\nStyleCop — статический анализатор C# кода на предмет соответствия стилю — был официально представлен публике в начале 2008 года. По IT-меркам это довольно давно, однако этот полезный инструмент почему-то до сих пор не получил широкую популярность (по крайней мере ту, которую заслуживает). Ниже я постараюсь проанализировать причины, а также расскажу о новом плагине к StyleCop. Почему StyleCop не очень популярен? Помимо разработчиков, которых вообще не заботит стиль кодирования (здесь и далее речь пойдет только о языке C#), многих отпугивает слишком большое количество ошибок при настройках по умолчанию. На мой взгляд, это странно — если тебя волнует стиль кодирования, то вполне естественно проверить все настройки, решив для себя, какие из них подходят для соблюдения выбранного стиля. Но здесь возникает вопрос — а что вообще понимается под стилем кодирования? Это какой-то единый стиль для всех, кто пишет на C#? Или каждая команда вольна выбирать его по своему усмотрению? Мне кажется, что сама идея существования «единых» правил — утопична. В целом, есть некоторые тенденции, которых со временем начинают придерживаться все больше и больше разработчиков. Однако язык развивается, добавляются новые конструкции (а значит, и способов их использования становится все больше), и начинается новая волна обсуждений и споров. Тем не менее, Jason Allor — создатель и главный идейный вдохновитель StyleCop — придерживается позиции, что правила стиля должны быть едиными для всех. В итоге эти правила и составляют те самые «настройки по умолчанию», которые часто останавливают тех, кто хотел бы использовать StyleCop. Мне не хотелось бы провоцировать споры о конкретных правилах — в конце концов, предлагаемый StyleCop стиль используется некоторыми командами в Microsoft (а значит имеет определенный «коэффициент доверия»). Однако мой личный опыт показывает, что чаще всего работает принцип «локального стиля» — т.е. свой стиль принимается в команде, в отделе, или во всей организации, а затем неукоснительно соблюдается. Здесь замечу, что если ваши соглашения заключаются только в устных договоренностях — можете считать, что никаких соглашений у вас нет. Принудительная проверка (например, в качестве одного из этапов сборки на build-сервере) — единственный гарант их соблюдения. Тут-то такие инструменты как StyleCop и показывают себя во всей красе. Как можно исправить ситуацию? К счастью, StyleCop имеет довольно развитую систему плагинов, что позволяет существенно расширять его функциональность (даже сам набор оригинальных правил технически оформлен как отдельный плагин). Но плагинов к StyleCop в публичном доступе не так много. А большинство из тех, которые можно найти, представляют собой несколько разрозненных правил и выглядят заброшенными. Мне кажется, что StyleCop был бы более полезен, если бы позиционировался не как инструмент для соблюдения определенного стиля, а стал бы неким «конструктором», с помощью которого можно было бы контролировать соблюдения того стиля, который нужен именно вашей команде. StyleCop+ Примерно с такими мыслями год назад я начал писать StyleCop+ — очередной плагин к StyleCop. Его основная идея была и остается неизменной — не навязывая конкретные правила, предоставить широкий спектр возможных проверок, позволяющих настроить свой собственный стиль. Плагин неспешно писался в свободное время и постепенно обрастал новыми возможностями. В итоге, я решил поделиться своими наработками и начал публиковать его на CodePlex. Положительные отзывы показали, что работа была проделана не зря. Ниже я хотел бы кратко рассказать об основных возможностях StyleCop+. Если вы уже используете StyleCop — они могут показаться вам интересными, если же нет — возможно, именно их вам и не хватало, а может они вдохновят вас на написание собственных расширений. Advanced Naming Rules История возникновения этих правил достаточно банальна — в нашей команде был принят m_field-стиль для полей классов (когда имена приватных полей начинаются с различных префиксов, в частности m_). StyleCop же поддерживает только this.field-стиль (когда приватные поля не имеют префиксов, но любое обращение к полям или методам класса должно сопровождаться обязательным this, чтобы отличать поля от, например, аргументов метода). Более того, StyleCop содержит и правило, запрещающее использование символов подчеркивания в именах вообще. Повторюсь, я не берусь утверждать какой стиль лучше. Но реальность такова, что разные стили уже существуют (даже в .NET BCL можно наблюдать группы классов, написанных разными командами в разное время ­— и с разными стилями именования). А раз так — то инструмент для соблюдения стиля просто обязан учитывать все это многообразие. Таким образом, в StyleCop+ появились правила, позволяющие настроить практически любую систему именования объектов. Функционально они полностью покрывают основные Naming Rules (SA13xx), так что последние можно просто отключить: Способ конфигурирования этих правил во многом похож на аналогичные настройки в ReSharper — вам доступен широкий набор сущностей, для каждой из которых можно задавать свои шаблоны именования. К примеру, следующее правило говорит, что имя должно либо начинаться с префикса m_, после которого следует имя в camelStyle (m_rulesMap), либо быть полностью записано в PascalStyle (RulesMap). Как видно из картинки, в шаблонах именования можно использовать ряд макросов, описывающих наиболее распространенные стили капитализации. Самих сущностей тоже довольно много, причем некоторые из них специально выделены отдельно. К примеру, кто-то может не захотеть переименовывать стандартные обработчики событий WinForms (такие, как buttonStart_Click) — а значит, им могут понадобится свои правила, не те, что для обычных методов. Или же кто-то может по-особому называть методы, являющиеся юнит-тестами (например, Setting_Null_Throws_Exception). А уж необходимость иметь разные правила для имен классов или полей в зависимости от модификаторов доступа и вовсе очевидна. Раз уж в именах проверяется капитализация, то не обойтись и без задания собственного списка аббревиатур (здесь под ними понимаются любые сочетания заглавных букв, которые должны допускаться в качестве отдельного слова). Также недавно появилась опция, позволяющая следить за тем, чтобы имена некоторых производных классов, обязательно заканчивались на имя базового (например, это общепринято для Attribute, Exception, Stream и т.д., но кто-то может захотеть добавить и свои исключения). More Custom Rules Как и другие плагины, StyleCop+ содержит различные дополнительные правила для проверки стиля. Их пока не очень много, но те, которые существуют, довольно гибко конфигурируются (спасибо активным пользователям, предложившим множество интересных идей). Для этих правил был создан отдельный интерфейс, позволяющий более дружелюбно редактировать настройки (в отличие от традиционного интерфейса на вкладке Rules, здесь настройки не ограничены флажками с булевскими значениями и относятся к самому правилу). Extended Original Rules Это экспериментальная возможность, в рамках которой StyleCop+ содержит правила, «паразитирующие» на оригинальных правилах StyleCop. Например, полезное правило SA1509: OpeningCurlyBracketsMustNotBePrecededByBlankLine в том числе запрещает и «встроенные» блоки кода. // some statements ... { int a; // declaration of local variables required to set value of f1 ... this.f1 = ...; } { int a; // declaration of local variables required to set value of f2 ... this.f2 = ...; } Его «правило-паразит» — SP1509 — содержит настройку, позволяющую разрешить такое написание. Ключевой момент состоит в том, что это правило не содержит собственного кода, дублирующего оригинальную проверку. Вместо этого «правило-паразит» запускает оригинальный код из StyleCop, но накладывает свои ограничения на результат (в приведенном примере оно не будет выдавать ошибку, если та вызвана «встроенными» блоками кода). Вместо заключения Если вы еще не используете StyleCop — обязательно попробуйте, он должен стать вашим must-have инструментом! Если вам не хватает функциональности — пишите расширения или ищите существующие! Что касается StyleCop+, он продолжает постепенно развиваться. Скачивайте, пользуйтесь, задавайте вопросы на CodePlex. А уж если так получится, что именно он побудит хотя бы одного разработчика начать пользоваться StyleCop — можно считать, что свою миссию этот проект выполнил на все 100%! Удачи! ____________________ http://stylecop.codeplex.com — StyleCop на CodePlex http://stylecopplus.codeplex.com — StyleCop+ на CodePlex http://blogs.msdn.com/b/sourceanalysis — блог Microsoft StyleCop http://devjitsu.com/blog — блог Jason Allor\n","id":20}
{"Host":"https://habr.com","Path":"/en/company/hpe/blog/264461/?mobile=no","Text":"Аналитики Gartner включили HP в число «провидцев» в своем «Магическом квадранте» по сетевой инфраструктуре ЦОД / Habr                    \n\n10-08-2015\nВ последние годы требования к сетевой инфраструктуре ЦОД стали быстро меняться. Вендоры стараются предоставить заказчикам гибкие сетевые решения с усовершенствованными функциями управления, помогающие сократить издержки, однако, самым важным критерием остаётся надёжность сетевой инфраструктуры центра обработки данных. Сети ЦОД Сегодня при реализации сетевой инфраструктуры дата-центра требуется упростить сетевые операции, увязать их с задачами бизнеса, увеличить гибкость ЦОД за счет программируемости сети и автоматизации. Сеть должна адаптироваться к изменению масштаба дата-центра, плотности оборудования в ЦОД и характера нагрузок. Для совместимости оборудования, уменьшения зависимости от конкретного производителя и упрощения внедрения инноваций сетевые решения должны создаваться на основе открытых стандартов. Сети современных центров обработки данных должны обеспечивать создание упрощенной и автоматизированной сетевой архитектуры. Увеличиваются требования приложений к пропускной способности, задержке в сети. Для соответствия новым требованиям внедряются одно- и двухуровневые архитектуры коммутации, используются интеллектуальные функции на уровне доступа. Это помогает повысить производительность межсерверного обмена трафиком и предоставить всем подключенным вычислительным ресурсам единообразный набор функций. Меняются и потребности в скоростях передачи данных, хотя в большинстве дата-центров пока что отсутствует спрос на 25GbE, 40GbE или 50GbE на уровне доступа к серверам или на 100GbE в ядре сети. В настоящее время в большинстве ЦОД достаточно 10 Gigabit Ethernet (10GbE) на границе сети для подключения серверов и 40GbE в ядре сети. Завоевывают популярность Ethernet-фабрики – одно- или двухуровневые инфраструктуры коммутации. Благодаря упрощению операций, ускоренному выделению ресурсов ИТ и быстрому развертыванию завоевали популярность конвергентная инфраструктура, референсные архитектуры, а также интегрированные системы. По данным Gartner, прошлом году доля конвергентных решений на мировом рынке инфраструктурных решений для ЦОД составляла 7% — на 57% больше, чем годом ранее. При оценке предложений вендоров заказчикам следует обращать особое внимание на «программно-ориентированные» подходы, считают аналитики Gartner. Кроме того, большинство вендоров сетевого оборудования для ЦОД применяют в своих продуктах коммерчески доступные микросхемы и различия между их решениями смещаются на уровень программного обеспечения, что также подчеркивает особую роль ПО. SDN По оценкам Gartner, количество реализаций ЦОД на основе SDN в 2014 году втрое превысило показатели предыдущего года. 2014 год стал годом практической реализации программно-конфигурируемых сетей (SDN). Для данного подхода характерны такие преимущества как быстрое развертывание требуемых ресурсов для рабочих нагрузок, а также усиленная безопасность и усовершенствованное управление. В числе экономических выгод — снижение расходов на сетевое оборудование и ПО, сокращение операционных затрат и уменьшение зависимости от вендора. Иногда функции SDN реализуют с помощью так называемых оверлеев – виртуальных сетей, развертываемых поверх физических сетей. Для надежности оверлейных решений нужны эффективные средства управления, обеспечивающие прозрачность сетевой инфраструктуры. Сегодня целесообразность долгосрочных инвестиций в решения, не поддерживающие SDN, ставится под сомнение. Многие участники исследования Gartner являются членами консорциума OpenDaylight, а некоторые выпустили продукты SDN на основе OpenDaylight. Магический квадрант Магический квадрант по сетевой инфраструктуре ЦОД. Источник: Gartner (май 2015 г.) Компания HP занимает второе место среди ведущих игроков мирового рынка сетевой инфраструктуры ЦОД. Ее отличает присутствие во многих географических регионах, широкий спектр шассийных, модульных коммутаторов и коммутаторов с фиксированной конфигурацией, способный удовлетворить потребности практически любой организации. HP является активным сторонником открытых SDN-архитектур и архитектур с дизагрегированной коммутацией. Компания обеспечивает исчерпывающую поддержку устройств и контроллеров через онлайн-каталог SDN App Store. Онлайн-каталог HP SDN App Store способствует выводу на рынок многочисленных инновационных решений в области программно-конфигурируемых сетей, предоставляет партнерам HP платформу для налаживания связей с заказчиками, а также открывает доступ к услугам консалтинга и технической поддержки. Являясь первой в отрасли экосистемой по продаже приложений SDN корпоративного уровня, HP SDN App Store меняет бизнес-модель сетевой индустрии, предоставляя разработчикам централизованную платформу для связи с заказчиками по всему миру. Заказчики получили возможность находить, изучать и приобретать сетевые приложения и загружать их для тестирования и развертывания. По мнению аналитиков Gartner, компанию HP следует рассматривать в качестве потенциального вендора при реализации любых сетевых решений для дата-центров, особенно, если эти решения основаны на стандартах или являются открытыми. Сильные стороны Компания HP реализует очень широкий и основанный на стандартах подход, отвечающий полному спектру требований заказчиков. Это подтверждается другими исследованиями Gartner, где HP названа «самым открытым» вендором из перечисленных в данном исследовании поставщиков сетевых решений. Программное обеспечение HP Comware обеспечивает поддержку расширенных функций Layer 3, FCoE, фабрик и SDN-сред по одной стандартной цене и может применяться для всего спектра сетевых продуктов компании. HP предлагает полную конвергентную инфраструктуру, включая сетевое оборудование и ПО, серверы и системы хранения данных, а также систему управления HP OneView. Располагая собственным SDN-контроллером, выпущенным в мае 2015 года, HP поддерживает среду OpenDaylight, которая становится все более надежной и масштабируемой, реализует эту поддержку в SDN App Store, предлагая решения на базе OpenDaylight. Однако многочисленные сценарии использования SDN, включая Helion OpenStack с приложением Virtual Cloud Networking, VMware NSX и HP Distributed Cloud Networking (DCN на базе разработок Nuage Networks, входящей в Alcatel-Lucent) и решения HP Networking Virtual Application Networks (VAN), могут дизориентировать заказчиков. Критерии включения в квадрант Вендор оценивается по всему портфелю продуктов и услуг, включая способность строить законченные инфраструктурные решения — архитектуры Ethernet-фабрики, коммутаторы разного уровня и класса, контролеры и приложения SDN, средства управления и оркестрации. Учитываются также стратегии миграции, поддержка виртуализации, решение вопросов масштабируемости. Особое внимание уделяется открытым средам, включая дезагрегирование и SDN. Оценивается также общее финансовое состояние вендора, вероятность продолжения инвестиций в продукты и расширение портфеля решений, рыночная стратегия вендора и его партнеров, взаимодействие с потенциальными заказчиками и способность предложить оптимальное решение. Кроме того, учитывается способность вендора реагировать на рыночную ситуацию, менять стратегию в соответствии с потребностями клиентов и динамикой рынка, а также влияние вендора на рынок в том, что касается ключевых трендов. Вендор должен обладать полнотой видения — понимать будущие условия рынка и быть готовым к ним, предвидеть будущее и воплощать в своих планах и продуктах новые идеи. Лидеры рынка стараются сделать сети ЦОД более гибкими, открытыми, соответствующими меняющимся архитектурам приложений, предложить заказчикам выбор и обеспечить защиту инвестиций. В основе соответствующих стратегий – открытые сети, дезагрегирование, SDN и другие новейшие архитектурные подходы. Принимаются во внимание инвестиции вендора в НИОКР для внедрения ключевых инноваций, включая планы, касающиеся открытый сетей, дезагрегирования, SDN и других новейших архитектурных подходов, способность продвигать инновации для удовлетворения новых требований к сетевой инфраструктуре ЦОД, а также инвестиции вендора в новые технологии, способствующие дальнейшему развитию бизнеса и рынка. Ключевой фактор в сетях ЦОД – инновации вендора в технологических областях, наиболее отвечающих потребностям рынка. Описание квадранта Лидеры демонстрируют готовность соответствовать меняющимся требованиям архитектуры большого числа дата-центров, обладают способностью формировать рынок, поддерживать сильные связи с торговыми партнерами и заказчиками. Претенденты демонстрируют стабильную работу на рынке, имеют четкую стратегию, но не формируют рынок. Провидцы (к которым аналитики отнесли HP) демонстрируют способность к наращиванию функциональности своих предложений, предлагая рынку уникальный подход, который отличает их продукты от конкурентов. Для провидцев характерны инновации в одной или нескольких ключевых областях сетевой инфраструктуры ЦОД, таких как управление, безопасность, эффективность операций и сокращение затрат. Нишевые игроки предлагают широкий спектр продуктов, но не обладают сильными возможностями для их продвижения. Нишевые игроки предлагают конкурентные продукты и в некоторых эти предложения могут быть оптимальным выбором.\n","id":21}
{"Host":"https://habr.com","Path":"/ru/companies/kauri_iot/articles/472430/","Text":"Как мы выбирали компонентную базу для умного дома: о датчиках и контроллере / Хабр                 \n\n_Kauri разрабатывает оборудование для IoT-решений и пишет под него софт. Недавно мы успешно протестировали работу сети Zigbee при помощи отладочной платы и датчиков, поэтому составили небольшой гид._\n\n_Поговорим о критериях для выбора, производителях, ценах, немного поругаем российский рынок._\n\nМы составили для себя следующий список интересных нам датчиков, которые отвечают за индикацию:\n\nПротечек\n\nЗадымления\n\nПроникновения (магнитоконтактный)\n\nДвижения на объекте\n\nРазбитие стёкол\n\nА также датчики, измеряющие:\n\nТемпературу\n\nРасход газа/воды\n\nПотребляемые мощности\n\nВлажность\n\nИз них мы закупили датчики движения/уровня освещенности, вторжения, протечки и температуры/влажности (исходя из решений под умный дом — предотвращение кражи, возгораний, затопления).\n\nСамый простой критерий для выбора — стоимость датчика и его доступность в России. Датчик сам по себе — простое устройство, разные компании выдают примерно одинаковое качество, поэтому мы отталкивались от цены на продукт — чем дешевле, тем лучше. Ведь в конечном итоге за все будет платить клиент, а на большой дом может уйти сотни датчиков. Если за каждый платить по 1000 рублей и больше, выйдет недешево.\n\nЕще один важный критерий — потребление энергии (наши датчики, например, работают от батарейки). Некоторые приборы ставят в труднодоступных местах, постоянная замена элементов питания может стать неприятной проблемой. Мы смотрели датчики, поддерживающую технологию ZigBee, которая является наиболее энергоэффективным стандартом связи. Так, наши датчики могут находиться в спящем режиме, потребляя минимальное количество энергии, и переходить в режим приема-передачи только по необходимости.\n\nА еще ZigBee хоть и обладает невысокой скоростью передачи, но считается вполне надежной, способной самовосстанавливаться сетью, которая легко развертывается и эксплуатируется.\n\nФирм много, мы просто посмотрели список. Лидером выступает Xiaomi — у них дешевый продукт, большой выбор, можно хоть на Aliexpress заказать. Все-таки датчики должны быть легкодоступными и популярными.\n\nСами по себе датчики бесполезны, их надо подключать к сети. Поэтому дальше надо решить, какую элементную базу и зигбишный SoC выбрать для того, чтобы протестировать этот стандарт связи.\n\nМы выбирали между кристаллами Texas Instruments и Silicon Labs. Дальше стали смотреть, как обстоят дела с поддержкой этих микропроцессоров в России. Когда вы начнете разбираться с их работой, могут возникнуть технические вопросы: здесь поможет подробная документация производителя.\n\nТакже есть некоторые компоненты, которые в России тяжело покупаются — нет прямых дистрибьюторов. А это важно: надо иметь возможность заключить партнерское соглашение: вы выбрали кристалл и договорились о том, что будете закупать партию ежемесячно. Соответственно, для вас сделают специальное предложение по ценам и тд. Кстати, некоторые производители могут «подарить» тестовые образцы при регистрации проекта.\n\nМы в итоге обратились к дистрибьюторам Silicon Labs, с ними можно заключить договор, цены адекватные, есть поддержка не только аппаратная, но и программная (всевозможные специализированные библиотеки, DLL — все это облегчает жизнь разработчикам).\n\nДалее нам нужна электронная плата, чтобы работать с микросхемой. Это процесс долгий. И для того, чтобы программисты сразу могли приступить к работе, Silicon Labs предоставляет, в частности, отладочные платы (полный пакет для разработчиков), драйвера.\n\nДанные о работе датчиков собирает контроллер и отправляет на сервер для дальнейшей обработки. Мы сделали собственный модульный Kauri-контроллер, чтобы он мог работать с максимально возможным количеством современных стандартов связей, необходимых для разных отраслей. От логистики, где преимущественно используется LoRa, до ритейла, где нужна технология RFID.\n\nНам также важно, чтобы контроллер работал автономно. И, даже если возникнут проблемы с сетью, дом должен оставаться “умным” и безопасным. Контроллеры Xiaomi, например, не предоставляют такого решения. Они передают всю информацию на сервера в Китай (кстати, туда же может прилетать информация о местоположении жилья).\n\nKauri-контроллер управляет всеми устройствами в локальной сети ZigBee при помощи скриптов на Python. Поэтому, в случае возникновения проблем с интернетом, система продолжит исправно выполнять свои задачи.\n\nСаму “коробочку” для контроллера — корпус — делают множество фирм. При ее выборе лучше отталкиваться от сферы эксплуатации — улица, дом. Если контроллер будет стоять на улице, выбираем IP не меньше 65 (чтобы выдержал дожди). Контроллер для помещений (дом, завод, склад) выбираем с IP40. Если нам нужно герметичное устройство (чтобы работало под водой) — выбираем IP66.\n\nПри выборе корпуса для помещения стоит обратить внимание на его внешний вид, он должен эстетично смотреться в доме. Для контроллера используем процессор семейства iMX 8M, Cortex A53.\n\nМы взяли отладочную плату, датчик, и программист отстроил Zigbee сеть. Далее проверили, появились ли датчики в сети и нормально ли они реагировали. И вот тут мы задумались о безопасности. Сама сеть ZigBee предусматривает криптографическую защиту данных.\n\nОтладочная плата является координатором Zigbee сети. Сеть может быть относительно открытой (происходит обмен ключами безопасности в момент подключения нового датчика) и закрытой (обмена ключами не происходит, то есть в сам датчик вшит нужный ключ безопасности).\n\nНедостаток открытой сети в том, что если кто-то в момент подключения сможет перехватить ключ, то дальше расшифровать всю передаваемую информацию не составить труда. А это особенно опасно, если речь идет о работе завода, например.\n\nВ нашем случае настройки будут задаваться на уровне конфигурации сети, это значит, что клиент сам будет выбирать способ подключения датчиков и тип безопасности.\n\nМы использовали элементную базу иностранного производства. В России, к сожалению, подходящих компонентов не делают. Вся технология ZigBee, микропроцессоры, матрицы — все это принадлежит американцам.\n\nВ России микроэлектроника практически не развита, даже резисторы не делают. В основном отечественные компоненты производят для космической отрасли, и все это стоит безумных денег. И в любом случае это не подходит для наших решений.\n\nЭто была обычная закупка для физического лица (без договоров) в розницу в обычном интернет-магазине. По стоимости вышло так:\n\nОдин датчик движения стоил 989 рублей\n\nДатчик открытия окна и окон — 989 рублей\n\nУтечки воды — 929 рублей\n\nТемпература влажность — 700 рублей\n\nНо мы закупали это для себя в офис, чтобы потестить. На том же Aliexpress эти датчики можно купить в два раза дешевле.","id":22}
{"Host":"https://habr.com","Path":"/en/post/414053/?mobile=no","Text":"5 заблуждений о GDPR / Habr                       \n\n14-06-2018\n1. Основная цель принятия GDPR – усложнить жизнь бизнесу На самом деле главная цель GDPR – дать возможность пользователям контролировать кто и как использует их персональные данные и иметь возможность легко и в любой момент запрещать использование или изменять условия использования персональных данных в маркетинговых целях. Персональные данные собираются компаниями, чтобы улучшить маркетинг и сделать его персонифицированным – нацеленным на каждого конкретного пользователя с учетом его предпочтений и интересов, которые собираются на основе поведения пользователя в интернете: посещение сайтов, оставленные лайки, передвижение мышки по странице. В интернете можно собрать такие данные об активном среднестатистическом пользователе, как: пол, возраст, семейный статус, профессия, интересы, потребительские привычки. Если к этому добавить мониторинг геолокации, то объем информации о каждом человеке, которая находится в руках некоторых компаний, становится пугающим, особенно при мысли об утечке этих данных. Еще страшнее становится при мысли о возможном манипулировании поведением и решениями пользователей – пример новостей, связанных с деятельностью Cambridge Analytica. В некоторых публикациях приводится пример программы, которая на базе анализа всего 10 лайков позволяет узнать человека лучше, чем его знают его сослуживцы. По 70 лайкам программа узнает о человеке столько же, как его близкий друг, по 150 лайкам – как родители, братья или сестры, а по 300 и более лайков — лучше, чем его знает супруг(а). Задумываясь об этом с точки зрения пользователя, можно видеть безусловную пользу в принятии GDPR. Его основная цель – ограничение бесконтрольного использования персональных данных в коммерческих целях, когда субъект таких данных не имеет представления о том, кто, в каких целях и каким образом использует информацию о нем, собранную в интернете из различных источников. 2. GDPR распространяется на российские компании, обрабатывающие персональные данные хотя бы одного гражданина страны-члена ЕС Это тоже заблуждение. GDPR регулирует работу с персональными данными не граждан ЕС, а всех лиц, находящихся на территории ЕС «This Regulation applies to the processing of personal data of data subjects who are in the Union…». GDPR, Art. 3 (2). По утверждению представителя Европейской Комиссии (которому удалось неофициально задать некоторые вопросы на международной Конференции в июне 2018 года [1]) GDPR не относится к обработке персональных данных лиц, которые находятся за пределами ЕС, даже если они выехали временно. В тоже время обработка персональных данных российских граждан, которые путешествуют в Европе, подпадает под действие GDPR. Опять же со ссылкой на неофициальные пояснения представителя Европейской Комиссии для того, чтобы привлечь внимание регулятора, требуется обработка прежде всего большого объема данных европейских пользователей. Если целью российской компании не является сбор или обработка данных европейцев, а лица, чьи данные она обрабатывает эпизодически оказываются в Европе, то регулятор вряд ли заинтересуется деятельностью такой компании с точки зрения соблюдения GDPR. Существует противоположное мнение о том, что если услуги осуществляется вне пределов ЕС (например, номер в гостинице, находящейся в России, можно забронировать удаленно с территории ЕС), организация не должна попадать под действие GDPR, поскольку ее деятельность не осуществляется на территории ЕС и не попадает под действие законодательства ЕС. Такое мнение не вполне соответствует положениям GDPR: если такая компания будет использовать данные лиц, преимущественно проживающих в Европе, то на нее распространяется GDPR. Если взять пример с гостиницей, то на момент проживания в гостинице европеец действительно не находится в Европе. Но если после его возвращения гостиница продолжит обрабатывать его данные и, например, отправлять ему маркетинговые материалы, то получится, что она работает с данными лица, проживающего в Европе. Ну а если данные не будут использоваться в маркетинговых целях, а собираются только для брони и регистрации проживания, то в этом проблемы нет: GDPR разрешает сбор и обработку данных для исполнения договора, и согласие субъекта персональных данных в этом случае не нужно. 3. Согласие пользователей на использование их данных нужно всегда Не совсем так. В случае не-европейской компании GDPR применяется только при использовании персональных данных для маркетинговых целей (предложения товаров или услуг) и мониторинге поведения пользователей в Европе. Если данные используются не для этих целей, то положения GDPR не будут применяться. This Regulation applies to the processing of personal data of data subjects who are in the Union by a controller or processor not established in the Union, where the processing activities are related to: (a) the offering of goods or services, irrespective of whether a payment of the data subject is required, to such data subjects in the Union; or (b) the monitoring of their behaviour as far as their behaviour takes place within the Union. GDPR, Art. 3 (2). Когда данные получаются для целей исполнения договора, согласие не требуется. Существуют также другие случаи, когда использование персональных данных не требует согласия. Но если после исполнения договора данные остаются у компании и хранятся ей (например, в CRM), в этом случае потребуется согласие пользователя. 4. За нарушение GDPR будут сразу штрафовать, штрафы будут очень высокие Штрафовать сразу никто не станет. Регуляторы в разных странах только начинают работать с новыми правилами и будут осторожно относиться к формированию практики, с оглядкой друг на друга. Они вряд ли они станут спешить сразу применять штрафы, скорее сначала будут предупреждения и предписания. Указанные в GDPR размеры штрафов – это верхний предел, при нарушении штрафы могут применяться не всегда и могут быть небольшими. Начисляться штрафы скорее всего будут исходя из того, что они должны быть соразмерными и действенными, а основная цель – не задушить бизнес, а направить на верный путь. Кроме того, параллельно будет формироваться судебная практика (в том числе Люксембургского Суда), появления которой также будут ждать регуляторы, прежде чем начинать массовые проверки и санкции. Во время неофициального общения с представителем Европейской Комиссии на конференции, прозвучала мысль, что возможно будет проведено несколько показательных процессов над некоторыми гигантами, чтобы на практике стало понятнее, какое поведение является неприемлемым и к чему оно может привести. В вопросе применения штрафов за нарушение GDPR следующие позиции представляются наиболее правильными: «В целом нужно воспринимать большие суммы штрафов в законе как заградительную меру, а не новый способ пополнения местных бюджетов стран Евросоюза» «Конкретные размеры штрафов будут определяться индивидуально, с учетом большого количества факторов. Многомиллионный штраф может быть наложен на организацию в том случае, если она сознательно и злостно нарушала права субъектов, тщательно это скрывая и получая от такой обработки ПДн высокую прибыль» Что же касается опасений российских компаний, относительно того, что они могут быть оштрафованы за несоблюдение GDPR, то такие опасения вероятнее всего не реализуются. Привлечь к ответственности компании, не имеющие представительства в Европе, регулятору будет не просто. Еще сложнее будет исполнить наложенные санкции на территории государства, не входящего в состав ЕС. Поэтому основное регулирование, которое прогнозируется в связи GDPR, будет проходить посредством саморегулирования в отрасли: европейский бизнес постепенно будет отказываться от работы с компаниями, которые не соблюдают требования GDPR. Соответственно, основное негативное последствие несоблюдения GDPR – это не штрафы, а потеря конкурентоспособности на европейском рынке. 5. Персональные данные нельзя переносить в другие страны без соответствующего надзора и разрешения Данные можно передавать, если существует договор с компанией, которые передаются данные, и если в таком договоре предусмотрены определенные гарантии. Кроме того, существует Конвенция Совета Европы 108 (в которой участвует Россия), в ней указывается следующее: «Сторона не должна запрещать или обусловливать специальным разрешением трансграничные потоки персональных данных, идущие на территорию другой Стороны, с единственной целью защиты частной жизни» Точного ответа о том, как соотносятся положения Конвенции 108 и ограничения GDPR на передачу данных пока нет, возможно между ними есть противоречие. Но в любом случае, данные можно передавать при наличии договора, а также в некоторых других случаях, указанных в GDPR. [1] Pearse O'Donohue, Acting Director for the Future Networks Directorate of DG CONNECT at the European Commission.\n","id":23}
{"Host":"https://habr.com","Path":"/ru/post/349374/?mobile=yes","Text":"Редактор сценариев Age of Empires 2 можно превратить в машину Тьюринга / Хабр                \n\n19-02-2018\nНичто не может сравниться с вечером, потраченным на логику высказываний, машины Тьюринга и редактор сценариев AOE2… Среди прочих замечательных возможностей, доступных в редакторе, самой потрясающей является функция определения триггеров. Они приводят к выполнению действия на основании условий. Мне показалось очевидным, что начинать стоит отсюда, потому что здесь осуществляется задание правил. При этом возник вопрос: какие типы триггеров мне нужно искать конкретно? Разумеется, те, которые удовлетворяют требованиям машины Тьюринга! Это значит, что условия и действия этих триггеров должны позволить реализовать следующее: 1. Считывать и записывать символы 2. Менять позицию в моём «пространстве памяти» 3. Иметь «ленту» или «память» 4. Иметь пул используемых символов Кроме того, мне нужно иметь возможность записывать какие-нибудь правила. Здесь также будет очень полезен доступ к логике высказываний. С этого я и начал свои исследования (то есть с поисков в Гугле), и через полчаса получил все нужные мне ответы. Достаточно удовлетворяют требованиям следующие объекты, условия и эффекты: Пул символов: любое число игровых юнитов Пул памяти: все типы юнитов, доступных в игре (см. примечание в конце поста) Изменение позиции в памяти: Create Object Запись в позицию памяти: Create Object, Task Object, Kill Object Чтение из позиции памяти: Own Objects, Own Fewer Objects Вот пример использования типов игровых юнитов в качестве «ячеек» памяти: В этом примере память выглядит так: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 Но мы можем очень просто изменить память с помощью триггеров, чтобы она выглядела так: То есть: 8, 1, 8, 1, 1, 8, 1, 1, 1, 1 В следующем разделе триггеры представлены как способ запуска «программы» и отображения очень странного потока данных. Триггеры можно настроить на автоматическое срабатывание при запуске игры, а также на постоянное срабатывание для организации бесконечного цикла. Это основная движущая сила программы, которая позволяет структурировать такие «программы» двумя способами: Параллельное выполнение: T1 T2 T3 C1 (T) C1 (F) C1 (F) C2 (F) C2 (T) C2 (F) C3 (F) C3 (F) C3 (T) Все три триггера (T) срабатывают одновременно и сконфигурированы таким образом, что условия (С) заставляют их срабатывать в нужное время. Древовидное выполнение: T1 запускает T2 запускает (T3 или T4) T1 «управляет» программой, являясь единственным триггером, начинающим работать при запуске игры и выполняющим бесконечный цикл. Он передаёт выполнение триггеру T2, а затем T2 запускает T3 и T4, но они сами между собой решают, кто из них должен выполняться (похоже на то, как ведут себя триггеры при параллельном выполнении, или на то, что ниже делает XOR). Мне кажется, что это более практичный способ структурирования таких программ, и он эффективнее, потому что содержит меньше инструкций. Имеет ли редактор сценариев какую-то возможность для реализации логики выражений? Да. У нас есть доступ к условиям, то есть мы можем создать if -> then, а это в сочетании с Own Objects и Own Fewer Objects, позволяет получать доступ к памяти и выполнять сравнения. Мы можем объединить два выражения/условия, поэтому можно просто реализовать операции «AND»: Own Objects (Unit type: Soldiers, Amount: 7) Own Objects (Unit type: Archers, Amount: 10) Также с помощью триггеров можно реализовать частичное «OR». Проблема с ним в том, что если обе ветви истинны, то будут выполнены оба триггера. Поэтому более полезным оператором будет «XOR»: Trigger 1 Trigger 2 Condition 1 (T) Condition 1 (F) Condition 2 (F) Condition 2 (T) Effects Effects Оба триггера срабатывают одновременно, но выполняется только одна ветка эффектов, потому что условие Condition 1 должно быть истинным в одной, но ложным в другой, и наоборот для условия Condition 2. Также с помощью Own Fewer Objects, Create Object, Task Object и Kill Object можно легко реализовать сложение и вычитание: Trigger 1 Own Fewer Objects (Unit type: Soldiers, Amount: 9) Create Object (Soldier) Task Object (Move to location) Trigger 2 Own Fewer Objects (Unit type: Soldiers, Amount: 17) Create Object (Soldier) Task Object (Move to location) Пример того, как это может выглядеть в игре: Нам необходим Task Object, потому что игра не будет создавать больше одного объекта в одной точке, поэтому мы заставляем его переместиться. Результат этой операции можно найти в счётчике юнитов в верхней части экрана. Более сложный, но полезный набор триггеров и условий позволяет выполнять операции проверки на равенство, в том числе «равно», «меньше или равно», «больше или равно». Однако для их правильной работы нам нужно ввести специальный символ «останова» — короля (King): Конфигурация для «больше ли 23, чем 9» будет иметь вид: Trigger 1 (On-startup, Loop) Own Fewer Objects (Unit type: Soldiers, Amount: 23) Create Object (Soldier) Activate Trigger (2) Trigger 2 (No loop) Own Objects (Unit type: Soldiers, Amount: 9) Own Fewer Objects (Unit type: King, Amount: 0) Create Object (King) Эта система будет постоянно создавать солдат, пока их не станет 23. Если у нас их окажется 9 или больше, это будет означать, что 23 больше, чем 9 и для обозначения этого мы создаём короля (King). В процессе создания сценариев можно изменить юнита «останова» на любого другого, или создать любое нужное их количество. В целом это не особо полезно, но всё равно здорово! К сожалению, мне не удалось найти простого способа записывать видео, но если кому-то это удастся, то я добавлю их сюда. Так, похоже я не совсем понимал, что такое Тьюринг-полнота. Я не только должен иметь возможность создать машину Тьюринга, но это ещё и должна быть любая машина Тьюринга. Чтобы реализовать её, мне нужна «бесконечная» память. В нашем случае, мы ограничены количеством типов юнитов, но мы можем создать произвольное количество для решения этой проблемы: http://aok.heavengames.com/university/modding/an-introduction-to-creating-units-with-age-2/\n","id":24}
{"Host":"https://habr.com","Path":"/en/post/39824/?mobile=no","Text":"Не мешайте мне работать! / Habr            \n\n14-09-2008\nНаверняка не ошибусь, если напишу, что каждый программист хочет работать максимально эффективно, а каждый работодатель хочет, чтобы все его работники были максимально эффективны. Однако на самом деле часто бывает так, что даже самый мотивированный и профессиональный программист не может работать эффективно, а работает только на 10-20% своей силы. И постепенное его мотивация угасает, а 10-20% превращаются в 1-5%. Из-за чего это может происходить? А происходит это из-за многочисленных преград, которые мешают работать эффективно. Часть из этих преград невозможно убрать, не сменив работу, а с большинством можно бороться и побеждать их и именно об этом пойдет речь в этой статье. Ниже — мой личный топ-список таких преград с комментариями для программистов и примерами из моей жизни. Думаю, примерно такой же список актуален и для других IT-профессий. 1. Недостаточно мощные для эффективной работы компьютеры. Самый важный для меня деморализующий и убивающий эффективность фактор — это недостаточно мощный компьютер. На таком компьютере слишком много времени тратится на всё. Любая операция занимает больше времени, чем могла бы и всё это время в сумме может быть чудовищным. Например, рассмотрим случай с разработкой большой компьютерной игры (цифры взяты не с потолка, а из реальных проектов). Сборка большой игры может занимать минуты и десятки минут. Более мощные компьютеры могут ускорить билд в несколько раз. Я видел 2 рядом стоящих компьютера, где собирался один и тот же проект — на одном время сборки было около 5 минут, на другом — 25 минут. 20 минут потерянного времени программиста только на сборку! Запуск большой игры, а особенно под отладчиком, может занимать минуты на слабых компьютерах. Для того, чтобы что-то проверить или отладить, надо загрузить уровень — это тоже могут быть минуты. При этом тоже самое на мощном компьютере может занимать 20-30 секунд. В итоге я встречал людей, которые сидя на слабом железе тратят по 5-10 минут на ту же операцию, что на мощном компьютере выполняется за 20-30 секунд. Учитывая, что таких операций за день надо сделать много, можно посчитать реальные потери времени. Иногда это час-два каждый день, то есть до 20% времени работы. Какое решение тут можно посоветовать? Во-первых конечно же капать на мозг менеджерам и другим начальникам, чтобы они купили более мощные компьютеры. Это простой способ, но вы не можете повлиять на вероятность того, что он сработает. Во-вторых, надо всегда оптимизировать все процессы, которые занимают много времени. Это самый универсальный совет. И, имхо, самый полезный совет во всей этой статье. Если сборка проекта слишком долгая — переделайте систему сборки или используйте ускоряющие тулзы (Incredibuild, например). Если запуск игры слишком долгий — оптимизируйте запуск. Сделайте, чтобы он был быстрым и на слабой машине. Пользователи в итоге вам еще и спасибо скажут. Если уровни грузятся долго — ускоряйте загрузку уровней. Оптимизируйте все операции, которые отнимают у вас много времени. Вы потратите на это немало силы и времени один раз, но это в итоге окупится, так как вся команда начнет работать быстрее и эффективнее, тратя меньше времени на долгие операции. 2. Лишние собрания и митинги, а также другие факторы отвлечения. Рабочие собрания и митинги — это хорошо. Они позволяют донести до всех нужную информацию, создают в коллективе нужный настрой на работу и т.п. Но в то же время, если таких собраний становится слишком много — это жутко демотивирует и мешает сосредоточиться. В итоге если в день запланировано 2-3 собрания, то можно другой работы уже не планировать — ее сделать не успеешь. Я для себя решил просто — если собрание не принесет мне пользы и не даст новой информации, то я его игнорирую (под это определение попало процентов 70-80 собраний на текущей работе). Другие факторы отвлечения от работы — это сообщения по громкой связи или другие крики и шум, частые вопросы на разные темы, форумы, мессенджеры и интернет в целом (когда он используется не для работы) и т.п. Большинство этих факторов вы можете сами убрать, например, усилием воли отказавшись от интернета на время работы и работая в наушниках. А часть вы не сможете решить самостоятельно, а только можете попробовать написать свод правил совместно со всеми сотрудниками. Например, я знаю команду, которая сидит в одной большой комнате и в которой официально запрещен любой шум с 12 до 15 (даже звонки сотовых). В любом случае влияние этих мешающих факторов надо стараться уменьшать. 3. Жесткий график работы. Я про это уже писал в статье про время прихода на работу и в её второй части. Вкратце напишу, что гибкий график позволяет работать именно в то время, когда работа идет наиболее эффективно и не «спать» на работе. Знаю случаи, когда люди (очень профессиональные) приходили на работу по жесткому графику, а потом досыпали в туалете. Или же просто засыпали за рабочим столом. Кому от этого лучше и какая эффективность такой работы? 4. Ненужная или неподходящяя работа. Под ненужной работой я понимаю всё, начиная от «пойдем поможешь разгрузить компьютеры» до «сделай-ка вот эту ненужную фичу, а потом посмотрим нужна она или нет». Если у вас хороший менеджер, то вы вполне можете ни разу не встретиться с такими задачами. Если же нет, то надо уметь от них отказываться или незаметно саботировать их. Я обычно сразу указываю на ненужность той или иной работы и пытаюсь от нее отказаться. Если менеджер уверен, что эту работу нужно сделать — он будет настаивать и в итоге работа скорее всего будет сделана. Но для реально ненужных работ в момент обсуждения может выясниться, что это действительно неважно или это может сделать кто-то более подходящий. Итак, если вы видите, что работа скорее всего ненужная или вы недостаточно подходите для ее выполнения — не бойтесь спорить с менеджером. Он получит дополнительную информацию в диалоге, которой ему возможно не хватало, и примет в итоге более правильное решение. 5. Излишняя секьюрность. Пароли на любой чих. Это для меня новый пункт, с которым я раньше не сталкивался. Помнится, первый шок на новой работе у меня был, когда в первый же день мне сказали придумать 3 разных пароля длиной не менее 12 символов, где должны быть и заглавные и прописные буквы и цифры. Звучит пугающе, особенно когда ты стоишь перед клавиатурой и должен набрать их прямо сейчас. В каждой крупной фирме есть куча серверов и сетевых дисков. Но в некоторых фирмах доступ к ним под паролем. В итоге, чтобы выложить что-то в папку temp на сервере — надо набирать пароль. Конечно, Windows кеширует пароли, но ведь есть еще новые образы VMWare и в них всегда приходится вбивать эти пароли заново. Также сюда относятся плохо организованные политики прав доступа к серверам, базам данных, репозиториям с кодом и т.д. Например, в крупной фирме можно потратить несколько недель только на то, чтобы тебя вписали во все нужные группы для доступа к исходникам проекта. А потом еще потратить столько же времени на настройку ssh и на создание своих учетных записей на разных серверах. Это все может так неэффективно работать, если общение с админами идет через заявки, а не лично. У меня на текущей работе любой запрос админам должен быть сделан через специальную страницу в интранете. Потом они могут до недели-двух этот запрос обрабатывать. В итоге простое «Прошу добавить меня в группу build на сервере 1» может обрабатываться неделю. После первых долгих ожиданий я для себя решил, что лучше сходить и поговорить лично, чем так долго ждать. В итоге, если просить лично, то все такие задачи выполняются мгновенно прямо при тебе. 6. Антивирусы и другой предустановленный обязательный софт. Такой обязательный софт может тормозить, как тормозят антивирусы, или глючить, но вы не можете его отключить или удалить, так как в правилах написано, что нельзя. Например, недавний случай: полная сборка одного проекта с включенным антивирусом у меня занимала 15 минут, а без антивируса — 5 минут. Что делать в такой ситуации, терять время зря? Надо иметь смелось отходить от правил в таких случаях. Я, например, выключал сеть, отключал антивирус, делал пересборку, а потом включал все обратно — это и безопасно и время не тратится зря. А потом воспользовался своим же советом из пункта 1 и оптимизировал систему сборки, чтобы она даже с включенным антивирусом занимала 5 минут. А какой у Вас топ-список вещей, мешающих работать? И как вы с ними справляетесь?\n","id":25}
{"Host":"https://habr.com","Path":"/ru/company/ivi/blog/236065/?mobile=yes","Text":"Все на одного или как мы построили CDN / Хабр                                     \n\n09-09-2014\nСреди высоконагруженных (highload) систем существует большая разница между системами с высокой нагрузкой в плане количества запросов в секунду (RPS, requests per second) и высокой нагрузкой в плане генерируемого трафика (того, который меряется гигабитами в секунду). В нашем ivi.ru нагрузка есть и та, и другая. Сейчас я хочу рассказать про то, как мы генерируем сотни гигабит в секунду, и никому от этого не плохеет. Мечты о велосипеде Летом 2011-го года случилось страшное: рунету так понравилось смотреть бесплатное кино, что рост нагрузки некоторое время обгонял рост мощностей. Всё это шло в каналы единственного провайдера из единственного московского датацентра. Доходило до того, что некоторые московские же абоненты получали кино через Амстердам. Понятно, что в таких условиях бороться за звание дома высокой культуры быта лучшего интернет-кинотеатра России достаточно проблематично. Было принято волевое решение использовать CDN (Content Delivery Network, сеть доставки контента), и всё завертелось (включая моё скромное участие). Когда мы хотим донести до пользователя несколько мегабит видео-трафика, у нас есть две фундаментальные проблемы (проблем, конечно, больше, но они не носят такого фундаментального характера): 1. Нужна серверная мощность, чтобы выдать пользователю эти мегабиты 2. Нужны каналы связи, которые эти мегабиты до пользователя донесут Протокол TCP настаивает, что чем больше время оборота пакета (RTT, return-trip-time), и чем больше потери пакетов (packet loss), тем меньше скорость передачи данных. Опять же, междугородние магистральные каналы – самые дорогие, а значит – забитые. Как следствие – вероятность потери пакетов увеличивается с увеличением расстояний. Соответственно, нам, с нашим HTTP (работающим поверх TCP) надо держать серверы поближе к абонентам. Всё это можно держать у себя, а можно заплатить денежку дяде и воспользоваться его CDN. И платить можно только за фактическое потребление, и всякие дурные темы, вроде нехватки серверов или забитых каналов, становятся чужой головной болью. Опять-таки, у такого дяди несколько клиентов, а значит у него большие объёмы и по закупке серверов, и по закупке каналов. Т.е. низкая себестоимость. Вот только насколько он хорош? С другой стороны, строительство своего CDN имеет три основных недостатка: 1. Надо научиться его строить 2. Придётся работать со множеством поставщиков (вместо одного, ладно – двух, оператора CDN) 3. Поддерживать, развивать и умощнять придётся самим. Но так ли это страшно на самом деле? Велосипедная фабрика Большинство операторов коммерческих CDN предлагает бесплатное (или за разумную денежку) тестирование. И мы этой возможностью воспользовались. Международные операторы CDN удивили тем, что могли отправить абонента из, скажем, Новосибирска на сервер где-нить в Южной Америке. И, разумеется, серверных мощностей на территории России (а мы за рубежом кино не показываем), у них на тот момент не было. Справедливости ради скажу, что сейчас некоторые из них ставят сейчас свои узлы у нас в стране. А про Южную Америку мне потом специалист другого отечественного оператора CDN разъяснил. В поражённых западным капитализмом сетях другая цель балансировки – они считают, что каналов связи хватает. У них ограничителем является серверная мощность. Вот и балансируют туда, где сервера посвободнее… И зарубежные, и отечественные CDN в итоге показали уровень качества (под которым мы понимали эффективную скорость закачки контента конечным пользователем), сопоставимый с тем, что мы могли сделать сами уже в тот момент. А вот по ценам всё оказалось далеко не так радужно, как в теории. Понимаете теперь, почему ни названий компаний, ни цифр замеров, здесь не будет? Кстати, уже в этом году вышло исследование «State of the Union: Ecommerce Page Speed & Web Performance» (доступное, например, тут) – что использование CDN замедляет загрузку сайта. Тут, как говорится, совпало. О том, почему это происходит, я планирую написать отдельно. Ну да ладно, возвращаемся к нашему CDN. Стало очевидным, что сеть должна быть своя. Как её построить – вот стал основной вопрос. Но тут нам повезло. Во-первых, оказалось, что архитектура серверов на нашей центральной площадке уже тогда разделялась на edge- (пограничные) и origin-серверы (исходные). А edge-серверы, как оказалось, очень неплохо выносились на внешние площадки. Во-вторых, оказалось, что провайдеры знают такой ресурс как ivi.ru, и в большинстве своём хотят работать с нами, чтобы локализовать трафик. В заметном количестве случаев операторы сами выходили на контакт и предлагали сотрудничество. Это, безусловно, помогало в работе по строительству новых узлов. В нескольких регионах представители местных провайдеров мне прямо говорили: «Для нас качество, с которым смотрится ivi.ru – важное конкурентное преимущество». В-третьих, выяснилось, что нам ничего из дополнительной функциональности, предлагаемой провайдерами CDN, не нужно. Не нужно перекодирования контента «на лету» (весь контент заранее закодирован во всех возможных вариантах). Не нужно экзотических протоколов, вроде RTMP (у нас всё по HTTP, и даже новомодный HLS – это HTTP). Не нужно толстого канала с QoS до Москвы (для управления узлом и обновления кэша достаточно «обычного» интернета в количестве 50-100Мбит/с), даже падение такого интернета не останавливает обслуживание абонентов. Не нужно «распасовщика» и «запатентованных алгоритмов балансировки» (это сейчас расписывать не буду, оставлю для другой статьи). В итоге мы смогли в очень сжатые сроки развернуть собственный CDN по всей территории страны. В результате этой работы сейчас узлы ivi.ru присутствуют в 23 городах России. Если честно, после 20 мне уже неинтересно стало пересчитывать, новые узлы появляются постоянно. Само развёртывание нового узла занимает один рабочий день. Размеры узлов колеблются от одного до восьми серверов. На многосерверных узлах, само собой, стоит ещё и сетевое оборудование: cisco серий 3750X или 4500-X. На части узлов серверы подключаются аггрегированным линком 4 * 1 GbE (это узлы малого и среднего размера). На крупных узлах серверы подключаются 10GbE интерфейсами. В некоторых городах у нас есть несколько узлов, хотя с этим мы сейчас боремся. Для повышения эффективности нам целесообразно иметь один большой кластер, нежели несколько маленьких. Ведь маленькие закэшируют один и тот же самый популярный контент, а если объединить те же серверы в кластер, то объём уникального закэшированного контента будет значительно больше. Более половины генерируемого ivi.ru трафика сейчас генерируется узлами вне Москвы. Интересно суточное колебание (график показывает отношение сгенерированного в регионах трафика к суммарному, время московское). Кликабельно: Очень чётко видно, что ночью нагрузка на CDN минимальная. Тому есть несколько причин, но основная из них – ночью люди смотрят непопулярный контент. Такой, который не закэширован (и не закэшируются). Максимальная же нагрузка на CDN – это время, когда люди к востоку от Москвы уже проснулись, а москвичи пока ещё спят :) Локализация трафика на узле колеблется от 40% (маленькие односерверные узлы) до 90% (на самых крупных узлах). Без сомнения, это не могло не сказаться на качестве обслуживания абонентов. Вот красивый график: Свежие данные приводить не буду – они уже давно колеблются на уровне статистической погрешности, и такой красивой «ступеньки» там уже не увидеть. Эта статья задумывалась как обзорная про наш CDN. Следующим аспектом я планирую рассказать про балансировку нагрузки между городами и весями. А какие ещё аспекты нашего CDN вам были бы интересны?\n","id":26}
{"Host":"https://habr.com","Path":"/ru/post/561300/?amp&amp","Text":"Весенний Дзержинск: природа, фестивали и химическая промышленность / Хабр                \n\n07-06-2021\nВ прошлый раз мы вернулись в Карабаш, а теперь – в Дзержинск, чтобы увидеть, чтобы увидеть, как город меняется с приходом весны. Посмотрим также на фестивали и другие мероприятия, на городскую среду, затронем момент с развитием промышленности и снова – вопрос экологии и влияния человека (именно жителей) на чистоту в городе. Экология Вот на этом месте было то самое «Белое море», о котором говорили в прошлый раз. В водоем годами отправлялись едкие химикаты с предприятия «Капролактам», которое производило едкие щёлочи – хлор и каустик. К счастью, эту территорию рекультивировали. Но вокруг бывшего «Белого моря» остаются еще странные места, на которые стоит обратить внимание тем, кто отвечает за экологию. Угадайте, что охраняют эти собаки? Подсказка: настоящий портал в ад – «Черную дыру». Сейчас здесь занимаются исправлением этой ошибки советского химпрома. С этой горой под настилом пока никто не справился. Здесь тоже какие-то отходы. А вот свежие проблемы – нежелание людей, предприятий или других сущностей выбрасывать мусор туда, куда это нужно делать. За пределами города В промышленных городах иногда больно смотреть на здания, которые или недостроены, или построены еще десятки лет назад, но не используются. Столько ресурсов, человеческих сил было вложено в то, что сейчас попросту разваливается. Интересно, существует ли «пилюля», которая могла бы все это возродить? Из области фантастики – запустить в здание на фото ниже программистов, предпринимателей, каких-нибудь дизайнеров, какие-нибудь рекламные агентства и так далее. Но это проще сделать, например, в Крыму – там, где можно приманить людей хотя бы с помощью климата. Но заброшенные здания остаются такими же. Наверняка, притягивают желающих побегать по опасным местам. Не самые удобные остановки тут местами есть. Но, как написали в комментариях в прошлом посте, «такие виды остановок или улиц города можно сфотографировать в половине городов и городков России» – тут все верно. Явно, эта остановка еще и просто не используется, потому что находится рядом с закрытым предприятием. Городская среда В 2016 году в России утвердили приоритетный проект «Формирование комфортной городской среды». В нем предусмотрено участие жителей в принятии решений, которые касаются выбора приоритетных объектов для благоустройства, а также поддержку инициатив горожан. Под эти параметры подпадает один из проектов, размещенных на сайте и предназначенных для поддержки инициатив: «Инициативное бюджетирование». Жители города могут заявить свое желание построить что-то в Дзержинске и получить финансирование из городского бюджета. Проект должен относиться к развитию территории – например, сейчас на сайте администрации есть заявка на установку колеса обозрения, стоимость его – 30 млн рублей. (Колесо обозрения, которое было в городе раньше, продали на Авито – у собственника истек срок аренды). Художнику, который нанес котиков на этот дом, жителям и всему городу стоит сказать «спасибо». Это хотя и черно-белое, но очень «яркое» пятно для той части города, которая находится не вдоль главных улиц. И прямо противоположная ситуация: эта надпись была и в прошлом посте. И ее с зимы никто не закрасил. На минуточку, это территория детского сада. А сетевые магазины – всегда рядом. Или у дома. В зависимости от названия. Человеческая природа: мусор по краю дороги. Смотрите в любом городе России. Напишите в комментарии, вы выбрасываете окурки, фантики, другой мусор прямо рядом с тропинками и дорогами, по которым ходите? Если нет – откуда берутся все эти люди, которые засоряют улицы? Набережная с эко-дизайном. Некоторые российские застройщики сейчас используют такого же рода деревянные детские площадки. Речь не о цветных – а именно о турниках, качелях, других объектов из бревен. Бывает, что на них люди жалуются – типа «не цветные и детям не интересно». На мой взгляд, это, наоборот лучше, чем сплошной пластик. Что вы думаете? Напишите в комментариях. Интересно, чтобы было, если бы начали придумывать какие-то новые услуги для детских площадок? Например, поставили бы вендинговый аппарат. Или вагончик с бургерами и хот-догами. Не везде набережная выглядит ухоженной. Еще одно место, которое можно бы было изменить. На этих ступенях могли бы сидеть люди, смотреть выступления артистов, или же читать и работать. Также на набережной – физкультурно-оздоровительный комплекс. В центре города есть и такие ухоженные, свежевыкрашенные здания. Смотрится опрятно. В таких же цветах стоит здание «Корунда» – в 1915 году запустили этот завод минеральных кислот. Остаются и менее ухоженные здания. Наши старые знакомые кораблик, паровозик и башенка отмерзли. Менее страшно не стало. Но есть же хорошие площадки, неужели здесь тоже дети гуляют? Здесь, казалось бы, мы имеем дело со внутренним двором не самого старого жилого дома. Либо управляющая компания отказывается хоть какие-то улучшения вносить, чтобы не вызывать у детей депрессию и страшные воспоминания о детстве, либо жители сами не хотят менять это положение вещей. Но все равно – как же здорово быть детьми, которые не сильно-то замечают этот мусор. И как стыдно быть взрослыми, которые его тут развели. Пока дети практически играют в мусоре, недалеко взрослые стояли с семечками. И тут ведь наверняка все ругают власти и президента – почему, пока они не могут донести мусор до урны, им никто не пришел и не навел порядок? А вот и минутка киберпанка: можно заказать доставку продуктов на дом. Хотя, в этом магазине и одежда тоже продается. А этот дом находится прямо в центре города – и он заброшенный. На самом деле, это хорошо: дом признали аварийным и уже пару лет как расселили жителей. Будет здорово, если на его месте появится новый жилой дом или другое здание. А так выглядят типичные дворы. Типичная школа. Слегка разваливается, но никто не трогает, пока цела. Мероприятия Фестивали и другие подобные мероприятия важны для развития. Они формируют позитивный опыт жителей и привлекают людей из других городов. В Дзержинске прошел городской мотофестиваль при поддержке Harley-Davidson, местной компании «ХимАвто» и «Нашего радио». 29 мая Дзержинск отпраздновал День города. Он состоял сразу из нескольких фестивалей, в том числе позиционирующихся для молодежи – «Мечтай! Действуй!» и «Арт-пространство»: с комиками, воркаутерами, блогерами и так далее. Здесь же продвигали донорство, проходил клуб исторической реконструкции, были диджеи, спортивные мероприятия – стритбол, лазертаг. Выступал «ЧИЖ». Развитие промышленности Индустриальный парк «Ока-Полимер» войдет в состав особой экономической зоны «Кулибин» – заявку в правительство РФ об этом в Нижегородской области готовили в апреле. Пока других новостей нет, но сам факт интересный. Особая экономическая зона – это территория, которая имеет особый юридический статус. Компании, в том числе это могут быть стартапы – даже студенческие, в таких зонах могут получать преференции с точки зрения налогов. Также им могут оказывать экспертную поддержку и так далее. В общем, для промышленных городов, возможно, это сейчас лучший способ развивать новые компании и технологии. В Дзержинске хотят продолжать заниматься химией. Энергомощностей здесь хватит, инженерные коммуникации есть, подъездные пути и железнодорожный узел – тоже. Если подходить к делу с большей заботой об экологии, чем это делали в СССР (сделав город столицей химической промышленности) – результат может быть интересным. Итог В комментариях к прошлому посту сказали: «С другой стороны, когда у города такая слава на всю страну — «самый грязный город», на мой взгляд, это супермотивация, шанс для его руководства обратить ситуацию на 180 градусов!» С этим сложно не согласиться. Хотя в городе много облезлых домов, у него есть и плюсы: есть технологический задел, есть существующие предприятия, есть возможность развивать новые технологии и компании. Городу остается воспользоваться возможностями и изменить ситуацию. А мы будем наблюдать за этими изменениями.\n","id":27}
{"Host":"https://habr.com","Path":"/en/post/260971/?mobile=no","Text":"Brubeck — быстрый, statsd-совместимый агрегатор метрик от GitHub / Habr             \n\n23-06-2015\nИстория появления Одной из главных целей команды разработчиков GitHub всегда была высокая производительность. У них даже существует поговорка: «it's not fully shipped until it's fast» (продукт считается готовым только тогда, когда он работает быстро). А как понять, что что-то работает быстро или медленно? Нужно мерять. Измерять правильно, измерять надёжно, измерять всегда. Нужно следить за измерениями, визуализировать всевозможные метрики, держать руку на пульсе, особенно, когда дело имеешь с высоконагруженными онлайн системами, такими как GitHub. Поэтому метрики — это инструмент, позволяющий команде предоставлять столь быстрые и доступные сервисы, почти без даунтаймов. В своё время GitHub одними из первых внедрили у себя инструмент под названием statsd от разработчиков из Etsy. statsd — это агрегатор метрик, написанный на Node.js. Его суть состояла в том, чтобы собирать всевозможные метрики и агрегировать их в сервере, для последующего сохранения в любом формате, например, в Graphite в виде данных на графике. statsd — это хороший инструмент, построенный на UDP сокетах, удобный в использовании как на основном Rails приложении, так и для сбора простейших метрик, наподобие вызова nc -u. Проблема с ним начала проявляться позже, по мере роста количества серверов и метрик, отправляемых в statsd. Так, например, некоторые метрики показывались некорректно, а некоторые, в особенности новые, вообще не собирались. Виной тому были почти 40%-ые потери UDP пакетов, которые просто не успевали обработаться и отбрасывались. Природа однопоточного Node.js с использованием единственного UDP сокета дала о себе знать. Но масштабировать было не так просто. Для того, чтобы распределить сбор и обработку пакетов по нескольким серверам, нужно было шардировать не по IP, а по самим метрикам, иначе бы на каждом сервере был свой набор данных для всех метрик. А задача шардирования по метрикам непростая, для её решения GitHub написал свой парсер UDP пакетов и балансировку по названию метрики. Это сгладило ситуацию, позволило увеличить количество инстансов statsd до четырёх, но являлось полумерой: 4 сервера statsd, еле собирающих метрики, плюс самописный балансировщик нагрузки, занимающийся парсингом UDP пакетов, в итоге вынудил переписать всё более правильно, на чистом С, с нуля, сохранив обратную совместимость. Так появися Brubeck. Brubeck Но переписывание Node.js приложения (event-loop, написанный на С на основе libuv) на чистый С, с использованием того же самого libuv — сомнительное занятие. Поэтому решено было пересмотреть саму архитектуру приложения. Во-первых, отказались от event-loop на сокете. Действительно, когда в тебя льётся 4 миллиона пакетов в секунду, нет смысла каждый раз крутиться в цикле и спрашивать, не появились ли новые данные для чтения, так как, скорее всего, они там уже появились, и не одни :) Event-loop заменили на пул потоков воркеров, использующих один общий сокет с сериализацией доступа к нему. Позже, механизм улучшили ещё, добавив поддержку SO_REUSEPORT для сокетов из linux 3.9, что позволило отказаться от сериализации доступа воркеров к сокету в самом агрегаторе. (прим. по этой теме интересно будет почитать статью как nginx внедрил поддержку SO_REUSEPORT). Во-вторых, наличие нескольких потоков, работающих с одними и теми же метриками, означает, что у нас разделение данных. Для безопасного доступа к разделяемым данным необходим механизм синхронизации доступа, например, локи (lock), что не есть хорошо в условиях высокой конкуренции за доступ к данным и при необходимости высокой производительности. На помощь приходят lock-free алгоритмы, в частности, lock-free реализация хеш-таблицы, в которой хранятся метрики. (на самом деле там lock-free только на чтение, а на запись optimistic locking, но это не страшно для приложений с высоким reads-to-writes rate, т.к. метрики добавляются и удаляются гораздо реже, чем приходят в них сами данные). В-третьих, агрегация данных внутри одной метрики синхронизировалась через spinlock — крайне дешёвый механизм в плане затратов ресурсов CPU и переключения контекстов, что так же не вызвало затруднений, т.к. борьбы за данные внутри одной метрики почти не было. Результат Простая многопоточная архитектура агрегатора позволила добиться неплохих результатов: на протяжении последних двух лет единственный сервер с Brubeck дорос до обработки 4.3 миллиона метрик в секунду, без потери пакетов даже в пиковой нагрузке. Вся инфа и данные достоверно взяты с блога разработчиков. Brubeck был выложен в open-source: github.com/github/brubeck В нём уже есть многое из statsd, но ещё не всё. На данный момент разработка ведётся активно, сообщество находит баги и быстро исправляет.\n","id":28}
{"Host":"https://habr.com","Path":"/en/post/396299/?mobile=no","Text":"Физика в мире животных: лапа геккона / Habr           \n\n27-07-2016\nФото: Wikimedia Гекконы — обитатели тропических и субтропических областей Старого и Нового Света. Эти ящерицы живут и на континентах, и на островах, ареал их распространения обширен. У гекконов есть одна особенность — они умеют удерживаться практически на любой поверхности. Вес тела животного удерживает даже одна лапа. Поверхность может быть любой — дерево, скальная порода, даже полированное стекло. На способность геккона крепко держаться за что угодно обращали внимание еще древние греки. Аристотель пытался понять принцип закрепления лапы ящерицы, интересовались гекконами и средневековые ученые. Изучают их и в наше время. Есть несколько теорий, объясняющих выдающиеся способности этих ящериц в «альпинизме». Присоски на пальцах. Одно из первых объяснений, которое выглядело вполне логичным. Правда, после изучения лапы геккона под микроскопом оказалось, что присосок на пальцах нет. К сожалению, миф о присосках живет и по сей день. Электростатика. Еще одно правдоподобное объяснение, которое удалось опровергнуть (хотя есть и некоторые подтверждения этой теории, о них поговорим ниже), создав условия, при которых заряда на лапах геккона просто не могло быть. Животное все равно крепко держалось на гладкой поверхности. Опровержение было получено еще в 30-х годах прошлого века. Немецкий ученый Вольф-Дитрих Деллит (Wolf-Dietrich Dellit) направил поток ионизированного воздуха в сторону лап геккона, который держался на металлической поверхности. Ионизация, по мнению Деллита, должна была нейтрализовать или значительно уменьшить силу сцепления лап с поверхностью, если бы механизм сцепления имел электрическую природу. Этого не произошло, поэтому был сделан вывод, что гекконы используют что-то еще. Канадский ученый Александр Пенлидис считает, что этот эксперимент был некорректным. Дело в том, что контакт между лапами геккона и поверхностью чрезвычайно тесен, вследствие чего ионизированные молекулы просто не в состоянии проникнуть между сверхмалыми структурами лап и поверхности и нейтрализовать взаимодействие. Сцепление лап геккона с неровностями поверхности. Это объяснение тоже не подходит, поскольку гекконы могут передвигаться по вертикальной поверхности из полированного стекла. Более того, они могут передвигаться и по потолку из того же материала. Фото: wikipedia С появлением электронного микроскопа лапу геккона удалось изучить во всех деталях. Как оказалось, она покрыта чрезвычайно тонкими щетинками, длина которых составляет до сотни микрометров. Концентрация щетинок на единицу площади поверхности лапы очень высока: более 14 000 волосков на 1 мм2. Каждая щетинка, в свою очередь, не является монолитным образованием, а делится на конце на 400-1000 еще более мелких волокон. Толщина таких волокон составляет 0,2 мкм. На 1 см2 контакта с поверхностью приходится около 2 млрд волокон, каждое из которых к концу расширяется. а. Лапка геккона б. «Подушечка» пальца геккона под микроскопом в. Одна из щетинок лапы геккона г. Она же, под бОльшим увеличением д. Максимальное увеличение щетинки (фото: somuchnews) Американские ученые выяснили, что сила сцепления лапы геккона токи составляет 10 Ньютон на 1 см2. Такое сцепление возможно лишь для гладких поверхностей, где задействованы практически все волокна на лапах животного. Если речь идет о поверхностях, часто встречающихся в местах обитания гекконов — скалы, деревья, здесь задействована лишь часть волокон на лапах (в силу большого числа неровностей на этих поверхностях), но и этого достаточно для удержания животного на месте. Как оказалось, микроскопические волоски на лапах геккона сцепляются с опорной поверхностью посредством ван-дер-ваальсовых сил. Ван-дер-ваальсовы силы — силы межмолекулярного (и межатомного) взаимодействия с энергией 10—20 кДж/моль. Основу ван-дер-ваальсовых сил составляют кулоновские силы взаимодействия между электронами и ядрами одной молекулы и ядрами и электронами другой. На определенном расстоянии между молекулами силы притяжения и отталкивания уравновешивают друг друга, и образуется устойчивая система. Именно такую систему и составляет лапа геккона с поверхностью, с которой она соприкасается. Сложное строение лапы обеспечивает и еще одно ее свойство — гидрофобность. Лапа отталкивает воду и грязь, благодаря чему геккон может неплохо передвигаться и по влажным поверхностям. Геккон без проблем открепляет лапу от поверхности, на которой она закреплена. Для этого используется специальный механизм. Дело в том, что прикрепившаяся к какому-либо материалу щетинка может без труда открепиться, если угол между волокном и поверхностью составит более 30°. При движении, изменяя угол соприкосновения лапы и поверхности, геккон без труда закрепляет и открепляет лапы. Затраты энергии на этот процесс минимальны. Силы Ванд-дер-ваальса или что-то еще? Два года назад канадский ученый Александр Пенлидис (Alexander Penlidis) решил самостоятельно изучить механизм прилипания лап геккона к поверхностям. Как оказалось, при соприкосновении лапы и поверхности возникает обмен электрическими зарядами. В итоге образуется положительный электростатический заряд у лапы и отрицательный — у поверхности. Пенлидис ставил эксперимент с двумя типами полимерных поверхностей — тефлоном AF и полидиметилсилоксаном. Согласно выводам, сделанным ученым по результатам исследования, сила адгезии коррелировала с величиной электростатического заряда лапы и поверхности. А из этого следует, что именно электрический заряд играет главную роль в сцеплении лапы с поверхностями. Исследование интересное, но оно не отвечает на важный вопрос — каким образом геккон держится на очень неровных поверхностях, где обеспечить адгезию с использованием электрического заряда гораздо сложнее, чем на ровной поверхности. Возможно, лапы геккона имеют двойной механизм сцепления — и силы ван-дер-ваальса, и электрический заряд. Влияние воды В подавляющем большинстве случаев ученые проводили эксперименты с гекконами в сухой среде. Ученые из Акронского университета решили проверить, насколько хорошо ящерица может перемещаться по увлажненным поверхностям. Как оказалось, если распылить на стеклянную пластину воду, то животное держится на такой поверхности гораздо хуже, чем на той же пластине без капель воды. Тем не менее, удержаться на влажной поверхности геккону удается. Но если пластину погрузить на небольшую глубину в воду, а геккона снова поместить на пластину, то ящерица не может удержаться на поверхности в таких условиях. Если погрузить лапы геккона в воду на полтора часа, а затем посадить его на стекло, он соскальзывает, не в силах закрепиться. По мнению Алиссы Старк (Alyssa Stark) из Акронского университета, это объясняется тем, что вода мешает силам ван-дер-ваальсового взаимодействия, и лапы геккона не могут закрепиться на поверхности. Не только лапы В механизме закрепления лап на поверхности участвует все тело геккона, утверждают ученые из Массачусетского университета в Амхерсте. Тело рептилии, по словам Альфреда Кросби (Alfred Crosby), играет роль пружины, которая прижимает лапы к поверхности. И чем больше масса тела геккона, тем сильнее эта пружина. Благодаря этому механизму в любой поверхности отлично держатся и крупные виды гекконов, а не только их мелкие родственники. Несмотря на то, что Александр Пенлидис смог доказать влияние электрического заряда на адгезионную способность лап геккона, большинство специалистов поддерживают все же точку зрения о механизме сцепления на основе сил ван-дер-ваальса. Сейчас ученые пытаются объяснить еще одну интересную проблему — происхождение этого механизма в процессе эволюции. «Гекконовый скотч» С тех пор, как механизм работы лапок геккона в целом стал понятен, люди пытаются воcпроизвести его искусственно. В частности, агентство DARPA создало альпинистское оборудование, позволяющее человеку с массой 122 кг (масса тела + полезная нагрузка) взобраться на стеклянную отвесную стену на высоту в 7,6 м. Инженер из Стэнфорда создал робота, который может взбираться по практически отвесным гладким поверхностям. Манипуляторы робота тоже созданы по образцу лапы геккона. А специалисты из Пенсильванского университета разработали новый тип высокоточного захвата, который можно использовать на производстве для работы с мелкими деталями. Ведется и разработка сверхклейкого скотча, который может выдержать много циклов использования и поверхность которого не загрязняется при длительном использовании. В NASA разработали специальное крепление, которое можно использовать как условиях Земли, так и в условиях невесомости в космосе. Оно позволяет крепить грузы к поверхностям при помощи специальной «липучки», созданной по образу и подобию поверхности лапки геккона.\n","id":29}
{"Host":"https://habr.com","Path":"/ru/company/lenovo/blog/339980/?mobile=yes","Text":"Решения Lenovo для дата-центров / Хабр               \n\n16-10-2017\nВ 2004 году Lenovo выкупила у IBM подразделение по производству ПК, что вызвало волну скептицизма пользователей и журналистов. В 2014 году, когда Lenovo уже возглавляла мировой рейтинг производителей персональных компьютеров по объему продаж в денежном и количественном выражении согласно IDC, компания поставила себе новую цель: выйти в лидеры рынка производителей оборудования для дата-центров. И выкупила у IBM бизнес по производству серверов на базе архитектуры x86. В течение нескольких лет с момента завершения сделки, Lenovo предлагала своим заказчикам две линейки оборудования для дата-центров: собственную ThinkServer и купленную у IBM System x. В целях оптимизации и расширения продуктового портфеля, этим летом Lenovo представила две новые торговые марки оборудования для ЦОД – ThinkSystem и ThinkAgile, которые приходят на замену всем предыдущим поколениям систем. Стоит отметить, что оба новых бренда строятся на базе технологий System x. Представленное обновление является крупнейшим за историю компании. В общем итоге Lenovo анонсировала 26 новых систем. В августе в Москве компания рассказала подробнее о новых продуктах и о своей стратегии развития. Предлагем вашему вниманию первую часть статьи с рассказом о трансформации компании. Традиционная инфраструктура Рынок дата-центров меняется. Согласно прогнозам IDC, расходы на аппаратную инфраструктуру к 2020 году распределятся следующим образом: при условии, что общий объем рынка вырастет на $13 млрд., сегмент традиционной серверной инфраструктуры сократится на 4%, в то время как сегменты программно-определяемой инфраструктуры, высокопроизводительных вычислений (HPC) и гипермасштабируемой инфраструктуры вырастут. Согласно IDC, сегодня Lenovo занимает 3 место по объему продаж на мировом рынке производителей серверов на x86-архитектуре, и планирует наращивать свою долю, работая с каждым из этих сегментов, отметил генеральный директор Lenovo Global Technology Россия Дмитрий Паршин. Рынок традиционных инфраструктурных решений для дата-центров является самым большим и высококонкурентным. Хотя этот сегмент сократится, он останется крупнейшим к 2020 году и Lenovo будет продолжать работать с ним. Lenovo не просто обновила свою линейку инфраструктурных решений, но и анонсировала целый ряд новых продуктов, среди них: 4-процессорные высокоплотные серверы SR850, системы хранения данных серии DS, 25 и 100 гигабитные коммутаторы NE2572 и NE10032 соответственно. Кроме того, Lenovo представила самый плотный 8-процессорный x86 сервер в мире SR950, который занимает всего 4U в стойке. Используя новый портфель продуктов, Lenovo может предложить широкий спектр решений для дата-центров, и, что ещё важнее для заказчиков, обеспечить комплексную сервисную поддержку. Новое серверное оборудование построено на процессорах Intel Xeon Scalable. Все решения для традиционной инфраструктуры представлены под новой торговой маркой ThinkSystem. В него входят: серверы, системы хранения данных, коммутаторы. Программно-определяемая инфраструктура В отличие от сегмента традиционной инфраструктуры, остальные направления рынка аппаратной инфраструктуры вырастут к 2020 году. На рынке отмечается тренд — заказчики уже реже покупают компоненты для решения задач по отдельности, выгоднее приобрести всё у единого поставщика и получить при этом комплексную сервисную поддержку. Кроме того, приобретая по отдельности серверы, СХД, коммутаторы и программное обеспечение, их еще необходимо настроить и интегрировать друг с другом. Этот процесс зачастую долгий и сложный. Сейчас же можно заказать преднастроенные и предынтегрированные решения прямо с завода производства: зачастую весь комплекс поставляется уже в стойке, и для начала работы бывает достаточно просто подключить его в сеть. Бренд ThinkAgile представляет собой такие решения для программно-определяемой инфраструктуры. В него входят: — Гиперконвергентные системы — Программно-определяемые СХД — Программно-аппаратные комплексы Гипермасштабируемая инфраструктура Наиболее активное развитие к 2020 году покажет сегмент гипермасштабируемой инфраструктуры – он вырастет на 18%. К гипермасштабируемым дата-центрам относятся комплексы, имеющие минимум 5000 серверов, размещённые на площади более 1000 квадратных метров. Крупнейшие заказчики в этом сегменте – это так называемая “Super 7+1”, в которую входят такие компании как Facebook, Google, Microsoft, Amazon, Baidu, Alibaba, Tencent, AT&T. Системы Lenovo используются в 6 из 7 крупнейших в мире ЦОД. Недавний пример компетенций компании – дата-центр Alibaba в России. Lenovo поставила 1500 кастомизированных, преднастроенных и предынтегрированных серверов, их ввод в эксплуатацию занял всего 7 дней. Высокопроизводительные вычисления и искусственный интеллект Согласно исследованию IDC, в сегменте High Perfomance Computing (HPC) Lenovo – самый быстрорастущий производитель в мире. В 2015 году компания установила 23 кластера, за неполный 2017 – уже 92. Более 100 специалистов в компании по всему миру занимаются продвижением только HPC-решений. Пример внедрений Lenovo HPC в России – сотрудничество с «Гражданские самолёты Сухого». С помощью оборудования компании производятся расчеты аэродинамики, обтекания воздушного судна в условиях обледенения, оптимизация компоновки самолёта и двигателя. Клиентский опыт Дмитрий Паршин отдельно отметил важность клиентского опыта при взаимодействии с Lenovo. Недостаточно производить качественное и производительное оборудование и продавать его по доступной цене. Важно учитывать и другие факторы. Во-первых, скорость поставки. Согласно Gartner, Lenovo входит в ТОП-25 мировых компаний с самой развитой системой поставок. 90% всех SKU компания может поставить за 7 дней. К сожалению, в России это сделать сложно – необходимо учитывать 10-14 дней только на таможенное оформление. Всего у Lenovo 5 заводов по всему миру. Бывают случаи, когда для одного заказчика на хаб дистрибьютора оборудование поставляется одновременно из Мексики, Венгрии и Китая, чтобы затем в одном грузовике его привезти в Россию. Во-вторых, компания уделяет огромное внимание сервису. Lenovo постоянно работает над его улучшением. Например, для всех систем ThinkAgile компания анонсировала новый тип сервисной поддержки: при возникновении проблемы заказчик сразу попадает на уровень L2, минуя L1, и решением сервисного случая сразу займется выделенный специалист. В-третьих, Lenovo продолжает производить самые надежные серверы на архитектуре x86 в мире. Согласно данным очередного исследования ITIC, в котором ежегодно принимают участие более 750 руководителей и ИТ-директоров компаний по всему миру, серверы Lenovo обеспечивают наименьшее время незапланированного простоя в год среди всех х86 в отрасли. Важно отметить, что согласно этому и предыдущим исследованиям ITIC, надежность оборудования System x при переходе в Lenovo сохранилась на том же высочайшем уровне, что и при IBM. Также стоит отдельно сказать об обновлённом интерфейсе управления новыми системами. Он сильно упростился. Новая версия сделана с учётом современных требований и практик пользовательского опыта. Он наглядный, интуитивный, поддерживает несколько языков, а в течение полугода в нем реализуется поддержка русского языка, что часто важно для заказчиков из государственного сектора. Отдельная структура В процессе реорганизации самой компании Lenovo, в 2016 году на должность президента подразделения по производству продуктов для ЦОД был приглашён Кирк Скауген, который до этого работал 24 года в компании Intel и занимал различные руководящие должности, в том числе генерального директора серверного департамента и старшего вице-президента компании. Трансформация Lenovo также привела к выделению подразделения по производству продуктов для ЦОД в отдельную вертикаль внутри компании – Lenovo Global Technology. Если раньше продавцы Lenovo могли предлагать заказчикам сразу все продукты компании от серверов до смартфонов, то сейчас у сотрудников появилась специализация. Сегодня у Lenovo Global Technology есть выделенная организационная структура и менеджмент, отдельная система логистики, отдельный маркетинг. Это позволяет компании сфокусироваться на предоставлении оптимальных решений и услуг для дата-центров. Один из недавних примеров подобных решений Lenovo – модернизация суперкомпьютера для центра вычислений в Барселоне (именно он изображен на заглавной фотографии к статье). Сейчас суперкомпьютер занимает 13 строчку в списке Top500. Задача вычислительного центра – выявление геномов, ответственных за возникновение хронического лимфолейкоза, одного из наиболее распространенных онкогематологических заболеваний. В результате исследования из 20 000 геномов были выделены 4, связанные с этим видом лимфолейкоза. Их дальнейший анализ позволит ученым лучше понять причину возникновения заболевания и приблизиться к нахождению лекарства. Суперкомпьютер был построен с использованием новых продуктов Lenovo на базе процессоров Intel Xeon Scalable еще до официального анонса последних. На видео можно посмотреть весь процесс модернизации суперкомпьютера: Спасибо за внимание! О новых системах ThinkSystem и ThinkAgile расскажем во второй части статьи.\n","id":30}
{"Host":"https://habr.com","Path":"/ru/news/693642/","Text":"Учёные создали динамическую наноструктуру, меняющую оптические свойства при внешнем воздействии / Хабр                 \n\nСотрудники физического факультета ИТМО разработали динамическую наноструктуру, в которой можно менять оптические свойства с помощью внешних воздействий. Основа материала — полимер, который может сжиматься и разжиматься в зависимости от температуры. Учёные ИТМО доказали, что наноструктуры с наночастицами кремния усиливают свет в семь раз, а в сочетании с золотом этот результат улучшается до 35 раз. Причём заставить полимер изменить форму можно неограниченное количество раз. Потенциально такие материалы можно использовать для разработки интеллектуальных автоматических термочувствительных детекторов и в других роботизированных устройствах. Работа выполнена за счёт\nгранта\nРоссийского научного фонда. Результаты исследования\nопубликованы\nв журнале Advanced Optical Materials.\n\n_Визуализация финального шага исследования с использованием наноструктур с наночастицами кремния (слева) и кремния и золота (справа). Источник: авторы исследования / пресс-служба ИТМО_\n\nОптические системы, свойства которых изменяются под влиянием внешних факторов, могут использоваться в разных сферах: от систем обработки изображений до интеллектуальных регулируемых датчиков. Наибольший интерес сейчас представляют микро- и наноразмерные системы. Возможность изменять наноструктуру систем позволит учёным влиять на их оптические свойства.\n\n«Сейчас, если мы создаём какие-либо наноструктуры, чаще всего они никак не изменяются. Например, если мы сделаем массив из наноцилиндров, в дальнейшем мы никак не сможем его переделать, он останется зафиксированным. А мы захотели создать динамическую наноструктуру, которая в зависимости от разных внешних воздействий может менять свой отклик и иметь несколько состояний. Таким образом мы можем менять свойства нашей системы в любой момент времени», — объяснила первый автор статьи и аспирантка третьего года обучения физического факультета Елена Герасимова.\n\nК тому же синтез разработанных наноструктур простой, быстрый и не требует дорогого оборудования для дополнительного воздействия, например, с помощью литографии. Чтобы создать такие материалы, исследователи физического факультета ИТМО используют химические методы. Ещё одно преимущество разработки в том, что наноструктура может менять своё состояние неограниченное количество раз.\n\nВ основе наноструктуры лежат синтезированные гидрогелевые сферические частицы из поли(N-изопропилакриламида) (pNIPAM). Это один из наиболее изученных и часто используемый учёными термочувствительный полимер. При комнатной температуре полимерные частицы впитывают воду (свойство гидрофильности) и увеличиваются в объёме. Но когда температура поднимается до +33 °C, у полимера происходит объёмный фазовый переход, и он вытесняет из себя воду (свойство гидрофобности), уменьшаясь в объёме в два раза.\n\n_Дорожная карта проведённого исследования. A — схема синтеза микросфер pNIPAM (серый), модифицированных либо наночастицами кремния (желтый), либо наночастицами золота (красный) и кремния. B — иллюстрация фазового перехода pNIPAM. C — усиление второй оптической гармоники для pNIPAM, модифицированного кремниевыми наночастицам, и pNIPAM, модифицированного кремниевыми и золотыми наночастицами. Изображение предоставлено авторами исследования._\n\nДалее поверхность полимера исследователи модифицировали разными наночастицами. В одном случае это были наночастицы кремния, а во втором — сочетание наночастиц кремния и золота. Кремний — это диэлектрик с высоким показателем преломления света, а золото — плазмонный материал. Вместе они усиливают электромагнитное поле и, соответственно, оптические свойства наноструктуры. Но как объединить наноструктуру с наночастицами кремния, если у них одинаковый отрицательный заряд? С этой задачей справился ещё один автор статьи, аспирантка первого года обучения физического факультета Лидия Михайлова:\n\n«Мы прибегли к хитрости и модифицировали поверхность частиц катионным полиэлектролитом PAH (полиаллиламина гидрохлорид), который позволил не только скомпенсировать отрицательный заряд поверхности кремния, но и придать полученным частицам положительный заряд», — рассказала она.\n\nОбе наноструктуры рассматривались как в сжатом, так и в увеличенном состоянии. Фазовый переход, который меняет состояние наноструктуры, также может регулировать её оптические свойства, например ― генерацию второй оптической гармоники. Это процесс, при котором наноструктуру облучают светом на одной длине волны, а она излучает свет на вдвое меньшей длине волны. Другими словами, если направить на наноструктуру инфракрасный свет, она будет излучать зелёный свет за счёт своих нелинейных свойств.\n\n«Совмещение наночастиц с разными свойствами позволило нам увеличить эффективность генерации второй оптической гармоники. Когда наноструктура находится в сжатом состоянии, расстояние между наночастицами золота и кремния сокращается, и материал начинает светиться в 35 раз сильнее в сравнении с разжатым состоянием. У наноструктуры с наночастицами кремния показатель интенсивности увеличивается только в семь раз, так как нет свободных электронов, которые появляются при облучении в сочетании с золотом и могут изменить оптические свойства», — отметил младший научный сотрудник физического факультета Виталий Ярошенко.\n\nВ перспективе наноструктуры с улучшенными свойствами генерации второй оптической гармоники можно будет использовать в оптических системах нано- и микромасштаба. Ими могут стать интеллектуальные автоматические термочувствительные детекторы, роботизированные устройства, перестраиваемые оптические метаповерхности и другие схемы, в которых механические изменения в полимерах можно вызвать с помощью внешних воздействий.\n\n«Мы хотели попробовать сделать наноструктуру, которая будет работать автоматически. Сейчас уже существуют материалы, способные менять генерацию второй оптической гармоники, но для них нужно заранее измерить вторую гармонику и вручную растянуть массив из наноструктуры. В зависимости от степени растяжения меняются и свойства второй гармоники. Но если процесс изменения материала будет происходить автоматически, это позволит нам внедрить его в более самостоятельные элементы системы, которые не требуют воздействия человека», — подчеркнула Елена Герасимова.\n\nВ будущем учёным предстоит понять, как перенести наноструктуру в более подходящую среду. Дело в том, что полимер может сжиматься и разжиматься только в воде, поэтому исследования проводились в коллоидных растворах. Одно из возможных решений — это изменение способа синтеза полимера, чтобы он не высыхал на воздухе, а вырабатывал из него влагу, необходимую для изменения состояния.","id":31}
{"Host":"https://habr.com","Path":"/en/company/cloud_mts/blog/422501/","Text":"Новые техпроцессы для производства микросхем все чаще откладывают — почему? / Habr             \n\nВ конце августа производитель полупроводниковых интегральных микросхем GlobalFoundries (работает с AMD)\nпрекратил\nразработку 7-нанометровых техпроцессов. За несколько месяцев до этого компания Intel\nобъявила\n, что вновь откладывает выпуск своего 10-нанометрового чипа.\n\nО причинах этих решений и ситуации в индустрии, рассказываем дальше.\n\n_/ фото Intel Free Press  CC_\n\n## Пара слов о технологических процессах ##\n\nОтдельные транзисторы на чипе формируются\nметодом фотолитографии\n. В этом случае на кремниевую подложку наносят тонкую фоточувствительную полимерную пленку, называемую фоторезистом. Затем этот фотослой обрабатывают светом (производят так называемое экспонирование) через фотошаблон с необходимым рисунком. Проэкспонированные участки смываются в проявителе, а затем производится вытравливание кристаллов.\n\nКомпании уменьшают техпроцессы, чтобы увеличить количество продукции из одной заготовки и снизить энергопотребление финального чипа. Производитель получает возможность увеличить быстродействие микросхемы, оставив её размеры на прежнем уровне.\n\nДолгое время эта тенденция (на уменьшение техпроцессов) оставалась справедливой. Но сейчас ИТ-компании начали откладывать или вообще прекращать разработку новых техпроцессов. Отчасти это связано с удорожанием оборудования и высоким уровнем брака.\n\nПодробнее в ситуации разбираемся далее.\n\n## Почему GlobalFoundries отменили 7-нм ##\n\nGlobalFoundries производят кремниевые пластины на восьми заводах по всему миру. Компания должна была выпустить на рынок 7-нм микросхемы во втором квартале 2018 года. Однако за пару недель до предполагаемого релиза, GlobalFoundries решили всё отменить.\n\nВместо этого, организация\nсосредоточит\nсвои усилия на разработке специализированных норм производства\n14LPP\n(Low-Power Plus) и 12LP (Leading-Performance) и создании различных встроенных\nзапоминающих устройств\n.\n\nПлатформа 14LPP\n— это усовершенствованная версия 14-нм процесса на базе 3D\nFinFET\n-транзистора. Она повышает производительность устройств на 55% и уменьшает их энергопотребление на 60% (по сравнению с 28-нм). А\n12LP\n— это техпроцесс изготовления полупроводников, заточенный под нужды систем ИИ, смартфонов и автомобильной электроники.\n\nПо\nсловам\nCTO GlobalFoundries Гэри Пэттона (Gary Patton), причиной стратегического поворота стали не технические проблемы, а финансовые вопросы. Компания вложила миллиарды долларов в разработку 7-нанометровых микросхем. Первое поколение, в котором используют иммерсионную литографию, было почти завершено. Но на второе и третье (они требовали более глубокие УФ-диапазоны для увеличения плотности транзисторов) средств уже не хватало.\n\nВместе с отменой 7-нм, GlobalFoundries остановили разработку 5-нанометровых и 3-нанометровых техпроцессов. Из-за смены курса GlobalFoundries сократит пять процентов сотрудников и пересмотрит соглашения с AMD и IBM. В частности, с IBM компания поработает до конца года, а дальше прекратит исследования новых техпроцессов.\n\n## Кто еще отложил разработку ##\n\nЕще одной организацией, которая отложила выход чипов по новому технологическому процессу, стала Intel. ИТ-гигант\nпередвигает\nмасштабный релиз 10-нм схем уже два года. В этот раз старт продаж первых 10-нм продуктов\nпередвинули\nна конец 2019.\n\nПо словам\nпредставителей\nкомпании, причина задержки — низкий выход годных процессоров. Есть\nмнение\n, что проблема связана с технологией multi-patterning и применением кобальта.\n\nПроизводственные объемы растут медленнее, чем планировалось. Технически Intel уже поставляет 10-нм микросхемы малыми партиями. Например, первые Core i3-8121U — 10-нм процессоры семейства Cannon Lake — уже\nработают\nв ноутбуках Lenovo. Однако о массовом производстве чипов говорить не приходится.\n\n_/ фото Intel Free Press  CC_\n\nДругие игроки рынка тоже не торопятся ставить разработку новых техпроцессов на поток. В UMC пока\nостановились\nна 14-нм техпроцессе, а в Samsung\nобещают\n7-нм, но тоже не раньше 2019.\n\n## Основные причины ##\n\nКак мы уже говорили, дороговизна перехода — одна из причин, почему GlobalFoundries свернули свои проекты. И по\nмнению\nGartner, она является основной. По оценкам аналитиков стоимость разработки 7-нм технологии составляет примерно 270 млн долларов.\n\nОборудование для\nEUV-литографии\n, нанолисты, экзотические материалы вроде рутения — все эти вещи стоят недешево, но без некоторых из них уже сложно обойтись. Чтобы окупить инвестиции в производство,\nнужно выпускать\nпо 150 млн чипов в год. Поэтому реализация 7-, 5-, 3- и 2-нм процессов может оказаться коммерчески невыгодной.\n\nПри этом даже если микросхему и создают, то «выхлоп» по производительности не всегда оказывается значительным. Например, в Qualcomm\nсчитают\n, что 5-нм процесс не сильно превзойдет 7-нм по характеристикам, а вложить в его разработку придется несколько миллиардов долларов.\n\nВторая причина — велика вероятность ошибки и ее цена. К примеру, задержки в поставках 10-нм техпроцесса Intel «\nвлетели в копеечку\n» одному из ИТ-гигантов с капитализацией в 20 млрд долларов.\n\nРезиденты HN\nвыделяют\nи другие причины замедления прогресса в индустрии полупроводников. Например, один из пользователей полагает, что уменьшение размеров кристалла плохо сказывается на его охлаждении. Поэтому компании стараются вложить средства в разработку более энергоэффективных технологий, а не уменьшение размеров кристаллов (именно по этому пути пошли в GlobalFoundries).\n\n## Почему обновления все-таки нужны ##\n\nПредставители индустрии\nсчитают\n, что уменьшать техпроцессы все равно придется. Это позволит обеспечить эффективную работу систем ИИ, МО, 5G-сетей и IoT. По\nпредварительным расчетам\nразработчиков из TSMC, 7-нм техпроцесс улучшит производительность на 30% и вполовину уменьшит энергопотребление процессора (по сравнению с 10-нм).\n\nОднако Дэвид Хемкер (David Hemker), старший VP в компании Lam Research, производящей полупроводники,\nподчеркивает\n, что отрасли понадобятся новые решения, чтобы справиться с растущими сложностями производственных процессов.\n\n_/ фото Fritzchens Fritz  PD_\n\nПока что EUV-литография генерирует\nслишком много дефектов при производстве чипов\n. Но если продолжить совершенствовать технологию, она\nдолжна сократить\nвремя и расходы на разработку новых техпроцессов.\n\nНесмотря на все сложности производства, некоторые представители индустрии уже делают прогнозы на процессы менее 5-нм и говорят о сроках выпуска таких чипов. Так, например, в TSMC — тоже занимающейся производством полупроводников — уже\nстроят планы\nпо разработке 3- и 2-нм техпроцессов. А исследовательский центр Imec вместе с компанией Cadence Design Systems даже\nразработали\nтестовые образцы микропроцессоров по технологии 3-нм.\n\nПоэтому в будущем мы определённо увидим применение этим технологиям, только этот момент может наступить немного позднее, чем предполагалось изначально.\n\n---\n\nP.S. Дополнительные материалы из Первого блога о корпоративном IaaS:\n\nНовая функциональность в VMware vSphere 6.7\n\nПримеры использования СХД NetApp в различных сферах бизнеса\n\nТестирование дисковой системы в облаке\n\nP.P.S. Статьи по теме из нашего блога на Хабре:\n\nЯпонцы представили прототип процессора для эксафлопсного суперкомпьютера\n\nКомпания IBM представила первый в мире 5-нанометровый чип\n\nПочему компьютерные чипы стали быстрее «стареть» и что с этим делать\n\n---\n\n_Чем мы занимаемся в ИТ-ГРАД: • IaaS  • PCI DSS хостинг  • Облако ФЗ-152_\n\n---","id":32}
{"Host":"https://habr.com","Path":"/en/post/181768/?mobile=yes","Text":"Тестирование производительности Python 2.7 при обработке списков различными способами / Habr             \n\n01-06-2013\nВ ходе одного из моих питоновских проектов, с большой примесью ООП и обработкой большого числа данных — у меня возник вопрос, а насколько эффективно обрабатывать списки в классе с использованием вызовов его методов, или может использовать вызов внешней функции? Для этого были написаны 24 теста которые показали очень интересные результаты, Кому интересна данная тема, прошу в подробности. Тестовый образец: создается список из 500 тыс. элементов и заполняется случайными значениями lst = [] for i in range(1, 500000): val = random() + i lst.append(val) Обработка в 2 этапа: 1) сначала получение нового списка путем возведение в квадрат каждого элемента списка (несколькими способами) 2) потом добавление к полученному списку некоторой константы, которая может быть: локальной, глобально, аттрибутом (для класса) Например так @howlong def process_list_global_func_plus_local_value(): \"\"\"Функция. Обработка в цикле с вызовом глобальной функции и добавлением локальной переменной\"\"\" local_plus_value = GLOBAL_VALUE new_lst = [] for i in lst: new_lst.append(global_func(i)) for v in new_lst: v + local_plus_value Производительность Под производительностью понималось длительность выполнения каждого теста, помереная с помощью декоратора import time def howlong(f): def tmp(*args, **kwargs): t = time.time() res = f(*args, **kwargs) need_time = time.time()-t tmp.__name__ = f.__name__ tmp.__doc__ = f.__doc__ #print u\"%s time: %f\" % ((f.__doc__), need_time) print \".\", return need_time return tmp Были получены следующие результаты по двум группам тестов: Функция. Обработка Генератором без вызова внешних функций и добавлением локальной переменной — 0.192 — 100% Функция. Обработка Генератором без вызова внешних функций и добавлением глобальной переменной — 0.2 — 104% Функция. Обработка в цикле без вызова внешних функций и добавлением локальной переменной — 0.238 — 123% Функция. Обработка в цикле без вызова внешних функций и добавлением глобальной переменной — 0.245 — 127% Функция. Обработка с использованием map и добавлением локальной переменной — 0.25 — 130% Функция. Обработка с использованием map и добавлением глобальной переменной — 0.255 — 132% Функция. Обработка Генератором с вызовом локальной функции и добавлением локальной переменной — 0.258 — 134% Функция. Обработка Генератором с вызовом глобальной функции и добавлением глобальной перем. — 0.274 — 142% Функция. Обработка в цикле с вызовом локальной функции и добавлением локальной переменной — 0.312 — 162% Функция. Обработка в цикле с вызовом локальной функции и добавлением глобальной переменной — 0.32 — 166% Функция. Обработка в цикле с вызовом глобальной функции и добавлением глобальной переменной — 0.327 — 170% Функция. Обработка в цикле с вызовом глобальной функции и добавлением локальной переменной — 0.332 — 172% Класс. Обработка Генератором без вызова внешних функций и добавлением локальной переменной — 0.191 — 100% Класс. Обработка Генератором без вызова внешних функций и добавлением значения глобальной пер. — 0.20 — 104% Класс. Обработка Генератором без вызова внешних функций и добавлением значения аттрибута — 0.213 — 111% Класс. Обработка вызовом локальной функции и добавлением локальной переменной — 0.312 — 163% Класс. Обработка вызовом глобальной функции и добавлением локальной переменной — 0.318 — 166% Класс. Обработка вызовом локальной функции и добавлением глобальной переменной — 0.318 — 166% Класс. Обработка вызовом глобальной функции и добавлением глобальной переменной — 0.328 — 171% Класс. Обработка вызовом локальной функции и добавлением значения аттрибута — 0.333 — 174% Класс. Обработка вызовом глобальной функции и добавлением значения аттрибута — 0.34 — 178% Класс. Обработка вызовом метода класса и добавлением локальной переменной — 0.39 — 204% Класс. Обработка вызовом метода класса и добавлением глобальной переменной — 0.398 — 208% Класс. Обработка вызовом метода класса и добавление значения аттрибута — 0.411 — 215% Выводы: Наибольший интерес представляю процентные разницы. По обработке списка функцией 1) Самая быстрая обработка списка — в генераторе без вызова внешних функций и с использованием локальных переменных, с использованием глобальных переменных будет на ~5% дольше 2) при обработке списка перебором в цикле for время работы будет на 23% с использование локальных переменных и на 27% дольше с использованием глобальных переменных, чем при обработке через генератор 3) при использовании в цикле for для обработки списка внешней локальной или глобальной функции скорость будет ниже более чем на 60% 4) при использовании map в качестве функции обработки элементов списка практически такой же как ри использовании вызова внешней функции в генераторе 5) самым долгим способом обработки списка является обработка в цикле с использование вызова внешней глобальной функции По обработке списка в классе 1) в классах доступ к аттрибутам дольше чем к локальным переменным, примерно на 10%, и на 5% дольше чем к глобальным переменным 2) в классах доступ к методам дольше, чем к локальным функциям примерно на 22% и на 20% дольше чем к глобальным функциям 3) при обработке списка в классе с использованием генератора получается также быстро, как и с использованием функции с генератором 4) самым долгим способом (более чем в 2 раза дольше) оказалось использование вызова метода в цикле и добавлением значения аттрибута, что явилось для меня большим удивлением Код доступен в одном файле по адресу http://pastebin.com/rgdXNdXb Дальше планирую исследовать скорость доступа к элементам списков, туплов, различных видом словарей, а потом вопросы целесообразности кеширования функций и методов классов\n","id":33}
{"Host":"https://habr.com","Path":"/en/post/402653/?mobile=no","Text":"У робомобилей есть проблемы с велосипедистами / Habr           \n\n27-03-2017\nРобомобили отлично отслеживают другие автомобили, и у них всё лучше получается замечать пешеходов, белок и птиц. Главной проблемой остаются лишь самые лёгкие, тихие и юркие средства передвижения. «Задача обнаружения велосипедов – пожалуй, самая сложная из задач, с которыми сталкивается разработка систем для робомобилей», – говорит инженер-исследователь Стивен Шладовер из Калифорнийского университета в Беркли. Нуно Васкончелос [Nuno Vasconcelos], эксперт по компьютерному зрению из Калифорнийского университета в Сан-Диего говорит, что проблема обнаружения велосипедов сложна из-за их сравнительно малого размера, скорости и разнообразия. «Машина – по сути, большой блок из вещества. Масса велосипедов гораздо меньше, и выглядеть они могут по-разному – у них много форм, расцветок, и бывает, что люди увешивают их барахлом». Поэтому точность обнаружения автомобилей в последние годы превзошла точность обнаружения велосипедов. Большая часть улучшений происходила при обучении систем, в которых они изучали тысячи фотографий с промаркированными объектами. И большая часть обучения концентрировалась на изображения автомобилей, а не велосипедов. Возьмём алгоритм Deep3DBox, недавно представленный исследователями из Университета им. Джорджа Мейсона и разработчиком роботакси Zoox из Менло-Парк. На общепринятом в индустрии тесте системы, в котором она пытается разбирать двумерные изображения, Deep3DBox определила 89% автомобилей. Несколько лет назад такие системы справлялись не более, чем на 70%. Deep3DBox также хорошо справляется с более сложной задачей: с предсказанием того, в какую сторону едет транспорт и с генерацией трёхмерного контейнера для объектов на двумерной картинке. «Глубокое обучение обычно используется для простого обнаружения последовательностей в пикселях. Мы придумали эффективный способ использования этой технологии для определения геометрических свойств объектов», – говорит участник проекта Яна Кошецка [Jana Košecká], программист из Университета им. Джорджа Мейсона. Но система заметно хуже справляется с обнаружением и ориентацией велосипедов и велосипедистов. Deep3DBox – одна из лучших систем, но в тестах она распознаёт только 74% велосипедов. И хотя она может правильно ориентировать более 88% автомобилей на картинках, в случае с велосипедами это получается у неё лишь в 59% случаев. Кошецка говорит, что коммерческие системы лучше справляются с этой задачей, когда разработчики получают доступ к огромным наборам изображений, полученных на дороге, при помощи которых можно тренировать компьютер. По её словам, большая часть пробных робомобилей дополняет к обработке изображений лазерное сканирование (лидар) и радары, которые помогают распознавать велосипеды и их положение относительно робомобиля, даже если они ничего не сообщают по поводу его ориентации. Свершиться новым технологическим прорывам помогают карты высокого разрешения – например, \"Road Experience Management\" от израильской компании Mobileye. Такие карты дают компьютеру преимущество для распознавания велосипедов, поскольку эти велосипеды выглядят, как аномалии на предварительно записанных изображениях дороги. В компании Ford Motor говорят, что трёхмерные карты высокой детализации лежат в основе 70 пробных робомобилей, которые она планирует выпустить на дороги в этом году. Соберите всё это вместе, и можно получить довольно впечатляющие результаты – и они были продемонстрированы в прошлом году устройствами от Google. Waymo, компания, отколовшаяся от отдела робомобилей Google, продемонстрировала собственную технологию сенсора, улучшающего способность системы распознавать велосипеды. Васкончелос сомневается, что имеющиеся сегодня системы для распознавания объектов и автоматизации способны заменить водителей-людей, но верит, что они уже достаточно развиты, чтобы помогать людям избегать ДТП. Распознавание велосипедистов уже начинают ставить в качестве дополнения к коммерческой системе автоматического торможения (AEB), устанавливаемой на обычные автомобили, и способной распознавать не только автомобили, но и пешеходов с велосипедистами. Первую AEB-систему, распознающую велосипедистов, предложила компания Volvo в 2013 году. Она обрабатывает данные с камеры и радаров, предсказывая возможные столкновения. Подобная технология в этом году будет проходить обкатку на европейских автобусах. Ожидается, что другие автопроизводители подтянутся вслед за этим, поскольку европейские регуляторы начинают оценивать AEB-системы по качеству распознавания велосипедистов в следующем году. Но такие системы всё ещё страдают от серьёзных ограничений, из которых следует очередная сложная задача для разработчиков: предсказание направления движения движущихся объектов. Особенно сложно будет вытащить ещё больше данных из AEB-систем, распознающих велосипедистов – как говорит Олаф Оп ден Камп [Olaf Op den Camp], старший консультант в Нидерландской организации прикладных научных исследований. Оп ден Камп, руководивший разработкой европейского теста для AEB-систем с распознаванием велосипедов, говорит, что именно движения велосипедистов предсказать тяжелее всего. Кошецка соглашается с ним: «Велосипедисты гораздо менее предсказуемы, чем машины, поскольку им гораздо легче делать внезапные повороты или выскакивать из ниоткуда». А это значит, что пройдёт немало времени, прежде чем велосипедисты смогут избегать человеческих ошибок, с которыми связано 94% ДТП, если верить регуляторам из США. «Все велосипедисты с радостной надеждой ждут этого момента», – говорит Брайан Вайденмейер [Brian Wiedenmeier], исполнительный директор Коалиции велосипедистов Сан-Франциско. Но он говорит, что правильным будет подождать до тех пор, пока технологии автоматизации повзрослеют. В декабре Вайденмейер предупреждал, что представленные компанией Uber Technologies роботакси нарушали правила дорожного движения Калифорнии, которые специально были разработаны для защиты велосипедистов от легковых и грузовых автомобилей, пересекающих выделенные велодорожки. Он поддержал отзыв регистраций таких автомобилей после того, как компания отказалась получать на них разрешения. Uber пока ещё тестирует свои робомобили в Аризоне и Питсбурге, и недавно получил разрешение на возвращение некоторых автомобилей на улицы Сан-Франциско, но исключительно в качестве машин разметки, за рулём которых обязательно будут находиться водители. Вайденмейер говорит, что Uber торопится выйти на рынок, и это неправильно. Он утверждает: «Как любую новую технологию, эту следует очень аккуратно проверять».\n","id":34}
{"Host":"https://habr.com","Path":"/ru/post/409305/?mobile=no","Text":"Звезда в созвездии Рыб «пожирает» планеты и засоряет пространство вокруг себя пылью и газом / Хабр                                                  \n\n24-12-2017\nСпутник XMM-Newton — один из научных инструментов, дающих ученым возможность расширить наши представления о Вселенной. Группа астрономов из США, изучая звезду RZ Piscium, обнаружила , что ее странное изменение светимости может быть вызвано наличием огромных газопылевых облаков на различных орбитах. Скорее всего, эти облака — следы разрушенных ранее планет, хотя есть и другие предположения. «Наши исследования показывают, что массивные скопления пыли и газа в случайном порядке блокируют свет звезды, изменяя ее светимость при прохождении перед диском относительно земного наблюдателя», — заявила Кристина Пунци, одна из участников исследования. Она говорит, что хотя у наблюдаемых изменений могут быть и другие причины, но ученые считают, что в этой системе именно пыль и газ являются основными причинами динамики светимости светила. Ну а сами газопылевые облака могут быть следствием уничтожения существовавших около звезды планет на своих орбитах. Правда, причины уничтожения планет точно неизвестны — тут ученые могут лишь высказывать свои предположения. Звезда RZ Piscium находится в 550 световых годах, в направлении созвездия Рыб. Она то теряет часть своей светимости на несколько дней, то начинает светить в 10 раз ярче. Эта звезда излучает гораздо больше энергии в инфракрасном диапазоне, больше, чем излучает, например, Солнце или подобные ему звезды. Именно это «энергетическое смещение» в сторону инфракрасного спектра является основным доводом американских ученых в плане наличия большого количества пыли и газа в этой системе. Лишь у четырех известных ученым звезд такого же класса светимость в инфракрасном спектре выше, чем у этой звезды. Ученые уже довольно давно наблюдают за этой звездой, причем ранее считалось, что на окраине этой системы есть огромный пояс астероидов, который и приводит к флуктуациям со светимостью. Были и другие версии. Например, некоторые ученые высказали предположение, что сейчас эта звезда находится в своеобразном переходном периоде и вскоре превратится в красного гиганта. Пыль и газ — это следы уничтоженных увеличивающейся в диаметре звездой планет, которые некогда курсировали по своим орбитам вокруг звезды. Так что собой представляет эта звезда — молодого жителя Вселенной или уже умудренного миллиардами лет существования старца? По мнению Пунци и ее коллег, у этой звезды есть и те, и другие признаки. Такой вывод сделан учеными после наблюдения за звездой при помощи спутника Европейского космического агентства XMM-Newton, а также 3-метрового телескопа рефлекторного телескопа Шейна в Ликской обсерватории, Калифорния, и 10-метрового телескопа из обсерватории W. M. Keck, расположенной на Гавайских островах. Научный аппарат XMM-Newton был запущен в 1999 году с космодрома Куру во Французской Гвиане. body, html { margin: 0; } function sendHeight(element) { if (element.offsetHeight > 0) { window.parent.postMessage({ sentinel: 'amp', type: 'embed-size', height: element.offsetHeight + 20, id: '61237112da3d598f9a4a531f', }, '*'); } } function sendScroll() { window.parent.postMessage({ sentinel: 'amp', type: 'embed-scroll', id: '61237112da3d598f9a4a531f', }, '*'); } document.addEventListener('DOMContentLoaded', function(){ var element = document.getElementById(\"habr-embed\"); var erd = elementResizeDetectorMaker({ strategy: \"scroll\" }); window.addEventListener('message', function(e) { if (e.data && e.data.type === 'scroll') sendScroll(); sendHeight(element); }); erd.listenTo(element, function(element) { sendHeight(element); }); }); Ученые подсчитали, что температура верхних слоев звезды составляет около 5300 градусов Цельсия, это немногим меньше, чем у Солнца (около 6000 градусов). При этом звезда богата литием, который обычно постепенно уничтожается в ходе термоядерных реакций при старении звезды. «Количество лития в составе звезды снижается с течением времени, так что этот элемент может служить своеобразными часами, которые отсчитывают время с момента рождения объекта», — говорит Джоель Кастнер, руководитель RIT's Laboratory for Multiwavelength Astrophysics. «Мы подсчитали, что возраст звезды составляет от 30 до 50 миллионов лет». То, что звезда действительно молодая, сомнений практически не вызывает. Но откуда тогда столько пыли и газа в пределах досягаемости гравитационного поля RZ Piscium? Есть предположение, что звезда постепенно уничтожает планеты-газовые гиганты, возникающие в этой системе. Например, просто сдувает вещество этих планет, чем и «засоряет» свое окружающее пространство. А может, часть вещества была захвачена с проходящей мимо RZ Piscium звезды-соседки. Сейчас большая часть газа и пыли в этой системе находятся на расстоянии, примерно равном расстоянию от Меркурия до Солнца. Статья ученых была опубликована в 21 декабря в издании «The Astronomical Journal».\n","id":35}
{"Host":"https://habr.com","Path":"/en/post/376941/?mobile=yes","Text":"Рассвет и закат электромобилей: первая половина XX века / Habr             \n\n11-03-2015\nЭлектромобиль Detroit Electric 1907 года преодолевал без подзарядки 130 километров, а BMW i3 выпуска 2013 года без подзарядки преодолевает… 130 километров. Как и новенький VW Golf с электрическим двигателем. Только Tesla выбивается из списка, так как Model X проезжает 426 километров на одном заряде. О том, как выглядел рынок электромобилей в начале XX века и почему в какой-то момент все пересели на авто с двигателями внутреннего сгорания — читайте под катом. Весь цикл статей: Заря электромобилей: XIX век Рассвет и закат электромобилей: первая половина XX века Электромобиль наносит ответный удар: вторая половина XX века Месть электромобиля: начало XXI века Самым многообещающим типом автомобиля в будущем можно считать электрический, но пока он ещё недостаточно усовершенствован. Электрические двигатели не дают ни шума, ни копоти, они, бесспорно, удобнее и совершеннее всех других… Энциклопедический словарь Брокгауза и Ефрона. В прошлой статье мы остановились на том, что в начале XX века электромобили пользовались особенной популярностью в Нью-Йорке. В России рассвет электромобилей запаздывал, не смотря на старания отдельных инженеров. В начале века по Москве уже ездил такой омнибус от фирмы «Дукс» на десять человек. А Ипполит Романов, изобретатель двухместного электромобиля, просил у Санкт-Петербургской Думы разрешить открыть десять маршрутов — для восьмидесяти омнибусов. Такой поворот событий не устраивал владельцев конки и извозопромышленников — те делали все возможное, чтобы проект Романова не состоялся. К сожалению, Романов в этой борьбе проиграл. На 1914 год в России было 8 электрических экипажей: 4 грузовика, 1 трехколесный фургон и 3 частных легковушки. Фирма «Дукс» пыталась производить такой автобус для передвижения по рельсам. Поскольку в США в начале века электромобилей и авто на паровой тяге было немало, зарядные станции там встречались также часто, как и автозаправки. Электромобиль на зарядке у какого-то сарая. Сенатор США от Род-Айленда Джордж Уитмор на электромобиле, 1906 год. С 1907 года в Детройте начали производить автомобили под маркой Detroit Electric. Эти электромобили собирали до 1939 года. Изначально машины оборудовали свинцово-кислотными аккумуляторами, но с 1911 по 1916 годы можно было выбрать версию с железо-никелевым аккумулятором Эдисона. Максимальная скорость составляла 32 км/ч, а проехать авто мог 130 километров. Во время Первой мировой войны цены на бензин были высоки, поэтому в 1910-е годы электромобили пользовались особым спросом — компания продавала до двух тысяч единиц в год. В 1920-е продажи снизились из-за снижения цен на авто с двигателем внутреннего сгорания. На этой фотографии — Томас Эдисон у автомобиля этой марки. Владели компанией Томас Эдисон, Джон Рокфеллер и жена Генри Форда — Клара Форд. EV-Opera-Car, модель 68/17 B. Модель 1915 года. Печатная реклама автомобилей Detroit Electric 1920-х годов. Гибридные автомобили придумали не вчера и не позавчера. В 1916 году Клинтон Эдгар Вудс начал производство автомобиля Woods Dual Power Model 44 Coupe с электродвигателем и двигателем внутреннего сгорания — сразу двумя. В СССР в 1935 году построили электромобиль на базе ГАЗ-А. В том же году в МЭИ построили аккумуляторный мусоровоз, переделав в него ЗИС-5. Машину оснастили сорока аккумуляторами общей ёмкостью 168 ампер-часов, их масса составляла 1400 кг. Такой электромобиль был способен перевозить мусор массой 1800 кг со скоростью 24 км/час на сорок километров. Мощность двигателя — 13 кВт. Шведский грузовой автомобиль с электродвигателем, 1943 год. Nissan TAMA с 1947 года производился в пассажирской и в грузовой версиях. Это был первый автомобиль с электрическим двигателем Nissan. Четыре электромобиля НАМИ-ЛАЗ грузоподъемностью 0,5 и 1,5 тонн с 1948 года использовала московская почта. Еще десять опытных образцов до 1958 года возили почту в Ленинграде. Пик производства электромобилей попал на 1910-е годы. От электромобилей не было гари и копоти, женщины выбирали их, так как не нужно было обладать недюжей физической силой, чтобы запустить их — в отличии от двигателей внутреннего сгорания. Какие же были недостатки у электромобилей? Дороги стали слишком хорошие, люди хотели ездить далеко — а в начале века было сложнее расставить через каждую сотню километров зарядные станции. Без подзарядки электромобили могли проехать сто-сто тридцать километров, как и некоторые машины сейчас. Благодаря Генри Форду цены на автомобили снизились, а рост добычи нефти вызвал снижение стоимости топлива. Электрический стартер — его разработали в 1912 году — сделал автомобили с двигателем внутреннего сгорания более удобными. С тридцатых годов производство электромобилей перестало быть массовым, не смотря на попытки использовать их в служебных целях. В 1960-1970-е годы люди задумались об экологии и снова вспомнили про электрические двигатели. Но это уже другая история…\n","id":36}
{"Host":"https://habr.com","Path":"/en/post/198472/?mobile=no","Text":"Как я был идеальным заказчиком / Habr           \n\n22-10-2013\nЭто поучительный рассказ о том, как после многих лет работы на стороне исполнителя мне довелось побывать по ту сторону баррикад и заказывать разработку на стороне. Это рассказ о том, почему для разработчика нет ничего страшнее идеального заказчика. Как я уже говорил, я много лет занимался работой с заказчиками в софтверной компании. Так вот – мне приходилось работать с разными заказчиками – с отечественными и с зарубежными, с международными корпорациями и со стартаперами-одиночками. Я видел всё, что только бывает. Я видел пафосных московских менеджеров, к которым надо было приезжать раз в неделю, потому что говорить по телефону – это западло, и видел простого американского парня, который взял и прилетел на месяц к нам в Сибирь под новый год, чтобы лично объяснить разработчикам, чего он хочет. Я работал со странными людьми, которые составляли ТЗ в половину странички, с неадекватами, которые переписывали спецификацию раз в неделю, с мудаками, которые считали себя рабовладельцами. Мы делали фичи, которые не поддерживало железо, разрабатывали приложения под ось, которая еще не вышла и адаптировали под ретину дизайн, нарисованный ребенком в редакторе Paint. Я работал с психами, которые грозились миллионными штрафами за день просрочки промежуточного релиза. Я имел дело с перекупщиками, которые понятия не имели, чего хочет конечный покупатель, я встречал неадекватов, которые понимали как надо только после того, как мы сделаем, как просят. Я работал с заказчиками, которые «я вообще-то тоже программист» и пытались учить нас делать свою работу. Я знаю, что такое переделывать всё с нуля по три раза за проект. Однажды у меня был заказчик, соскочивший с проекта, потому что у него сгорел офис со всем железом и данными. Однажды нам пришлось за 200 баксов делать клон, хотя нет – продвинутый клон родного яблочного приложения в то время, когда они еще не открыли сторонним разработчикам доступ ко многим своим фичам. В общем, я работал со всеми видами невозможного и невыполнимого. Я понимаю, каково делать то, не знаю, что, так, чтобы еще вчера было готово. Так вот — каждый раз, когда я встречал очередного «чего там работать, сделайте как в фейсбуке» клиента, я давал себе слово, даже нет – я клялся могилами предков, что вот уж я бы на его месте так себя не вел. Я бы на его месте работал так, что разработчик еще и приплачивал бы за удовольствие иметь со мной дело. Уж я бы на его месте мог бы стать просто самым лучшим заказчиком. И однажды я им стал. Как и любая софтверная компания, однажды мы захотели сделать «свой продукт». И как в любой софтверной компании у нас не было свободных разработчиков. Поэтому было принято героическое решение применить модный аутсорсинг. Ну а я уже стал проектировщиком и проджект-менеджером. Немного поторговавшись, мы нашли исполнителя, заключили настоящий договор, и я официально стал заказчиком. Как и клялся, я стал самым лучшим из всех заказчиков. Я платил нормальные деньги. Такие, за которые мы сами стали бы работать. У меня был настоящий бэклог, содержащий четкие, конкретные и, самое главное, непротиворечивые требования. Я не менял требования в течение проекта, не вмешивался в спринты и не просил что-то переделать потому что плохо подумал. Исполнитель мог сам выбирать технологии разработки, архитектуру и писать код по своим правилам. У нас не было испорченного телефона, я был основным контактным лицом и я же формировал требования. Никакого языкового барьера и проблем с доступностью для общения. У меня была внятная спека — каждый экран был отрисован профессиональным UX-дизайнером, поведение каждой кнопочки было расписано. Я сам проводил приемочные тесты. Я знал реалии разработки, понимал что у всех бывают баги и задержки. Я не был программистом, который знает как надо. В общем, во всех отношениях я старался быть таким заказчиком, о котором сам всегда мечтал. Но со временем я стал замечать, что мой подрядчик не разделяет моего восторга. Мне начало казаться, что для него я просто ночной кошмар. Вероятно, он считал меня своим худшим клиентом и проклинал тот день, когда прочитал моё объявление. Прошло немало времени, прежде чем я понял, почему со мной так тяжело работать. И это понимание полностью изменило мое представление об отношениях заказчика и исполнителя в сфере ИТ. Во-первых, я слишком хорошо ориентировался в требованиях. Соответственно меня было тяжело убедить в том, что какие-то детали нуждаются в дополнительных пояснениях или примерах и получить таким образом дополнительное время. Любые пояснения и примеры приводились сразу же. Во-вторых, я был конечным заказчиком – именно заказчиком, даже не представителем. Поэтому со мной было невозможно договориться не заметить каких-то багов или подписать акты под честное слово всё поправить потом. Ну и самое главное. Я слишком хорошо представлял себе результат, который хотел получить. Это самое парадоксальное, потому что обычно проблема как раз в обратном. Дело в том, что в моём случае всё проектирование было на нашей стороне. Подрядчик получал готовые и свёрстанные страницы с подробными пояснениями в стиле «если нажать сюда, то появляется вот такой попап». Я думаю, любой согласится, что чем абстрактнее требования, тем проще работать. А мои требования были настолько конкретизированы, что разработчик полностью лишался пространства для манёвра. И если он не мог что-то реализовать (особенно на фронт-енде), то у него просто не было возможности сделать по-другому – сделать так, как умеет. В результате любую проблему приходилось решать самым сложным способом, потому что обходной путь всегда был отрезан. Так и получилось, что моё самое главное достоинство обернулось самым главным недостатком. Какая в этом мораль? Такая, что нельзя разделять проектирование и разработку. Идеальный заказчик не должен в деталях прописывать поведение каждого контрола. Идеальный заказчик должен описать задачи, которые приложение должно решать и условия, в которых оно должно это делать. Всё остальное разработчик должен делать сам – только тогда появляется шанс, что он сможет воплотить свои собственные идеи. Потому что с чужими идеями всё гораздо сложнее.\n","id":37}
{"Host":"https://habr.com","Path":"/ru/articles/727970/","Text":"Криптовалюта — катализатор развития искусственного интеллекта и блокчейна / Хабр                                                            \n\nС появлением криптовалют началось активное развитие альтернативной системы расчётов. Теперь финансовые транзакции между сторонами могут осуществляться напрямую, без промежуточных посредников в виде банков. В результате косвенные издержки (комиссионные) сокращаются, степень анонимности платежей возрастает. Далее разберём, как распространение новых денег влияет на развитие технологии блокчейна и искусственного интеллекта.\n### Роль блокчейна в системе расчётов, как это связано с ИИ ###\nПри осуществлении криптовалютных транзакций используется блокчейн (реестр операций), представляющий собой неразрывную цепь блоков. В каждый такой элемент записывается информация о совершённых операциях. Внесённые сведения не могут быть удалены или изменены. Вместимость блоков ограничена — на создание новых требуется время и вычислительные мощности.\nПопуляризация криптовалют ведёт к увеличению доли данного вида расчётов как среди физических лиц, так и между компаниями. Постепенно операции рассматриваемого типа начинают использоваться на уровне государств. Нагрузка на сети возрастает, что ведёт к повышению стоимости и времени переводов. Решение проблемы в совершенствовании технологии блокчейна, увеличении вместимости и скорости генерации отдельных блоков.\nПример: один блок в блокчейне биткоина создаётся примерно за 10 минут, имеет фиксированную ёмкость 1 мб. Таких характеристик оказалось недостаточно в изменившихся условиях, было выдвинуто ряд предложений по совершенствованию классической технологии. Из-за возникших разногласий 1 августа 2017 произошло разделение (хардфорк) основной сети на две параллельные, появился Bitcoin Cash. Ёмкость блока новой криптовалюты возросла до 8 мб.\nБолее совершенный форк — Litecoin: разработчики увеличили скорость формирования блоков в 4 раза (создаётся за 2,5 минуты) посредством упрощения процесса добычи — майнинга.\nСледующий шаг — замена традиционного майнинга “умными” алгоритмами Proof-of-Stake (сентябрь 2022). На примере Ethereum, за формирование и одобрение блоков отвечают валидаторы, участники системы, внесшие в стейкинг от 32 ETH. С целью оптимизации пул валидаторов разделён на комитеты (от 128 до 2048 участников в каждом). Процедура консенсуса делится на “эпохи” по 6,4 минуты, состоящие из 32 таймслотов по 12 секунд. Каждый новый блок создаётся в рамках таймслота случайно выбранным ИИ валидатором, роль остальных сводится к голосованию за принятие/непринятие.\nВнедрение указанной технологии обусловлено повсеместным распространением блокчейна Ethereum. Частые перегрузки сети привели к неадекватно высоким комиссиям в пиковые моменты. Переход на Proof-of-Stake становится решением проблемы, но с сохранением децентрализации могут возникнут проблемы. Валидаторы — это крупные держатели криптовалюты, число которых ограничено. При участии в системе большого количества финансовых учреждений и государственных институтов сеть перестаёт быть свободной.\nЛокомотивом для совершенствования блокчейна выступает возрастающая роль криптовалют в международных финансовых расчётах. Прочие направления имеют вспомогательное значение, но также влияют на рассматриваемые процессы. Причины объективны: глобализация мировой экономики предполагает сведение в минимум количества посредников и сокращение национального государственного регулирования. Многие крупные корпорации являются транснациональными, нуждаются в независимых каналах для совершения транзакций.\nВместе с распространением криптовалют в мире увеличивается энергопотребление, повышается спрос на ключевые материалы для микроэлементов и полупроводников — редкоземельные и цветные металлы. В будущем потребуются более совершенные способы генерации и добычи, что становится стимулом для развития технологий альтернативной (зелёной энергетики), внедрения новых производственных стандартов — иначе обеспечить сокращение выбросов в атмосферу не получится.\nПо информации от Организации экономического сотрудничества и развития, большая часть операций с использованием цифровых знаков (токенов) приходится именно на банкинг и финансовый сектор в целом (30 %).\n\nПри этом с помощью блокчейна могут быть переданы права на владение любым активом: акциями, золотом, недвижимостью и т. п. Синхронизация данных обо всех операциях достигается за счёт механизма консенсуса. Участвующие в сети узлы и ноды приводятся в соответствие автоматически.\nИспользование искусственного интеллекта даёт возможность автоматизировать арбитраж и руководство в процессе принятия решений, обеспечить масштабируемость без ущерба безопасности (синхронизация расчётов), повысить качество взаимодействия между майнерами/валидаторами за счёт оптимизации стратегий. Чем сложнее становится блокчейн, тем больше функций переходит ИИ, что стимулирует совершенствование применяемых программ.\n### Блокчейн, искусственный интеллект и криптовалюта в новой реальности ###\nВ последние несколько лет активно развиваются проекты метавселенных. Виртуальные миры совершенствуются, предлагая пользователям реалистичные аналоги жизни. Присутствуя посредством аватара, можно взаимодействовать с другими участниками игры, выполнять задания, приобретать цифровые земельные участки и другие активы. За управление метавселенной, включая генерацию игровых процессов, создание реалистичного окружения, отвечает искусственный интеллект.\nСобственность реализована как NFT — невзаимозаменяемые токены, подтверждающие право на владение. Для внутренних расчётов используются криптовалюты, которые можно приобретать за ликвидные цифровые монеты и привязанные к курсовой стоимости фиата стейблкоины. Все виды упомянутых токенов создаются на блокчейне. Криптовалюта является единственным адекватным средством расчётов между жителями виртуальных миров.\nПоявление криптовалюты создало необходимые условия для совершенствования и развития метавселенных, привлечения в виртуальные миры крупных инвестиций. Как следствие, возросли требования к технологиям блокчейна и ИИ.\nЦифровое искусство — ещё одна сфера применения невзаимозаменяемых токенов. Известные бренды анонсируют эксклюзивные коллекции предметов и параллельно выпускают их виртуальные копии (NFT). Современные художники также используют технологию, создавая оригинальные произведения в цифре. Данный вид творчества востребован среди коллекционеров, наиболее удачные работы приобретаются с целью инвестирования. Также на блокчейне создаются массовые виды NFT — оригинальные твиты. При этом ИИ отвечает за точность и качество исполнения, блокчейн служит для генерации, а криптовалюта является основным способом покупки активов.\nЗапись музыки с дальнейшим переводом в формат NFT позволяет авторам обойтись без посредников, напрямую продвигая свои треки через интернет. Для поклонников расширяются возможности коллекционирования. Направление не сможет полностью заменить классические площадки, но становится востребованным дополнением.\nРазвитие криптовалют ведёт к совершенствованию ИИ и блокчейна, что является необходимым условием для выпуска уникальных NFT с идеально выверенными параметрами, высокой степенью детализации. Без появления и распространения нового вида денег возникновение данной технологии было бы невозможно.     Перспективы\nПервоначальная реакция правительств стран на появление криптовалют была неоднозначна. Децентрализованные деньги пытались запрещать, теперь активно обсуждаются вопросы регулирования. Сама суть технологии блокчейна не позволяет полностью контролировать процесс выпуска и использования криптовалюты, что делает прогресс неизбежным.\nВ перспективе новый способ расчётов будет становиться всё более востребованным. Также схожие решения уже создаются на уровне государств (цифровой рубль, доллар, юань). При этом совокупная капитализация криптовалютного рынка относительно невелика, составляет всего 327 млрд. долларов США. Для сравнения, общий объём деривативов (производных инструментов) 542,4 трлн. долларов, а мировой наличности 7,6 трлн. долларов (данные ОЭСР).\n\nНесмотря на ограниченную эмиссию, криптовалюты имеют кратный потенциал роста, в плане капитализации. Расширение рынка будет подталкивать развитие блокчейна, предъявляя новые требования к ёмкости блоков и скорости их генерации. Усложнение связанных процессов прямо стимулирует развитие искусственного интеллекта.\nФундаментальная стоимость криптовалют обусловлена спросом и доверием участников расчётов. Принципы ценообразования рыночные в чистом виде. В результате исключается зависимость от вмешательства государства в процесс формирования справедливого курса. При совершении международных транзакций такая система является самой честной и прозрачной.","id":38}
{"Host":"https://habr.com","Path":"/en/company/cloud4y/blog/349308/?mobile=no","Text":"Кладбища стартапов полны провидцев без опыта и знаний / Habr             \n\n19-02-2018\nЧасто, когда я посещаю конференции или нетворкинг-ивенты, я удивляюсь, как много людей работают на периферии технической индустрии. Гуру социальных сетей, SEO «ниндзя», блоггеры и тому подобные. Это тусовка промоутеров технического сообщества. «Проруби свой путь к успеху», «познакомься с нужными людьми», «станьте суперзвездой бизнеса». Они нашли свою «серебряную пулю». Они могут похвастаться пассивным доходом от веб-бизнеса, всё время путешествуя по миру, в то время как остальные смертные рабствуют на своих рабочих местах по 9 часов 5 дней в неделю. В мире, где мы ищем серебряные пули, эти люди, похоже, собрали арсенал из них. Более того, они нашли аудиторию, которая готова массово покупать их «серебряные пули». Наиболее наглядным примером этого являются некоторые из последователей идей книги «Как работать по 4 часа в неделю и при этом не торчать в офисе «от звонка до звонка», жить где угодно и богатеть» от Тимоти Феррисса (The 4-Hour Workweek, Tim Ferriss). Негативный момент заключается не в книге самой по себе. Феррис действительно описывает некоторые интересные советы по управлению ресурсами с целью получения максимальной отдачи от работы. Однако, нежелательным, на мой взгляд, является менталитет хакера для достижения успеха, порождённый им в предпринимательских кругах (hack-your-way-to-success mentality). Это мышление противоречит всему тому, что я знаю о предпринимательстве. Это тот образ мыслей, который я вижу, когда люди говорят о том, что у них есть замечательная идея и они хотят отдать разработку на откуп молодому студенту колледжа, который умеет кодить, или передать всю работу на аутсорсинг в регионы с низкой стоимостью труда. Это мышление, которое предполагает, что предпринимательство — это серия мероприятий для создания и укрепления деловых связей и встреч с целью сбора средств, и даже допускают существование некоторых бизнес-связей «серебряных пуль», которые заменят реальную стратегию продаж. Это приводит к пассивному подходу в очень сложном деле. То, что упущено во всем этом, — установка на мастерство; то, что основным драйвером успеха на самом деле являются опыт и целенаправленная фокусировка на повышение квалификации в своей профессии/ремесле, а не некоторая рискованная серия «хаков». Происходящее на периферии — будь то дерзкие шутки, которые мы видим в Twitter от IT-знаменитостей, или заголовки в TechCrunch, на самом деле, не имеют смысла в качестве основы для бизнеса или профессии. Также как и количество запланированных вами встреч или нетворкинг-ивентов. В лучшем случае, эти вещи третичны, а в худшем — просто отвлекающие факторы. Для успеха в карьере требуется применение и накопление опыта. Это предполагает, что для любой компании вы либо предоставляете экспертизу, либо являетесь пассивным наблюдателем. Мастера своего дела являются драйверами. Добавленную стоимость создаёт их опыт, полученный из-за пытливости ума, и серьёзного отношения к своему ремеслу. Запуск бизнеса по своей природе является интенсивным курсом в наборе опыта и экспертных знаний. Уникальность стартапов — это отсутствие ресурсов. Недостаток ресурсов заставляет основателей быстро адаптировать свои навыки для удовлетворения любых потребностей проекта. «Я не знал, как это сделать, поэтому мне просто нужно было понять это», — это то, что я регулярно слышу от успешных основателей, тогда как «я не смог найти кого-то, чтобы сделать X, поэтому мне пришлось подумать, следует ли продолжать вообще» — это общее для основателей безуспешных стартапов. Если вы примите этот вызов, вы поймете, что стартап — это не что иное, как «тренажёр». На самом деле это отличный учитель именно по причине необходимости быстрого накопления знаний для запуска в сжатые сроки для выживания бизнеса. Например, основатель с техническими знаниями и опытом, достаточными для работы в крупной компании, должен адаптироваться и получить больше опыта в смежных технических областях, так как человеческие ресурсов недостаточно, чтобы просто передать эти задачи другому специалисту. Это верно и для решения задач в других нетехнических областях, будь то продажи, финансы, маркетинг, управление или дизайн. Для успешного запуска следует проявлять интерес и к этой деятельности. Заполнить эти роли на ранней стадии будет некому. При исследовании этих неизвестных ранее для них «территорий» основатели сталкиваются со встречным ветром проблем запуска бизнеса, и становится ясно, что стартап — это только про профессиональный рост и развитие характера. При любом уровне фактического успеха венчурного проекта, подчиненного прихоти внешних сил, этот рост является нематериальным дивидендом, который делает этот опыт бесценным. Вот почему идея пассивной 4-часовой недели саморазрушается. Отдых на пляже или путешествия по миру вместо активного участия в создании арсенала собственных знаний и навыков — это профессиональная халатность. К тому же, это не связано с практической применимостью. Никакая серьезная компания не была создана пассивно. Пассивное мышление заставляет людей думать: «У меня отличная идея. Я найму команду, чтобы её реализовать» или «У меня есть знакомство, которое будет двигать продажи вверх, в то время как я играю в кресле провидца». Кладбища стартапов полны провидцев без экспертизы или надлежащих навыков для реализации идей, потому как они не исполняются сами по себе, а реализуются благодаря интенсивному воздействию квалифицированных кадров. Самое главное, что подумав о бизнесе как о серии хаков и транзакционных отношений, вы никогда не соберете опыт, с которым ваш будущий «я» и будущий бизнес смогут преуспеть. Конечно, одной только экспертизы основателей может быть недостаточно. Тем не менее, это то, что позволяет бороться со сложностями при создании бизнеса. Вы не просто работаете над идеей, вы получаете знания, которые помогут вам преуспеть в следующих проектах. Именно опыт и мастерство позволяют кому-то вроде Элона Маска переходить от проекта к проекту и из сектора в сектор, имея знания о том, как решать самые серьёзные проблемы. Это не просто его способность находить интересные идеи. Его знания в разных областях позволяют ему выполнять то, что он делает. Он является воплощением студента, обучившегося многим дисциплинам в своём университете, которым является создаваемый им бизнес. Если вы хотите оптимизировать что-либо, оптимизируйте в долгосрочной перспективе. Используйте сегодняшние проблемы, чтобы стать мастером своего дела. Нет никакой гарантии, что какое-либо предприятие будет успешным, но полученное мастерство будет играть в вашу пользу в течение длительного периода вашей карьеры. P.S. на правах рекламы Несмотря на отсутствие гарантий успеха, существует несколько хорошо зарекомендовавших себя методов, способных помочь предпринимателю избежать риска траты огромных денег и сил ради создания никому ненужного продукта. Бережливый стартап (Lean Startup) — это концепция предпринимательства для быстрого тестирования идей и выбора бизнес-модели. Используя этот подход, компании могут проектировать продукты и услуги, которые бы соответствовали ожиданиям и потребностям клиентов, без необходимости большого объёма первичного финансирования или затратных продуктовых запусков. Бизнес связан с рисками, возникающими в ситуациях неопределённости. Неизвестно будет ли новый продукт пользоваться популярностью у клиентов, хватит ли инвестиций и доходов, чтобы не попасть в разрыв ликвидности. В начале бизнеса важно принимать решения, которые не увеличивают возможные потери. Так, путь покупки собственных серверов и оборудования, которые в случае неудачи придётся продать с большим дисконтом, может быть не самым оптимальным. В условиях высоких рисков вариант аренды или использование, так называемых, услуг IaaS (инфраструктура как услуга) и SaaS (программное обеспечение как услуга) может быть более эффективным. Гибкость и возможность быстрого масштабирования — это ключевые преимущества облака полезные для стартапов, действующих в условиях чрезвычайной неопределенности.\n","id":39}
{"Host":"https://habr.com","Path":"/en/company/ruvds/blog/341878/?mobile=no","Text":"Отладка React-приложений в VS Code / Habr                        \n\n09-11-2017\nПрошли те дни, когда мне, в процессе разработки, приходилось тратить время, переключаясь между терминалом, браузером и редактором. Теперь всё делается иначе — быстрее и удобнее. Сегодня я расскажу об оптимизации повседневных дел React-разработчика. А именно, речь пойдёт о том, как связать Visual Studio Code и Google Chrome. Это даст возможность отлаживать браузерный код в редакторе. Средства отладки VS Code и jest от Facebook Настройка тестового проекта Прежде чем мы начнём осваивать отладку React в VS Code, создадим учебное приложение, с которым будем экспериментировать. Я часто пользуюсь create-react-app, так как очень не люблю вручную создавать пустые проекты. Поэтому предлагаю задействовать его и в этом руководстве. Как вариант, если у вас уже есть приложение, вы можете воспользоваться им. Итак, создадим тестовый проект. Для этого надо сделать следующее: Установите create-react-app глобально, выполнив команду npm i -g create-react-app. После установки создайте проект командой create-react-app vscode-tutorial. В результате у вас должна появиться новая папка с React-приложением. Настройка VS Code Теперь установим расширение VS Code, которое позволит редактору взаимодействовать с Chrome. VS Code подключается к Chome с использованием протокола удалённой отладки. Это — тот же протокол, который используют инструменты разработчика Chrome. Но, благодаря такому подходу, вместо того, чтобы работать со стандартными инструментами Chrome, вы можете использовать для отладки браузерного кода VS Code. Установка расширения Debugger for Chrome Итак, для того, чтобы наладить взаимодействие VS Code и Chrome, нужно установить расширение, которое называется Debugger for Chrome. Для его установки надо перейти на панель расширений и выполнить поиск по названию расширения. В результате должно получиться примерно следующее: Поиск расширения Debugger for Chrome Подключение VS Code к Chrome Далее, нужно настроить VS Code на подключение к Chrome. Делается это так: Щёлкните по значку отладки. Откройте выпадающее меню (около кнопки Play) и выберите пункт Add Configuration…. В выпадающем списке Select Environment выберите Chrome. Если вы запутались — вот как всё это выглядит Настройка связи VS Code и Chrome В корень проекта будет автоматически добавлена папка .vscode. В ней будет находиться файл launch.json, который используется для настройки отладчика VS Code для текущего проекта. Каждый раз, когда вы создаёте новый проект, вам нужно выполнять ту же последовательность действий для настройки удалённой отладки (ещё можно просто скопировать папку .vscode из одного проекта в другой). Модифицируйте в этом файле свойство url для того, чтобы оно указывало на сервер, использующийся при разработке. Для create-react-app это http://localhost:3000. В результате ваш launch.json должен выглядеть так (благодарю Кеннета Аухенберга из команды VS Code за совет по работе с этим файлом): { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Chrome\", \"type\": \"chrome\", \"request\": \"launch\", \"url\": \"http://localhost:3000\", \"webRoot\": \"${workspaceRoot}\" } ] } Полный список конфигурационных опций можно найти здесь. Использование отладчика Теперь почти всё готово к работе. Следующий шаг заключается в использовании тестового проекта для того, чтобы проверить отладчик. Запуск отладчика Запустить отладчик можно, выполнив одно из следующих действий: Нажать F5. Щёлкнуть по зелёной кнопке Play на панели отладки. Выбрать команду меню Debug → Start Debugger. Если отладчик будет успешно запущен, вы увидите, как появится маленькая панель инструментов в верхней части окна VS Code: Панель инструментов, которая появляется при включении отладчика Установка точки останова Точки останова используются для того, чтобы сообщить отладчику о том, что ему нужно приостановить выполнение кода в определённом месте. Это позволяет программисту исследовать переменные, стек вызовов и вносить в код изменения в процессе выполнения приложения. Установим точку останова в тестовом приложении. Откроем файл src/App.js и щёлкнем мышью по полю левее строки 11. Тут должна появиться красная точка, которая указывает на то, что точка останова была добавлена. Вот, чтобы было понятно, анимированная версия этой инструкции: Установка точки останова Запуск сервера разработки И наконец, для того, чтобы посмотреть как всё это работает, нужно запустить сервер разработки, выполнив команду npm start в терминале. Эта команда запустит новый сервер, доступный по адресу http://localhost:3000/. Запуск сервера Перейдите по адресу http://localhost:3000/ и вы увидите, как страница «застынет». Это происходит из-за того, что сработала точка останова. Если перейти в VS Code, можно заметить, что строка 11 будет выделена, а на панели отладки появятся сведения о стеке вызовов. Сработавшая точка останова Если у вас всё заработало — примите поздравления! Вы только что узнали о том, как настроить удалённую отладку в VS Code. Если вы хотите узнать подробности о самом процессе отладки в VS Code — почитайте это. Непрерывное тестирование с помощью jest Мне удобно, чтобы в процессе работы над кодом выполнялись модульные тесты. А именно, чтобы они вызывались всякий раз, когда я вношу изменения в программу. Благодаря create-react-app всё, что нужно для реализации такого сценария, настраивается автоматически. Для того, чтобы запустить jest, достаточно ввести в терминале команду npm test. Благодаря этому система будет наблюдать за файлами и автоматически запускать тесты при их сохранении. Выглядит это примерно так: Непрерывное тестирование в VS Code Итоги В этом материале мы рассказали о том, как настроить взаимодействие VS Code и Chrome для организации удалённой отладки React-приложений. Надеемся, эта простая методика поможет сэкономить немного времени, если раньше вам приходилось постоянно переключаться между редактором, браузером и терминалом. Уважаемые читатели! Пользуетесь ли вы какими-нибудь полезными мелочами, которые повышают производительность труда при разработке веб-приложений?\n","id":40}
{"Host":"https://habr.com","Path":"/en/post/330670/?mobile=no","Text":"Настройка Reverse Proxy Apache (Debian 8) с автоматической выдачей Let's Encrypt / Habr            \n\n10-06-2017\nТак как зачастую, сайтов в организации много, а IP адресов мало, нужно иметь решение с Reverse Proxy. Для моих целей раньше всегда выступал Microsoft TMG, но у него есть свои недостатки, как и плюсы. Один из основных минусов, это то что на TMG нужно подгружать сертификаты публикуемого ресурса, что с Let's Encrypt довольно неудобно, ввиду обновления сертификатов каждые 90 дней. Решение было найдено: поднять Reverse Proxy на Apache и сделать так, чтобы работала автовыдача сертификатов Let's Encrypt. А после чего спокойно публиковать его на Firewall, при этом порты буду перенаправляться с http на https. За основу берем что у нас стоит чистый Debian GNU/Linux 8 (jessie). Подробнее под катом. Ну что-ж, поехали. aptitude install -y build-essential aptitude install -y libapache2-mod-proxy-html libxml2-dev aptitude install -y apache2 После чего активируем следующие модули: a2enmod proxy a2enmod proxy_http a2enmod proxy_ajp a2enmod rewrite a2enmod deflate a2enmod headers a2enmod proxy_balancer a2enmod proxy_html a2enmod proxy_ftp a2enmod proxy_connect a2enmod ssl и рестартуем Apache: service apache2 restart Тут нас поджидает первая неудача, Apach'у для правильной работы не хватает модуля mod_xml2enc, НО! в Jessie этот модуль не работает, нам последовательно нужно внести следующие команды: aptitude install apache2-prefork-dev libxml2 libxml2-dev apache2-dev mkdir ~/modbuild/ && cd ~/modbuild/ wget http://apache.webthing.com/svn/apache/filters/mod_xml2enc.c wget http://apache.webthing.com/svn/apache/filters/mod_xml2enc.h apxs2 -aic -I/usr/include/libxml2 ./mod_xml2enc.c cd ~ rm -rfd ~/modbuild/ service apache2 restart После чего, все у нас хорошо, модуль стоит. Едем дальше ) Так как мы хотим опубликовать HTTPS сайт, до того момента пока мы не установим Let's Encrypt, нам нужно сделать самоподписанный сертификат для нашего сайта, вводим комманду: mkdir /etc/apache2/ssl cd /etc/apache2/ssl openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout server.key -out server.crt Нам нужно создать файл конфигурации и назвать его понятным именем: touch /etc/apache2/sites-available/sambi4.conf И задаем файлу примерно такое содержание: <VirtualHost *:80> ServerName sambi4.ru Redirect permanent / https://sambi4.ru/ #отвечает за перенаправление на https </VirtualHost> <VirtualHost *:443> SSLEngine On SSLProxyEngine On ProxyRequests Off ProxyPreserveHost On ProxyVia full SSLCertificateFile /etc/apache2/ssl/server.crt #указываем путь к нашему самоподписанному сертификату SSLCertificateKeyFile /etc/apache2/ssl/server.key #указываем путь к нашему самоподписанному ключу сертификата ProxyHTMLInterp On ProxyHTMLExtended On <proxy *> Order deny,allow Allow from all </proxy> ProxyPass / https://192.168.199.78/ #IP адрес публикуемого ресурса. ProxyPassReverse / https://192.168.199.78/ #IP адрес публикуемого ресурса. ServerName sambi4.ru ServerAdmin sambi4@sambi4.ru #считается хорошим тоном указывать email админа DocumentRoot \"/var/www/html\" #эта строка нужна для того чтобы апач запустился, без нее он не сможет опубликовать ваш ресурс. </VirtualHost> После завершения создания, не забываем включить наш сайт: a2ensite /etc/apache2/sites-available/sambi4.conf перезапускаем Apache: service apache2 restart После всех проделанных процедур, мы имеем настроеный Reverse Proxy на Apache2, теперь можно приступить к настройке Let's Encrypt: Из всех бесплатных сертификатов, жив остался только Let's Encrypt, но его особенность в том, что сертификат выдается сроком на 3 месяца. Нам нужно поставить сертификат, и сделать автоматическую выдачу при завершении срока сертификации. echo 'deb http://ftp.debian.org/debian jessie-backports main' | tee /etc/apt/sources.list.d/backports.list после: aptitude update ну а теперь ставим сам Let's Encrypt: aptitude install -y python-certbot-apache -t jessie-backports Дожидаемся процесса установки, и пробуем выпустить сертификат: certbot --apache И вот тут нас поджидает неудача: ERROR:letsencrypt_apache.configurator:No vhost exists with servername or alias of: sambi4.ru. No vhost was selected. Please specify servernames in the Apache config Связано это с тем, что в репозитариях до сих пор старая версия (на момент написания 0.10.2), в которой наблюдаются ошибки. А именно ошибки в python-скриптах. Решение как обычно просто: Качаем свежую версию certbot: git clone https://github.com/certbot/certbot.git После чего, идем по пути: cd /usr/lib/python2.7/dist-packages Удаляем папки (а лучше делаем бэкап): acme certbot certbot_apache И копируем файлы из нового релиза: cp /root/certbot/certbot /usr/lib/python2.7/dist-packages/ cp /root/certbot/acme/acme/ /usr/lib/python2.7/dist-packages/ cp /root/certbot/certbot-apache/certbot_apache/ /usr/lib/python2.7/dist-packages/ Теперь можно со спокойной душой запускать процесс выпуска сертификата: certbot --apache Отвечаем на вопросы и все! Поздравляю, сертификат мы выпустили, теперь нужно добавить скрипт автопродления сертификата, т.к. Let's Encrypt выдает сертификаты сроком только на 90 дней (мы об этом помним). Все просто. Нам в cron нужно добавить строчку: 30 2 * * 1 /usr/bin/certbot renew >> /var/log/le-renew.log Т.е. набираем: crontab -e И добавляем нашу строку (обязательно перейти на следующую срочку, иначе не сохранится) И все, повторить бесконечное множество раз с Вашими другими ресурсами. Удачи, админы!\n","id":41}
{"Host":"https://habr.com","Path":"/ru/articles/709314/","Text":"Как хостить телеграм-бота (и другие скрипты на Python) на Repl.it бесплатно 24/7 / Хабр                 \n\nОчень часто возникающий вопрос: где можно разместить скрипты на Python, Flask-приложение, телеграм или дискорд ботов?\nОдин из вариантов — на своем компьютере при наличии внешнего IP-адреса и опыта в настройке проброса портов на роутере. Или другие сервисы, как правило, требующие платной подписки.\nЦель этот статьи - подробная инструкция, как сделать хостинг Python-скриптов бесплатно и доступным 24/7 на примере телеграм-бота\n## Шаг 0 - регистрация бота ##\nСуществует огромное количество туториалов, как получить токен, поэтому все по-простому. Находим в телеграм BotFather, регистрируем нового бота, выбираем ему имя, получаем токен вида: 127466748171:HJfwijfw88jf32lc9FHjwpfkfgwerhjf\nОн нам понадобится в дальнейшем\n\n## Шаг 1 - регистрируемся на Repl.it ##\nСоздаем новый проект на Python\n\n## Шаг 2 - Пишем код бота ##\nВ проекте будет создан файл main.py. В нем размещаем код бота:\n\nТут стоит обратить на установку модуля pytelegrambotapi: импортируем сначала `pip`  и потом выполняем его через: `pip.main(['install', 'pytelegrambotapi']).`\nВ этом случае при запуске никаких дополнительных действий для установки не потребуется\n```\nimport  os\nfrom  background import  keep_alive #импорт функции для поддержки работоспособности\nimport  pip\npip.main([ 'install' , 'pytelegrambotapi' ])\nimport  telebot\nimport  time\n\nbot = telebot.TeleBot( 'СЮДА ВСТАВЬТЕ ВАШ ТОКЕН' )\n\n@bot.message\\_handler( content\\_types=[ 'text' ] )\ndef  get\\_text\\_message ( message ):\n  bot.send_message(message.from_user. id ,message.text)\n# echo-функция, которая отвечает на любое текстовое сообщение таким же текстом\n\nkeep_alive() #запускаем flask-сервер в отдельном потоке. Подробнее ниже...\nbot.polling(non_stop= True , interval= 0 ) #запуск бота\n```\n## Шаг 3 - Создаем Flask-сервер ##\nСоздаем в проекте еще один файл `background.py`  В нем будет запущен Flask-сервер, который будет принимать запросы от сервиса мониторинга и использоваться для поддержания работоспособности скрипта на ReplIt.\nFlask - модуль на python для разработки веб-приложений. Мы создадим \"шаблон\" сервера, в котором только одна страница, необходимая для нашей задачи.\nВсе дело в том, что в бесплатном режиме запущенный скрипт на Replit будет остановлен спустя некоторое время (10-30 мин) после закрытия вкладки браузера.\n\nОднако, если к веб-серверу был сделан запрос, таймер сбрасывается и скрипт продолжает работать.\n```\nfrom  flask import  Flask\nfrom  flask import  request\nfrom  threading import  Thread\nimport  time\nimport  requests\n\napp = Flask( '' )\n\n@app.route( '/' )\ndef  home ():\n  return  \"I'm alive\"\n\ndef  run ():\n  app.run(host= '0.0.0.0' , port= 80 )\n\ndef  keep\\_alive ():\n  t = Thread(target=run)\n  t.start()\n\n```\nВажно, что сервер запускается в файле не напрямую, а в отдельном потоке `t = Thread(target=run).`  Это обеспечит возможность одновременной работы Flask-сервера и телеграм-бота.\nЗапуск Flask-сервера\nПосле запуска в верхнем правом углу появилась ссылка **(она потребуется чуть позже)**  по которой можно увидеть результат работы Flask-сервера (в нашем случае сообщение I'm alive).\nНа этом этапе у нас работает эхо-телеграм-бот и веб-сервер, доступный из вне по адресу вида: **_YOUR\\REPL.your\\nickname.repl.co_** Однако, спустя 10-30 минут после закрытия вкладки браузера скрипт будет остановлен. Вся хитрость в том, что если \"кто-то\" будет периодически открывать ссылку, ведущую на страницу нашего веб сервера скрипты будут продолжать работать бесконечно долго.\n## Шаг 4 - настраиваем службу мониторинга ##\nДля того, чтобы скрипт работал постоянно, воспользуемся сервисом UpTimerRobot . Он будет раз в 5 минут создавать запрос к нашему web-серверу и продлевать время его работы. Регистрация не представляет трудности, поэтому перейдем к следующему этапу.\nПосле входа в личный кабинет, создаем новый монитор\nСоздание монитора в UpTimerRobot\nВ настройках нового монитора нужно указать название и ссылку, которую мы получили при запуске скрипта выше. Время опросы указываем - каждые 5 минут.\n\nСохраняем монитор и возвращаемся в ReplIt. В консоле сервера видим входящие обращения от службы мониторинга\n\nЭто значит, что все получилось и наш скрипт будет работать 24/7. Можно работать над ним и развивать проект!\nТакие дела! Успехов!","id":42}
{"Host":"https://habr.com","Path":"/ru/articles/470065/","Text":"Dagaz: Конец одиночества / Хабр                 \n\n**_Счастье для всех, даром_**\n\n**_и пусть никто не уйдёт обиженный!_**\n\n**_А. и Б. Стругацкие «Пикник на обочине»_**\n\nБоты, как бы хорошо они не играли, плохая замена живым игрокам. Если бот играет слабо — это не интересно. Если сильно — это обидно и снова не интересно. Баланс соблюсти чертовски трудно (тем более, что для каждого игрока он индивидуальный). Я уже давно собирался реализовать сетевую игру, но всё упиралось в необходимость содержания собственного сервера. К счастью, решение пришло с неожиданной стороны.\n\nEd van Zon — это человек, с которым я общаюсь довольно давно. Так получилось, что именно он подхватил падающее знамя\nZillions\n, когда разработчики (Jeff Mallett и Mark Lefler) внезапно потеряли интерес к развитию проекта. Так что, все\nэти игры\nопубликованы на сайте его силами. А ещё, он и\nChristian Freeling\nзанимаются поддержкой и развитием собственного сайта (само собой, тоже про настольные игры):\n\nНо всё это была присказка. Сказка начинается с того, что на этом сайте есть нора (вернее,\nяма\n, но суть не в этом), в которой живые игроки могут играть по переписке. Вернее могли, до тех пор пока технология Java-апплетов не стала считаться устаревшей. В последнее же время, поскольку их использование простыми смертными стало затруднительным, Эд задумался о более современных решениях. И тут подвернулся я, вместе со своим\nпроектом\n.\n\nПосле очень небольшого обучающего тура, с моей стороны, Эд, в течение буквально пары месяцев, настрогал три десятка новых игр на движке Dagaz и выложил их на сайте. Напомню, что\nMIT-лицензия\n(как и я сам), такие действия всячески поддерживает.\n\n**Была, впрочем, одна проблема**\nПодразумевалось, что во все эти игры игроки смогут играть между собой, а не только с ботами, Dagaz же, в его первоначальной реализации, такой возможности не предусматривал. Пришлось быстренько что-то придумывать. К счастью, у меня уже был session-manager , позволявший откатывать ошибочно сделанные ходы. В качестве бонуса, он сохранял историю игры в оперативной памяти и этим решено было воспользоваться.\n\n**Сохранение**\n\n```\nSessionManager.prototype.save = function ( ) {\n  if  (_.isUndefined( this .current) || _.isUndefined( this .current.board)) return  null ;\n  var  states = [];\n  var  board  = this .current.board;\n  while  (board.parent !== null ) {\n      states.push(board);\n      board = board.parent;\n  }\n  var  r = \"(\" ;\n  while  (states.length > 0 ) {\n      var  board = states.pop();\n      r = r + \";\"  + Dagaz.Model.playerToString(board.parent.player);\n      r = r + \"[\"  + Dagaz.Model.moveToString(board.move) + \"]\" ;\n  }\n  r = r + \")\" ;\n  return  r;\n}\n\n```\n\n**и загрузка**\n\n```\nSessionManager.prototype.load = function ( sgf ) {\n  var  res = Dagaz.Model.parseSgf(sgf);\n  this .states = [];\n  delete  this .current;\n  var  board = Dagaz.Model.getInitBoard();\n  this .addState(Dagaz.Model.createMove(), board);\n  for  ( var  i = 0 ; i < res.length; i++) {\n       var  p = res[i].name;\n       if  (p != Dagaz.Model.playerToString(board.player)) return  false ;\n       if  (res[i].arg.length != 1 ) return  false ;\n       var  move = this .locateMove(board, res[i].arg[ 0 ]);\n       if  (move === null ) return  false ;\n       board = board.apply(move);\n       this .addState(move, board);\n  }\n  this .controller.setBoard(board);\n  return  true ;\n}\n\n```\n\nУложились всего в несколько десятков строк (сам парсер SGF-формата  я конечно не считаю, желающие могут посмотреть его здесь ). К слову сказать, и сам SGF и session-manager поддерживают работу с деревом, а не просто с историей игры, но для наших текущих целей это не требовалось.\n\nВ качестве первой игры от Dagaz, с возможностью игры по сети, хотелось выбрать что-то оригинальное (тем более, что\nШашки\n,\nШахматы\nи\nГо\nу Эда уже были). Выбор пал на\nTurnover\n. Эту игру, во многом похожую на Шахматы, совсем недавно придумал Lúcio José Patrocínio Filho.\n\nФигуры здесь сборные. Самое большое кольцо ходит как\nшахматная пешка\n, среднее — как\nслон\n, а сердцевина представляет из себя\nладью\n. Комбинация из ладьи и слона даёт\nферзя\n(что вполне логично), а два кольца —\nшахматного коня\n. Все три части вместе образуют замок — главную фигуру, которую необходимо защищать. Здесь стоит сказать, что перемещается всегда всего одна, самая внешняя часть. Таким образом, внешнее кольцо замка, в любой момент, может переместиться ходом пешки (в том числе, прыжком через поле), но замок, при этом, будет разрушен.\n\n**Здесь есть ещё одна, пока не решённая, проблема**\nПотеряв последний замок, игрок проигрывает. На самом деле, потерять все замки игрок не может, поскольку в игре действуют правила шаха  и мата . Замки запрещено оставлять под ударом, но только при условии, что **все**  они атакованы. Кроме того, интригу в игру добавляет то, что новые фигуры (и замки тоже) могут создаваться из составных частей по ходу дела (в том числе, из материала противника). Всё это делает проверку на шах и мат очень сложной задачей и вот пример, с которым она пока не справляется:\n\nНа самом деле, здесь нет мата, но это довольно сложно. Поле D1 атаковано слоном на E2 и это последний замок. Золотые могут построить второй замок, сходив ладьёй с C3 на C4, но это поле тоже атаковано! Фокус заключается в том, что оба поля атакует одна и та же фигура, а она не может съесть оба замка одним ходом! Lúcio обнаружил эту ошибку совсем недавно и это то, над чем я буду работать в ближайшее время.\n\nРокировок\nи\nвзятия на проходе\nв игре нет, да и вообще, Turnover не очень похож на Шахматы. Королей в игре много, но они не могут двигаться (во всяком случае, без разрушения), а кони и ферзи «одноразовые», поскольку перемещаясь, внешнее кольцо разрушает фигуру. На мой взгляд, игра выглядит довольно интересной, хотя и совершенно не исследованной. Поиграть можно вот здесь:\n\nДля игры по сети, разумеется потребуется зарегистрироваться (игра то ведь по переписке) и послать кому нибудь «Challenge». Если хотите\nпоиграть с ботом\n(или просто\nподвигать по доске фигуры\n), никакой регистрации не требуется. Впрочем, при таком режиме, игра мало чем будет отличаться от опубликованной на\nGitHub-е\n. Также, не требуется регистрация для наблюдения за текущими или ранее сыгранными партиями.\n\n**В качестве бонуса**\nВ Dagaz поддерживается ещё один режим игры:\n\nТак что, желающие могут попробовать поиграть вслепую (в этот раз, только против бота).\n\nУчёт побед/поражений для Turnover пока не ведётся, но здесь всё в ваших руках. Если игра будет популярна, Эд обещал прикрутить к ней рейтинг. Ну и напоследок небольшой опрос, на тему дальнейшего развития проекта:","id":43}
{"Host":"https://habr.com","Path":"/ru/companies/timeweb/articles/659215/","Text":"Атака Ферма на RSA / Хабр                 \n\nВ 1643 году Пьер де Ферма предложил\nметод факторизации\n. Этот метод позволяет эффективно раскладывать целые числа на простые множители.\n\nАлгоритм шифрования и подписи RSA основывается на том, что факторизация — это задача с высокой сложностью. Открытый ключ RSA содержит составное число (обычно называемое N), которое является произведение двух простых чисел (обычно p и q).\n\nЕсли ключи RSA генерируются из «близко стоящих» простых чисел, то RSA можно взломать с помощью метода факторизации Ферма. И хотя это довольно известный факт, но, насколько я знаю, уязвимые ключи RSA не обнаруживались в «дикой природе» — до сегодняшнего дня.\n\nЯ применил метод факторизации Ферма к большим наборам открытых ключей RSA. И я смог обнаружить небольшое количество уязвимых ключей, которые принадлежали принтерам Canon и Fujifilm (первоначально выпускавшихся под маркой Fuji Xerox). В этих устройствах используется криптографический модуль от компании Rambus.\n\n## Что такое метод факторизации Ферма? ##\n\nИдея метода состоит в том, что произведение двух простых чисел можно представить в виде\n_N=(A-B)(A+B)_\n, где\n_A_\n— это среднее арифметическое двух простых чисел (A=(p+q)/2), а\n_B_\n— это расстояние от\n_A_\nдо искомых простых чисел (B=p-a=a-q).\n\nЕсли простые числа находятся близко друг к другу, то и\n_A_\nблизко к квадратному корню из\n_N_\n. Это позволяет определить значение\n_A_\nпутем перебора: начать с квадратного корня из\n_N_\nи увеличивать потенциальное значение\n_A_\nна единицу в каждой итерации.\n\nДля каждой итерации мы можем вычислить\n_B^2 = A^2 — N_\n. Если результат является квадратом, то мы угадали A. И теперь можем вычислить p=A+B и q = A-B.\n\nПервоначально Ферма описал этот метод в своём письме, датированным 1643 годом. Текст оригинального письма можно найти в\nOeuvres de Fermat\n, на 256 странице.\n\n## Кто пострадавший? ##\n\nНесколько принтеров серий Fujifilm Apeos, DocuCentre и DocuPrint генерируют самозаверяющие сертификаты TLS с уязвимыми ключами RSA. Информационный бюллетень Fuji содержит список всех затронутых принтеров.\n\nНекоторые принтеры Canon генерируют запрос на подпись сертификаты с помощью уязвимого ключа RSA. Насколько мне известно, это влияет на принтеры серий imageRUNNER и imagePROGRAF.\n\nКак принтеры Fujifilm, так и принтеры Canon используют криптографический модуль библиотеки Safezone от Rambus. Другие продукты, использующие этот модуль для создания ключей RSA, также могут быть затронуты. Код этой уязвимости —\nCVE-2022-26320\n.\n\nЯвляется ли это уязвимостью алгоритма RSA.\n\nНет, не является. Библиотеки RSA с корректной функцией генерации ключей эта уязвимость не затрагивает.\n\n## Как это происходит ##\n\nКлюч RSA уязвим, если два простых числа p и q находятся близко. Если простые числа генерируются независимо друг от друга и случайным образом, то вероятность того, что они будут близки, ничтожно мала.\n\nОднако функции генерации ключей RSA могут реализовывать ошибочный алгоритм, например такой:\n\nСгенерировать случайное простое число X;\n\n Найти следующее простое число и присвоить его q;\n\n Найти следующее простое число и присвоить его p;\n\nДля стандартных размеров ключей RSA разница p и q — тысячи или меньше. Такой алгоритм является уязвимым для метода факторизации Ферма.\n\n## Какой должна быть разница простых чисел, чтобы ключ RSA был уязвим? ##\n\nПри обычных размерах ключей RSA в 2048 бит метод ферма со 100 итерациями надежно факторизует числа, где разница p и q меньше 2^517. Другими словами, простые числа, различающиеся только младшими 64 байтами, будут уязвимы. Можно было бы возразить, что 100 итераций — это слишком много, однако алгоритм настолько быстр, что на практике это не будет иметь большого значения.\n\n## Могут ли случайно сгенерированные ключи быть уязвимы? ##\n\nДа, могут. Но для этого необходимо, чтобы они были идентичны, по меньшей мере, в своих старших 500 битах. Вероятность такого исхода — 1:2^500.\n\n## Как ты нашел ключи? ##\n\nЯ использовал несколько наборов открытых ключей:\n\nк которым у меня был доступ;\n\n которые были предоставлены другими исследователями;\n\n которые были общедоступны.\n\nЯ обнаружил уязвимые ключи Fujifilm в недавних сканированиях TLS-сертификатов Rapid7. Ещё некоторое количество сертификатов я обнаружил в логах Certificate Transparency. Связавшись с их владельцами, я узнал о принтерах Canon.\n\nКак оказалось, все уязвимые сертификаты были относительно недавнего происхождения (2020 год и позже). Я думаю, по этой причине такие уязвимости не были описаны ранее.\n\n## Что с SSH? ##\n\nСкорее всего, уязвимых реализаций SSH, создающих такие ключи, нет. Хотя я и не могу это доказать.\n\nЯ проверил несколько больших наборов ключей хоста и пользователя SSH, но ничего не обнаружил.\n\n## Затронуты ли PGP/GnuPG/OpenPGP? ##\n\nЯ применил метод к дампу серверов ключей SKS PGP и нашел четыре уязвимых ключа. Однако все ключи были с идентификатором пользователя, который подразумевал, что они были созданы для тестирования.\n\nВполне вероятно, что эти ключи были созданы вручную людьми, знающими об атаке и создающими тестовые данные\n\n## Рекомендации ##\n\nЕсли вы используете уязвимое устройство, то убедитесь, что вы обновили их и повторно сгенерировали ключи.\n\nЕсли внешние пользователи предоставляют вам открытые ключи RSA, то вы можете реализовать проверку на наличие этой уязвимости. Типичный случай — центр сертификации. Я поделился кодом эксплойта с такими центрами и некоторые из них реализовывали проверки в процессе выдачи сертификатов, дабы избежать принятия уязвимых ключей.","id":44}
{"Host":"https://habr.com","Path":"/en/post/242585/?mobile=no","Text":"Особенности работы Android TV box с multicast вещанием / Habr                        \n\n07-11-2014\nОднажды мне захотелось найти среди недорогих 2-х ядерных андроид ТВ приставок такую, которая просто будет работать с HD ТВ каналами, транслируемыми провайдерами в моём городе. Оказалось, что из 4-х купленных приставок с чипами RK3066, Allwinner А20, Amlogic 8726-MX только одна корректно и без зависаний видео воспроизводит HD каналы, получаемые в мультикаст режиме. При этом просмотр фильмов с разрешением 1080p с внутренней памяти или «флешки» происходил качественно на всех приставках. Итак, IPTV приставка CA9-DA (см. рис. 1). Рис. 1. Внешний вид IPTV приставки CA9-DA. Технические характеристики: — Двухядерный видеоускоритель Mali 400; — Двухядерный процессор Amlogic 8726-MX с архитектурой Cortex A9; — 1 ГБ оперативной памяти; — 4ГБ пользовательской памяти + слот для карт памяти SD/MMC; — Связь: Wi-Fi 802.11 b/g/n 2.4ГГц, Fast Ethernet. Комплектация (см. рис. 2): Рис. 2. Комплектация CA9-DA. — HDMI кабель; — пульт управления; — адаптер питания 5В/2А. Рис.3. Пульт к CA9-DA. Среди других приставок CA9-DA выделяет поддержка аппаратного декодирования видео в MX-Player. На данный момент MX-Player наиболее адаптирован для воспроизведения видео на андроид устройствах, поэтому именно он после недолгого практического отбора был выбран для воспроизведения ТВ. Установка всех дополнительных кодеков для MX-Player размещённых на Google Play обязательна. В описаниях на приставки нигде о поддержке аппаратного декодирования не упоминается, и пока не попробуешь в реальной работе, узнать не получиться. При работе остальных устройств декодирование видео можно было выставить только программное и, не смотря на то, что процессор при этом загружается не более чем на 50 %, изображение на экране распадается частично на квадратики. Именно этот нюанс позволил отбросить остальные приставки и продолжить изучать только CA9-DA. Уточняю. Обычное видео 576i адекватно воспроизводится всеми, а вот HD (720p) передаваемое в мультикаст режиме только этой приставкой. Пульт в комплекте простой (см. рис.3). Перемещение курсора осуществляется нажатием кнопок управления. Гироскопа внутри нет. Для удобства лучше подключать беспроводную мышь и клавиатуру. Или же использовать гироскопическую мышь-клавиатуру AirMouse. По моему субъективному мнению лучше беспроводной мыши ничего нет. Пульт для управления курсором не удобен, внешняя клавиатура при наличии виртуальной лишняя, AirMouse требует привыкания. Остаётся мышь. Разъёмы расположены на задней стороне (см. рис. 4): — Ethernet порт (10/100 Мбит/с); — USB-HOST 2 шт; — HDMI 1.4; — вход для блока питания. Рис.4. Задняя панель CA9-DA. На правой боковой стороне поместился приёмник для карт памяти SD (см. рис. 5). Рис.5. Правая боковая панель CA9-DA. На передней панели расположился датчик ИК излучения и кнопка вкл/выкл (см. рис. 6). Рис. 6. Передняя панель CA9-DA. Антенна Wi-Fi встроенная. Корпус изготовлен из глянцевого чёрного пластика сверху/снизу и матового светло-серого пластика с торцов. Логотип производителя отсутствует. Внутренний вид приведён на рисунках 7 и 8. Рис. 7. Верхняя часть платы CA9-DA. Рис. 8. Нижняя часть платы CA9-DA. Сверху к чипу AML8726-MX через теплопроводящую пасту прижимается радиатор. Работу Wi-Fi обеспечивает микросхема Realtek RTL8188ETV. При работе CA9-DA практически не нагревается. В приставке предустановлен Android 4.2.2 (см. рис. 9). Рис.9. Информация о CA9-DA. Изначально используется настройка разрешения экрана «Авто» (см. рис. 10). Рис. 10. Настройки экрана CA9-DA. Интересно, что само наличие этой настройки оказалось критически важным для вещания. В режиме «авто» включается разрешение 720p (именно с этим разрешением вещаются HD каналы), если при этом принудительно перевести приставку в режим разрешения 1080p, то иногда появляется подтормаживание видео. Так вот специально для эксперимента нашёл не дешёвую 4-х ядерную приставку MINIX X7 тоже на Adroid 4.2.2 и так-же поддерживающую аппаратное декодирование видео в MX-Player и обнаружил, что режима «авто» в ней нет! Смотришь фильм 1080p – устанавливай вручную в настройках Android разрешение экрана 1080p. Если забыл установить 720p при переходе на просмотр мультикаст ТВ, будешь иметь подтормаживание видео. 4-х ядерность и высокая тактовая частота чипа от этого не спасает. Дорогое не всегда лучшее. После такого поворота появился дополнительный стимул к более глубокому изучению возможностей CA9-DA. Одним из раздражающих факторов при просмотре ТВ каналов оказалась то, что переключение с канала на канал осуществляется в несколько кликов мыши или в несколько нажатий кнопок на пульте. Испытывались приложения взятые из Google Play. У «IPTV Alexander Sofronov» сбоев не наблюдалось, но переключение каналов осуществляется в три клика — выйти из просматриваемого канала, передвинуть курсор на следующий канал, нажать ОК. У «IP-TV Qsmart» переключение каналов уже в два клика — один раз в начале просмотра: вызываешь список каналов (слева на экране горизонтальная полоса со списком), далее передвигаешь курсор и нажимаешь ОК. Правда, если свернуть список каналов (иногда хочется убрать лишнее с экрана), а затем снова его вызвать — приложение виснет. У «Эмулятора IPTV приставок» переключение в одно нажатие, более того, интерфейс скопирован с наиболее известной ТВ приставки МАГ 250/254, что очень порадовало. Но приложение не поддерживает плейлисты в формате m3u. Единственный способ их проигрывания в приложении — использование портала, который такие плейлисты поддерживает. Не всякий провайдер организовывает такой портал у себя на сервере, поэтому универсальность приложения теряется. В результате при выборе приложения для просмотра ТВ каналов нужно искать компромисс с самим собой. По Wi-Fi как клиент приставка работает без нареканий. Проблема возникает при маршрутизации интернет трафика заведённого в приставку по LAN порту и последующей раздаче по Wi-Fi двум и более устройствам — режим AP. Внутренний трафик между клиентами проходит «на ура» (тестирование проводилось с помощью программы Iperf), а при выходе в интернет одновременно всех клиентов (запускался на каждом speedtest.net) CA9-DA “висла”. Непосредственно перед зависанием у приставки заметно начинает тормозить интерфейс — поведение мышки с рывками, подвисание переключения окон и т. д. Похоже, этот недостаток связан с “железом”, потому как на 4-х ядерной приставке такой проблемы не наблюдалось и с тремя клиентами, а с одним клиентом справляется и CA9-DA. И, наконец, был протестирован режим Miracast – беспроводное «зеркалирование» экрана андроид устройства, например, планшета на экране телевизора с подключённой андроид приставкой. Проще говоря, этот стандарт решает задачи очень похожие на AirPlay от Apple, WiDi от Intel или старое-доброе DLNA. Miracast работает с помощью Wi-Fi и первое логичное действие при включении Miracast напрашивалось само — включить в приставке и в планшете Wi-Fi и установить между ними соединение. Не тут-то было. После серии попыток выявлена следующая последовательность правильных действий. На приставке и планшете\\телефоне включить Wi-Fi, не спаривая (!!!) устройства в Wi-Fi Direct. На приставке запустить программу Miracast, а на планшете\\телефоне перейти в раздел Экран->Беспроводной экран (Miracast) и дождаться (!!!) появления ссылки на приставку в этом меню. После появления ссылки, по тапу происходит спаривание устройств и появляется изображение на телевизоре. Первоначальный коннект может быть вывален с ошибкой, но повторное ожидание иконки и подключение тапом приносит положительный результат. После запроса о причинах такого сложного алгоритма производитель приставки признал огрехи в работе миракаста. В своё оправдание он рассказал, что уже некоторое время ведёт переговоры с производителем чипа Amlogic 8726-MX на предмет улучшения работы Miracast. В итоге хочется сказать, что при выборе Android TV box чтение описаний предоставляемых производителем не дают понимания характера поведения приставок в различных режимах. Поэтому на данном этапе наиболее компромиссным и главное изученным вариантом является приставка с чипом Amlogic 8726-MX.\n","id":45}
{"Host":"https://habr.com","Path":"/ru/companies/ric/articles/525494/","Text":"Путь одной команды от «велосипедов» до IoT-платформы / Хабр                                                        \n\nПривет, Хабр!\n\nМы, команда Rightech, наконец-то решили начать вести блог. У нас накопилось много опыта в построении высоконагруженных IoT-систем, и мы решили, что просто обязаны им делиться! Совсем недавно прошел запуск публичной версии нашей платформы RIC (Rightech IoT Cloud), и теперь ей может воспользоваться каждый желающий. Но сначала расскажем, кто мы и откуда появились.\n\n### C чего всё начиналось ###\n\nВ далеком 2011 году мы скорее были командой фрилансеров из города Орел и начинали с того, что программировали ряд аппаратных разработок, связанных с реализацией систем автоматизированных измерений и управления. Подключали сенсоры и актуаторы к микроконтроллерам, реализовывали протоколы обмена, первичную обработку данных и автоматизацию.\n\nЧерез какое-то время команда стала замечать общее между всеми проектами. Мы писали на различных языках программирования, работали с разным оборудованием, но\n_выстраивали идентичную архитектуру решения_\n. В определенный момент мы пришли к выводу, что\n_можно создать универсальный фреймворк_\n, который позволит сразу перейти к разработке интерфейса системы.\n\nНаши заказчики искренне не понимали, почему они не могут смотреть/тестировать промежуточный результат. Почему он выглядит как бегущие строки в консоли, а для управления актуатором нужно не нажать на тачскрин, а отправить Modbus команду. К программированию интерфейсов мы приступали в тот момент, когда было уже готово API и проработана автоматизация.\n\nНа тот период времени наш рабочий процесс представлял собой следующую последовательность:\n\nфизически подключить компоненты системы в одну сеть;\n\nреализовать протоколы обмена данными;\n\nреализовать обработку данных (фильтрация, применение функций преобразования, проведение косвенных измерений и т.д.);\n\nпреобразовать данные в высокоуровневые структуры;\n\nописать конечные автоматы системы;\n\nреализовать API для вывода данных в интерфейс и запуска автоматов;\n\nреализовать интерфейс системы (тачскрин/нативное приложение/интеграция во внутренние системы заказчика).\n\nИ, как вы понимаете, из проекта в проект до последнего этапа заказчик с трудом понимал, что мы делаем, и это всегда было почвой для недоверия и конфликтов. Конечно, когда мы сдавали работу, заказчик был рад и доволен, но потраченные нервы и ощущение того, что мы по сути «пилили» на 90% очередной «велосипед» на новом стеке, оставалось.\n\nВ результате мы приняли решение — создать универсальный фреймворк. На наше решение повлияли и интересы заказчиков. Управленцы и владельцы бизнеса, как правило, заинтересованы в создании конечных пользовательских кейсов и часто не углубляются в механизм работы системы. Для успешного выполнения поставленных ими задач нам требовалось больше фокусироваться на разработке пользовательских сценариев и интерфейсов, так как именно они несут в себе основную коммерческую ценность создаваемого решения. При этом 90% затрат на разработку сосредоточено во внутренних, невидимых конечному пользователю системах проекта.\n\nПервоначальная идея платформы RIC трансформировалась и изменялась вместе с рынком, находила все больше областей применения. Разработанный нами за 4 года\n_фреймворк позволял сосредоточиться на процессах создания ценности решения, воплощения задуманной бизнес-логики и продвижения конечного продукта на рынок_\n. RIC был полностью готов в 2016 году, оставалось только создать компанию и найти деньги…\n\n### Rightech. История создания ###\n\nУ нас совершенно отсутствовал опыт в привлечении денег. Однако в 2016 году нам удалось привлечь первые инвестиции от фонда, вложившегося в компанию «Делимобиль». На эти деньги мы создали компанию Rightech, которая стала домом для нашего проекта. А первым действительно крупным внедрением нашей технологии, как вы уже догадались, стал каршеринг «Делимобиль». Сразу оговорюсь, что приложения и CRM систему разрабатывали не мы, но тысячи автомобилей и терабайты машинногенеренных данных стали достойной проверкой, которую RIC уверенно прошел.\n\nПомимо шерингов, к 2019 году мы успели автоматизировать Digital Out Of Home рекламу, построить сбор данных с газотурбинных генераторов электроэнергии и многое другое. Команда занималась не только рыночными внедрениями, но и развивала RIC в целом: реализовали множество транспортных протоколов, оптимизировали серверную инфраструктуру и расширили систему автоматизации.\n\nКак и следовало ожидать, нам стало тесно в рамках «Делимобиль» и мы начали развивать платформу в сторону публичного облака. К этому времени цели инвестирования со стороны фонда были достигнуты, и в начале 2019 года мы выкупили его долю. Это позволило команде проекта сконцентрироваться на реализации по-настоящему массовой технологии.\n\n### Подробнее о платформе ###\n\nТак что же такое IoT-платформа? Во что превратился наш фреймворк — заменитель «велосипедов» — RIC?\n\nЛюбой IoT проект состоит из следующих принципиальных компонентов или слоев:\n\n**устройства**  — «умные» устройства, датчики и актуаторы, на основе которых осуществляется сбор данных и автоматизация;\n\n**сети передачи данных**  — по ним происходит передача данных и команд между компонентами IoT системы;\n\n**платформа**  — инфраструктура, обеспечивающая интероперабельность (способность к взаимодействию) компонентов системы, обработку данных и передачу ее в прочие системы;\n\n**приложения**  — представляет собой конечную цель IoT-проекта, на этом уровне работают конечные пользователи и на нем же, по сути, создается ценность всей системы.\n\nRightech IoT Cloud (или RIC) — это облачная платформа Интернета Вещей, выступающая в качестве промежуточного программного обеспечения (middleware) при разработке IoT-решения. RIC включает в себя все необходимые программные элементы, позволяющие инженерам любого уровня создавать приложения с использованием любых IoT-устройств без необходимости создавать соответствующую программно-аппаратную инфраструктуру. Да-да, именно любых устройств. Мы выложили не все реализованные протоколы в публичную версию RIC. Если вы не нашли протокол своего устройства, можете нам написать и мы его обязательно добавим в публичную версию.\n\n**Небольшой лайфхак:**\nиногда не стоит выводить всю логику на сторону платформы. Какие-то важные процессы можно оставить на стороне железа на тот случай, если пропадет интернет. Или сценарии, в которых важно «реальное время», например, две команды должны быть выполнены в строгом интервале между ними. Тогда запуск этого сценария можно сделать через RIC, а логику исполнения оставить на устройстве.\n\nВ отличие от существующих аналогов платформа Rightech не требует написания кода, кроме относящегося к созданию Приложений, являющихся целью IoT-проекта. Интеграция платформы с уровнем приложений происходит посредством высокоуровневого REST API, и производится не сложнее, чем интеграция Google карт.\n\nПроще говоря, платформа — это конвейер IoT данных. И этот конвейер можно настраивать так, как захочется каждому отдельному пользователю. Он решает базовые задачи, связанные с обеспечением подключения устройств и их взаимодействия между собой, управления устройствами, обработкой, хранением и анализом поступающих от устройств данных, проектированием сценариев автоматизации бизнес-логики и их исполнением, интеграцию с приложениями и сторонними системами, обеспечение безопасности на каждом уровне взаимодействия компонентов системы и многие другие.\n\n### Публичное облако ###\n\nВесной мы запустили регистрацию на наше публичное облако, и теперь каждый пользователь может бесплатно подключить до 10 устройств и спрототипировать свой будущий бизнес или же автоматизировать, например, теплицу или дом.\n_Любой IoT проект может взять всё необходимое в платформе RIC и реализовать свою уникальную систему обработки и представления получаемых данных практически без программирования_\nтех самых 90% айсберга.\n\nНаш короткий рассказ подошел к концу. Надеемся, что мы вам понравились, и в свою очередь обещаем делиться своим опытом и актуальной информацией в сфере IoT.\n\nКстати, мы есть и в\nТелеграм\nс чатом единомышленников.\n\nJust do IoT!\n\n#### Полезные ссылки: ####\n\nОбучающие видеоролики на примере мини-кейсов —\nrightech.io/video-tutorials\n\nСоздайте свой IoT-проект уже сейчас —\ndev.rightech.io/signup\n\nПрисоединяйтесь к единомышленникам —\nt.me/rightech\\_iot","id":46}
{"Host":"https://habr.com","Path":"/ru/company/vdsina/blog/497958/?amp&amp","Text":"Они среди нас: исследуем уязвимости и вредоносный код в приложении Zoom для Windows / Хабр                \n\n20-04-2020\nКто добавил в Zoom этот «изюм»? В последнее время часто приходится слышать примерно такие реплики: «Мы/я не используем Zoom для конфиденциальных разговоров» Но что если кто-то кроме конфиденциальности сеанса Zoom заботится о целостности данных на своем устройстве? И не зря, потому что, как оказалось, для этого есть основания. На кону не только данные конференции Zoom, но и целостность и конфиденциальность всего, что участник конференции сохраняет на своем компьютере. Итак, я провел два вечера (в общей сложности около 5-6 часов), препарируя Zoom для Windows (версия 4.6.11 (20559.0413)). Я занимался только статическим анализом, не наблюдая за работой приложения в реальном времени. Но мне хватило и этого. Всё, что я обнаружил, описано в данной статье. Но я уверен, что это только верхушка айсберга. Представьте себе, что может накопать аналитик по безопасности, обладающий достаточным количеством времени и бюджетом… А я целенаправленно искал те уязвимости, которые можно использовать для раскрытия конфиденциальной информации третьим лицам (например, утечка информации в Facebook). Я провёл очень поверхностное исследование и не углублялся в детали. Я занимался этим в свободное время и больше этим заниматься не намерен. Я ставил перед собой чёткую цель — дать оценку качеству кода с точки зрения безопасности и наличия (или отсутствия) поддержки программного обеспечения, входящего в состав приложения Zoom (что особенно необходимо при использовании большого количества сторонних библиотек). Архаичное 32-битное приложение Серьёзно? Ну почему?! Microsoft добавила поддержку 64-битных приложений и различных улучшений безопасности для 64-битных процессоров, начиная с Windows 7. Это было в 2009 году, одиннадцать лет назад. Да и сама Windows 7 уже давно морально устарела (с 14 января 2020 года эта ОС больше не поддерживается). Но нет, в 2020-м Zoom по-прежнему выпускает только 32-разрядные приложения для Windows. zData.dll zData.dll использует компонент с уязвимостями. Устаревшая библиотека OpenSSL 1.0.2 (от 27 марта 2018) известна своими уязвимостями, которые больше никто не будет исправлять. С декабря 2019 года работа над OpenSSL 1.0.2 прекращена. Вот что пишут её разработчики: «По текущим и будущим релизам проект OpenSSL принял следующие решения: Следующей версией OpenSSL станет 3.0.0. Версия 1.1.1 будет поддерживаться до 2023-09-11 (LTS). Версия 1.0.2 больше не поддерживается. Теперь доступна расширенная платная поддержка. Версии 1.1.0, 1.0.1, 1.0.0 и 0.9.8 больше не поддерживаются». Или вот: Объединение SQL-выражений CWE-89: Некорректная нейтрализация специальных элементов SQL-команд (SQL-инъекции) В zData.dll бэкенд используется для хранения различных данных сеанса и конфигурации в зашифрованной базе SQLite. В некоторых случаях в коде операторы SQL, по-видимому, просто объединяются, как показано ниже: Пример 1: Пример 2: В зависимости от реализации оператора +=, zData.dll может потенциально создавать уязвимость SQL-инъекций, которые могут привести к раскрытию информации или выполнению произвольного кода на устройствах конечных пользователей Zoom. Ой, простите… а это, случайно, не ключ для криптографического дешифрования зашифрованной базы данных SQLite? Точно не знаю, на самом деле :-) Однако шифрование базы в принципе не мешает получить доступ к конфиденциальным данным, которые там хранятся. Похоже, что Zoom использует базу данных SQLite для хранения записей истории, журналов и, возможно, также для хранения конфиденциальных данных, таких как пароли и / или криптографические ключи. Я не стал углубляться в использование SQLite в качестве контейнера данных, оставляю это удовольствие более любопытным читателям. Zzhost.dll Угроза переполнения буфера Просматривая таблицы импорта бинарных файлов, я наткнулся на вызов sprintf (). Но ведь известно, что sprintf () — потенциально опасная функция, и компиляторы выдают предупреждения, если вы используете её. Похоже, что такие предупреждения не беспокоят разработчиков Zoom. CWE-120: копирование содержимого буфера без предварительной проверки размера входных данных (в функции логирования) CWE-676: использование потенциально опасных функций (sprintf) Чтобы успешно использовать эту уязвимость, необходимо контролировать содержимое исходного буфера. Сам я это делать не пробовал. Однако, поскольку это функция ведения журнала (то есть, логирования), злоумышленники теоретически могут влиять на содержимое исходного буфера. Еще раз, сам я не проверял, возможно ли это на самом деле. Но даже сам факт возникновения такой возможности и игнорирование предупреждений об опасности использования функций — очень плохая практика. Разработчики! Не делайте так! zCrashReport.exe Сохранение ключей реестра Windows Реестр Windows содержит достаточно важную информацию — особенно, когда мы обращаемся к нему через процесс с привилегированными правами. Я нашёл вот что: «Dumping registry keys» («Сохранение ключей реестра») для меня звучит как вредоносная функция, поэтому я попытался выяснить, действительно ли она сохраняет ключи и значения реестра Windows. Да, это так, я нашёл в коде соответствующий фрагмент: Захват экрана в Crash Reporter zCrashReport.dll предлагает нам следующие функции: В crashReport.exe, используются функции Windows API для захвата экрана и окон. Как думаете отсылает ли Zoom захваченные с экрана данные на свои серверы? Если да, то это можно расценивать как вредоносный код. Airhost.exe Компоненты с уязвимостями Airhost.exe использует libcurl 7.36.0 (вышла 26 марта 2014), в которой найдено 52 уязвимости. Ну а curl, в свою очередь включает libssh2, которая также отметилась рядом уязвимостей: Эти уязвимости, к счастью, не представляют угрозы конкретно для пользователей Zoom. Тем не менее, зачем использовать устаревшие и уязвимые библиотеки, если вы вообще заботитесь о своем коде? Проще использовать последнюю версию, чем гадать, повлияет ли очередная найденная уязвимость на ваше приложение, не правда ли? Шифрование/дешифрование с «захардкоженными» ключами Airhost.exe использует константу в качестве ключа симметричного шифрования: Выходная строка SHA256 “0123425234234fsdfsdr3242” используется для шифрования / дешифрования AES-256 через OpenSSL. А строка “3423423432325249” используется как вектор инициализации IV. Уязвимости zWebService.dll и tp.dll zWebService.dll и tp.dll используют libcurl 7.55.1. Даже несмотря на то, что эта библиотека не такая старая, как libcurl 7.36.0 (которая используется в airhost.exe), она всё-таки тоже немного устарела. Если бы разработчикам была по-настоящему важна безопасность конечных пользователей, они изучили бы этот список уязвимостей: Уязвимости turbojpeg.dll Turbojpeg.dll использует libjpeg-turbo 2.0.0 (build 20190715). Вот список уязвимостей класса Remote code execution (Удаленное выполнение кода) turbojpeg/libjpeg-turbo: Что дальше? На этом месте я прекратил копаться во внутренностях Zoom для Windows. Тем, кто хочет продолжить, я с удовольствием передаю эстафету: sha256 checksums of Zoom 4.6.11 (20559.0413) executable files\n","id":47}
{"Host":"https://habr.com","Path":"/en/post/50048/?mobile=no","Text":"Новые идеи интерфейса на примере конкретной программы / Habr           \n\n24-01-2009\nУже около 10 лет я использую компьютер для различных целей. Все действия, которые я производил на нём, чтобы достичь какого-нибудь определённого эффекта я считал долгое время правилом. Но в один прекрасный день понял, что интерфейс всех программ, с которыми я имел дело отнюдь не правило и что выполняемые мною действия могут быть разными, но эффект будет тем же. И с того дня у меня появился интерес к разработке интерфейсов и программированию. В данный момент я достиг того уровня знаний программирования, что могу воплощать свои идеи почти в полной мере. Однако эти идеи не основаны на каком-нибудь там расчёте коэффицента юзабилити, они основаны на чувстве красивого и на том, что приходит в голову. Я не в коем случае не утвержадаю, что чувство прекрасного у меня развито замечательно, тем не менее оно мне позволяет говорить о «симпотичности» какого-нибудь интерфейса с с моей стороны. Сразу хотелось бы оговориться, что слово «интерфейс» в моём рассказе в большей мере можно заменить словосочетанием «внешний вид» нежели «совокупность внешнего вида и удобства использования». Насчёт удобства использования разработанных мною элементов интерфейса я порассуждаю в следующем топике. Я вам предлагаю посмотреть обзор некоторый идей реализации интерфейса на примере конкретной программы, которую я сейчас пишу: редактор электронных словарей. Вид сразу после запуска Вот такую вот картинку видит пользователь после старта программы. Мне кажется, что когда пользователь запускает программу в первый раз он слабо понимает как ею пользоваться и с чего вообще начинать, поэтому я решил предоставлять первоначальную информацию пользователю в виде текста в текстовом поле. А также в текстовое поле был добавлен список недавно открытых/редактированных словарей, тем самым делая открытие словаря делом одного нажатия. Создание нового словаря Здесь комментировать в принципе нечего. Разве что обратить ваше внимание на отсутсвие диалога ошибки, которая предупреждает, что словарь с таким же именем уже существует. Кстати, то что сам виджет создания нового словаря не является диалогом является довольно спорным моментом. Я всё больше склоняюсь к тому, что всё-таки лучше сделать это диалогом. А вы как считаете? Загрузка словарей в БД Словари того формата, который использует этот редактор — текстовый файл определённого формата, причём размер этих файлов может достигать 50Мб, поэтому очевидно, что работать с этим файлов в течение работы программы тяжело, так как добавление новой строки в 50мб файл будет относительно долгим, а поиск набранного слова тем более. Поэтому было принято решение, чтобы загружать текстовый файл в SQLite базу данных, проводить с ней различные манипуляции, а потом уже из неё создавать файл нужного формата. Загрузку можно остановить, возобновить и отменить, что логично. Обратите внимание на то, что прогресс строку можно скрыть с помощью специальной горизонтальной кнопки. Управлени загруженными словарями Из-за того, что можно манипулировать только загруженными словарями, пришлось создать управление этими словарями. Я постарался максимально упростить эту часть программы. Нижние иконки однозначны, поэтому проблем с понимаем быть не может. Хотя плюсик всё-таки может быть непонятен с первого взгляда, но когда пользователь наведёт на эту кнопку курсор мыши он увидет небольшой ToolTip popup с описанием возможностей кнопочки. Настройки программы Почти во всех программах настройки реализованы в качестве диалога, однако так как я являюсь поклонником однооконности, настройки в редакторе выглядят так, как на скриншоте снизу. Каждый пункт настроек имеет подробное описание, которое появляется в виде popup окна после нажатия на кнопочку «Info». Тем самым не должно возникать ощущения нагруженности текстом, но в то же время пользователь всегда сможет понять что это за опция такая. Добавление/Редактирование/Удаление слов Основная часть программы, которую очень тяжело сделать лёгкой и удобной. Тем не менее я считаю нынешний интерфейс удачным. Однако в то же время, я расчитываю на то, что пользователь будет использовать горячие клавишы, поэтому их значение указано в ToolTip'е кнопки. Информирование пользователя(подсказки и статус-строка) Во многих программах есть такие диалоги, которые появляются после запуска программы и называются они «Tip of the day», однако читать их малое удовольствие. То решение, которое пришло в мою голову основывается на том, что пользователю необходимо сообщить пользователю о функциях программы незаметно и чтобы пользователю было приятно смотреть на эти подсказки. Поэтому в правой нижней части редактора каждые 10 секунд обновляется подсказка, которые можно отключить в настройках. Статусная строка же привлекает к себе внимание буквально на 5 секунд своим миганием, после чего исчезает. Скриншоты на PicasaWeb Для тех, кто хочет потрогать всё это: svn checkout lightlang.googlecode.com/svn/trunk/lightlang-editor lightlang-editor Программа спокойно компилируется в Windows на MinGW c Qt4.4 и Linux, а на Маке не пробовал. Да, возможно, моя первая попытка придумать что-то новое и не увенчалась успехом, но ведь главное на данном этапе не это, а то, что есть желание творить и улучшать, а не топтаться на месте. В качестве комментариев я бы хотел услышать как минимум ответ на вопрос: «Кажется ли Вам эти идеи удобными, приятными для глаза и актуальными?». Спасибо. P.S. Хотел сделать скринкаст, но не получилось. Подскажите, пожалуйста, какие-нибудь хорошие программы для записи скринкастов. XVidCap — у меня не работает(segment fault).\n","id":48}
{"Host":"https://habr.com","Path":"/ru/post/679858/?amp&amp","Text":"Визуализация решает или про иллюстрацию статистических тестов / Хабр                                                              \n\n29-07-2022\nМинутка фантастики Давайте заглянем в параллельный мир на минутку. Допустим, некий товарищ Х составляет некий отчет в некоторую организацию о том, как все изменилось – с процентами, динамикой, распределениями, тестированиями. Допустим, он этот самый отчет сделал – но ему его вернули с неофициальными комментариями «Много формул, ничего не понятно, надо проще» (понятно, почему фантастика – в нашем мире, особенно в госструктурах, это встретишь очень редко). Но именно в этих случаях и можно применять пакет ggstatsplot. Краткая идея Все очень просто – для некоторых задач проверки гипотез в этом пакете реализовано и автоматическое построение графика, и вывод на этом же графике результата теста. В верхней строчке всегда пишется значение тестовой статистики и р-значение для соответствующей гипотезы. Отлично, что для каждой гипотезы доступны 4 варианта теста: параметрический, непараметрический, робастный (устойчивый к выбросам) и байесовский Дальше строчка продолжается показателем размера эффекта и доверительный интервал размера эффекта. Байесовский тест также выполняется автоматически и информация по нему в строчке внизу. Подробнее. Порисуем А вот как это все выглядит на практике (комментарии к коду - это проверяемая гипотеза): library(AER) library(ggplot2) library(ggstatsplot) library(tidyverse) data(\"PSID7682\") data(\"PSID1976\") # Гипотеза: медиана количества отработанных часов равна 300 gghistostats(data = PSID1976, x = hours, test.value = 300, xlab = \"job hours in week\", centrality.type = \"nonparametric\") Все красиво и доступно - гистограмма значений переменной, линия, изображающая истинную медиану, имя теста - t-критерий Стьюдента, значение критерия - 13.88, р-значение гипотезы, которое позволяет отвергнуть нулевую гипотезу о том, что медиана равна 300. # Гипотеза: средняя зарплата в группах женатых и неженатых в 1976 году одинакова ggbetweenstats(PSID7682 %>% filter(year==1976), x = married,y = wage) Крайне информативный график - можно визуально сравнить распределения, квантили, отмечены средние ... ну и гипотеза о равенстве средних тут тоже отвергается. Есть и модификация для большого количества групп: # Гипотеза: средняя зарплата в разные года одинакова ggdotplotstats(data = Base3 <- PSID7682[,c(12,13)],x = wage,y = year,type = \"robust\") Собственно, точками показано изменение зарплаты в разные года. С помощью type = \"robust\" была задана необходимость использования теста, устойчивого к выбросам. Посмотрим еще на графики: # Гипотеза: среди живущих и неживущих в городе пропорции закончивших / незакончивших колледж одинакова ggbarstats(PSID1976, x = college, y = city) Это шикарный способ визуализации теста 2 на 2 категориальных данных на равенство пропорций. Тест хи-квадрат показывает, что нулевую гипотезу о равенстве пропорций можно отвергнуть. Из простого также можно отметить круговые диаграммы: # Гипотеза: истинная пропорция тех, кто учился / не учился в колледже: 50 на 50 ggpiestats(PSID1976, x = college) Есть возможность вывести и точечный график в показателями значимости корреляции: # Гипотеза: корреляция между количеством отработанных часов и заработной платой нулевая ggscatterstats(data = PSID1976,x = hours,y = wage, point.args = list(size = 1.5, alpha = 0.4, stroke = 0, na.rm = TRUE), smooth.line.args = list(size = 1, color = \"blue\", method = \"lm\", formula = y ~ x, na.rm = TRUE)) Возможности модификации элементов графика тоже большие - можно задать и параметры гистограммы, и параметры точек, и параметры линии. Гипотеза о нулевой корреляции также отвергается. Отдельно стоит сказать про корреляционную матрицу. Она тоже крайне хороша: ggcorrmat(data = PSID1976[,c(2,5,7,14,17)]) Незначимые корреляции сразу зачеркиваются. Однако это еще не все. Пакет эксклюзивен возможностью построения группы графиков, что позволяет отразить результаты тестов для разных групп данных. Примеры ниже: # Проверка гипотезы о равенстве средней зарплаты в группе женатых и неженатых в 1976 году для разных отраслей (yes - работа в промышленности, no - нет) grouped_ggbetweenstats( data = Base1, x = married,y = wage, plot.type = \"violin\", grouping.var = industry, output = \"plot\") # Проверка гипотезы о равенстве среднего количества отработанных часов 300-м для тех, кто живет и не живет в городе grouped_gghistostats(data = PSID1976, x = hours, test.value = 300, grouping.var = city) # Проверка гипотезы о равенстве пропорции уровня образования жены среди мужчин разного уровня образования для тех, кто живет и не живет в городе grouped_ggbarstats(data = PSID1976, x = college, y = hcollege, grouping.var = city, plotgrid.args = list(nrow = 2)) # Проверка гипотезы о равенстве пропорций живущих и не живущих в городе для людей с разным уровнем образования grouped_ggpiestats( data=PSID1976, x = city, grouping.var = college) Отдельно стоит упомянуть про визуализацию уравнений регрессии. Есть очень интересный способ визуализации доверительных интервалов и статистик коэффициентов регрессии: # Визуализация значений и параметров регрессионных коэффициентов mod <- lm(formula = I(log(wage)) ~ weeks*education+industry , data = Base1) ggcoefstats(mod, output = \"plot\") Значение коэффициента, визуализация размаха, результаты проверки гипотезы о равенстве данного показателя нулю - все видно. Но еще больше возможностей представляет пакет ggpmisc. Можно задать и вывести уравнение регрессии со статистикой: library(ggpmisc) formula <- y ~ poly(x, 3, raw = TRUE) ggplot(Base2, aes(x=hours, y=wage)) + geom_point() + stat_poly_line(formula = formula) + stat_poly_eq(aes(label = paste(after_stat(eq.label), \"*\\\" with \\\"*\", after_stat(rr.label), \"*\\\", \\\"*\", after_stat(f.value.label), \"*\\\", and \\\"*\", after_stat(p.value.label), \"*\\\".\\\"\", sep = \"\")), formula = formula, size = 3) Или можно просто вывести классическую табличку с коэффициентами на график: formula <- y ~ x + I(x^2) + I(x^3) ggplot(Base2, aes(x=hours, y=wage)) + geom_point() + geom_smooth(method = \"lm\", formula = formula) + stat_fit_tb(method = \"lm\", method.args = list(formula = formula), tb.vars = c(Parameter = \"term\", Estimate = \"estimate\", \"s.e.\" = \"std.error\", \"italic(t)\" = \"statistic\", \"italic(P)\" = \"p.value\"), label.y = \"top\", label.x = \"right\", parse = TRUE) Выглядит все очень лаконично, емко и точно может быть использовано для подготовки разного рода отчетов и выступлений.\n","id":49}
{"Host":"https://habr.com","Path":"/en/post/357308/?hl=ru_RU&fl=ru,en","Text":"Let's Encrypt выдал 14 766 сертификатов для фишинга PayPal / Habr            \n\n27-03-2017\nБесплатные сертификаты Let's Encrypt стали настолько популярными, что этот центр сертификации от Mozilla и EFF уже вошёл в число крупнейших центров сертификации в Интернете. К сожалению, возможностью получить бесплатный сертификат SSL пользуются не только обычные сайты, но и сайты для фишинга. Для них наличие зелёного значка HTTPS — важное свойство, чтобы отличие с настоящим сайтом не бросалось в глаза. Браузеры помечают такие фальшивки как «безопасные сайты». Специалисты по безопасности ранее обратили внимание, что Let's Encrypt выдаёт слишком много сертификатов со словом “PayPal” в названии домена. Эксперт SSL Store Винсент Линч (Vincent Lynch) даже обращался к центру сертификации с призывом прекратить выдачу таких сертификатов, потому что они с большой долей вероятности будут использоваться для фишинга. Теперь собраны доказательства, что фишинг с использованием сертификатов Let's Encrypt — гораздо более распространённое явление, чем можно было предположить. Винсент Линч собрал данные с помощью поисковой системы crt.sh (её запустил Comodo) и логов системы Certificate Transparency, куда Let's Encrypt отправляет информацию обо всех выданных сертификатах — скоро так будут делать все центры сертификации. По умолчанию поисковик crt.sh не даёт обработать такой обширный запрос, так что исследователи обратились непосредственно к разработчику crt.sh Робу Стрэдлингу (Rob Stradling), который сделал запрос напрямую к базе данных. Специалист подсчитал, что между 1 января 2016 года и 6 марта 2017 года Let's Encrypt выдал сертификаты для 15 720 доменов со словом “PayPal” в названии, причём количество таких сертификатов растёт в геометрической прогрессии, каждый месяц увеличиваясь в 1,5-2 раза. Как видим, фишерам понадобилось определённое время, чтобы освоить Let's Encrypt в качестве основного центра сертификации — но потом процесс пошёл. Let's Encrypt стал постоянным рабочим инструментом мошенников. С декабря 2016 года он ежедневно выдаёт примерно по 100 сертификатов “PayPal”, а в феврале 2017 года — более 180 сертификатов в день. Консервативное изучение случайной выборки из тысячи сайтов показало, что 96,7% из выданных сертификатов принадлежат доменам, на которых работают фишинговые сайты. Это соответствует 14 766 фишинговым доменам из всей выборки в 15 720 сертификатов. Большинство фишинговых сайтов быстро попадают в фильтры сервисов Safe Browsing и вскоре уходят в офлайн. Как только фишинговый сайт помечен как «опасный» в браузере, он становится бесполезным. Поэтому мошенники используют такое большое количество доменов, постоянно меняющих друг друга. Согласно исследованию CYREN, до пометки в Safe Browsing или ухода в офлайн среднее время жизни сайта составляет всего около двух суток. За март 2017 года есть пока неполные данные, но этот месяц может стать первым, когда количество выданных сертификатов на домены “PayPal” уменьшится. Авторы исследования говорят, что изучали ситуацию только с доменами “PayPal”, потому что это самая популярная мишень для фишинга, но такая же ситуация с доменами различных банков, сайтами Bank of America, Apple ID и Google. Например, вот 1963 сертификата с “applid” в названии домена. Использование сертификатов SSL мошенниками было одним из главных беспокойств в связи с запуском бесплатного центра сертификации в конце 2015 года. В прежние времена с платными сертификатами злоумышленники вряд ли смогли бы себе позволить покупку тысяч сертификатов, тем более выдача каждого из них сопровождалась определённой бюрократической процедурой. Сейчас сертификаты стали бесплатными, а их получение и продление можно автоматизировать. По оценке специалистов, по нынешней тенденции Let's Encrypt до конца года выдаст ещё 20 000 сертификатов на фишинговые сайты “PayPal”, так что общее число выданных сертификатов достигнет 35 000. Создатели Let's Encrypt считают, что следить за киберпреступностью и ловить мошенников — не их дело. Они не занимаются модерацией сайтов. Этот проект — одна из нескольких инициатив, направленных на «тотальное шифрование» Интернета. Цель — зашифровать абсолютно всё, и это подразумевает шифрование в том числе «плохих» сайтов, которые тоже переходят на HTTPS. Специалисты по безопасности годами учили пользователей, что зелёный значок защищённого соединения HTTPS означает безопасность. Ситуация ухудшается тем, что даже некоторые браузеры как Chrome при соединении по HTTPS с фишинговым сайтом выводят зелёную плашку с надписью «Безопасно». Теперь нужно проводить дополнительную разъяснительную работу и объяснять, что «защищённое соединение» может быть с вредоносным сайтом тоже. Каждому специалисту это прекрасно понятно, а вот среди пользователей не всегда есть такое понимание, чем и пользуются мошенники. Часть вины лежит на разработчиках UI браузеров. Сообщество ИБ раньше задавалось вопросом, действительно ли вредоносные сайты и программное обеспечение широко используют HTTPS. Теперь получен ответ, что в индустрии фишинга это действительно так.\n","id":50}
{"Host":"https://habr.com","Path":"/ru/companies/cloud_mts/articles/708636/","Text":"Новая теория сетей: безмасштабные сети Альберта-Ласло Барабаши / Хабр                 \n\nФизик Альберт-Ласло Барабаши всегда считал, что сетевые технологии во многом предопределяют жизнь каждого человека. Они связаны с многочисленными сферами: личными отношениями, бизнесом, медициной. Своими исследованиями и идеями Барабаши внес значительный вклад в развитие науки о сетях.\n\nВместе с ученым Рекой Альбертом физик создал модель Барабаши-Альберт (БА). В настоящее время это наиболее известная модель масштабно-инвариантной сети. Она базируется на двух основных принципах: неограниченном росте и преимущественном присоединении. Давайте поговорим о ней подробней и попробуем разобраться, чем она была хороша и почему критики пытались разгромить эту идею.\n\n## Барабаши и развитие теории сетей ##\n\nАльберт-Ласло Барабаши.\nИсточник\n\nБарабаши родился в Румынии, но в возрасте 22 лет вместе с отцом эмигрировал в Венгрию. Он получил степень магистра теоретической физики в Будапештском университете, после этого поступил в Бостонский университет, где получил степень доктора философии.\n\nНа сети Альберт-Ласло Барабаши обратил свое внимание в середине 1990-х годов. Выводы он опубликовал в 1999 году в Science. Его статья, в которой говорится, что безмасштабные сети широко распространены в природе, цитировалась более 30 000 раз. Он считал, что степенные законы описывают не только структуру всемирной паутины, но и многие другие сети (включая даже сеть сотрудничества киноактеров или электрические сети США).\n\nКак это работает? Барабаши утверждал, что такую систему объясняет механизм под названием «преимущественное присоединение». Когда новый узел присоединяется к сети, то он, скорее всего, подключится к другому наиболее заметному узлу (с большим количеством связей). Так, крупные центры обрастают все большим количеством связей.\n\nКоманда Барабаши рассматривала сети без масштабирования, определяя их ключевые свойства: устойчивость к отказу большинства узлов, уязвимость для целенаправленных атак на концентраторы.\n\n## Сети и дата-центры ##\n\nЕсли вы работаете с облаками, то на практике знаете, как важно строить устойчивые к отказам сети. У сервис-провайдеров для этого есть несколько инструментов. Так, связать высокопроизводительные сервисы между собой в частную сеть, особенно в рамках распределенной инфраструктуры, можно с помощью\nвысокоскоростного изолированного VPN\n.\n\nВ нашем облаке есть сервис Direct Connect, с помощью которого можно установить соединение между локальным офисом и любой облачной инсталляцией по выделенным операторским каналам МТС.\n\nКлиент получает маршрутизацию на уровне IP с высокой степенью безопасности между своими физическими и облачными сетями. Direct Connect увеличивает пропускную способность сети и обеспечивает более устойчивую работу, чем стандартное подключение.\n\nМожно связать не только корпоративный дата-центр и облако, но и несколько облачных сервисов в рамках услуги Interconnect.\n\n## Концепция безмасштабности сети ##\n\nа) Пример безмасштабной сети; b)\nSmall-world\nсеть; с) Сеть с\nдвумя сообществами\n. Источник:\nBig Network Analytics Based on Nonconvex Optimization\n\nДля понимания значительной части работ Барабаши, необходимо учитывать концепцию безмасштабности сети.\n\nПриведем в качестве примера Всемирную паутину: сети без масштабирования растут неравномерно. По мере расширения сети некоторые узлы увеличивают количество подключений, другие же этого не делают. Так, крупные новостные сайты могут стремительно и непропорционально увеличиваться. Связи между узловыми сайтами расширяются многократно – до бесконечности, без масштаба (то есть до неисчисляемого масштаба).\n\nЭто означает, что несколько узлов сети должны иметь гораздо больше соединений, чем другие, следуя математической формуле, называемой степенным законом. Следовательно, нет единого масштаба, характеризующего сеть.\n\nДругие примеры сетей без масштабирования: актеры и съемки в одном фильме, исследования сотрудничества (ученые являются соавторами статей) и даже сексуальные контакты.\n\n## Теория сетей и медицина ##\n\nБарабаши также внес значительный вклад в область медицины и биологии. Благодаря ученому были сделаны важные выводы о том, как болезни соотносятся друг с другом через общие гены.\n\nБарабаши показал, как именно безмасштабность сетей проявляется в биологических системах.\n\nЕсли взять, например, клеточный метаболизм, то мы видим, что молекулы, участвующие в сжигании пищи для получения энергии, участвуют в одной и той же биохимической реакции. Барабаши также рассматривал взаимодействия между белками, которые помогают регулировать деятельность клетки.\n\nОпределение основных принципов работы сети помогает понять, как ее изменить, взломать или защитить. Это открытие оказалось полезным для эпидемиологов. Разрушение контактной сети бактерий (например, при помощи вакцинации нескольких особей в популяции) способно прекратить распространение патогена.\n\nСоставление карты молекулярных сетей человеческой клетки может помочь в поиске лекарства против «узловых молекул», провоцирующих определенные болезни. Эти идеи были рассмотрены в совместных работах нескольких ученых, в числе авторов был и Барабаши: «Структура сетевой медицины для выявления возможностей перепрофилирования лекарств для COVID-19» или «Раскрытие генетической схемы нервной системы C. Elegans».\n\n## Критика теории сетей без масштабирования ##\n\nБольшой интерес научного сообщества к безмасштабной парадигме породил также противодействие и критику. Ученые отмечали, что к возникновению степенных законов приводит не только преимущественное присоединение. Не все сети могут масштабироваться: в качестве примеров приводились электросети, метаболические сети, физический интернет.\n\nТакже волну критики вызвало отсутствие статистической строгости в отображении степенных законов сетей, так как степенной закон в логарифмическом графике мог отображаться практически прямой линией. Без проведения тщательной статистической работы нельзя было однозначно утверждать независимость распределения сетей от масштаба.\n\nПредметом спора было также отсутствие точных формулировок. Что имели в виду исследователи под понятием «сети без масштабирования»? Это та сеть, которая подчиняется степенному закону с показателем 2 или 3, или сеть, в которой этот закон возникает из-за предпочтительной привязанности?\n\nМногие молодые ученые и студенты воспринимали сети без масштабирования как устоявшуюся науку, в то время как в научном сообществе продолжали идти дискуссии.\n\nИсследователь Аарон Клаузе утверждал, что сети реального мира демонстрируют огромное разнообразие. Чтобы объяснить его, требуются новые идеи и механизмы. Вопреки распространенному мнению, он считал, что сети без масштабирования довольно редки в природе. Его исследовательская группа собрала доказательства против масштабируемости сетей в огромном онлайн-сборнике «Колорадский индекс сложных сетей» (ICON).\n\nОднако зачастую критика была направлена не против теории сетей как таковой, а против личности Барабаши, который интенсивно продвигал свои проекты для привлечения инвесторов.\n\nВсе это не умаляет того факта, что именно Барабаши создал концепцию безмасштабных сетей и нашел универсальный способ описывать множество разных растущих структур природы, независимо от их размера и масштаба. Он внес огромный научный вклад в молодом возрасте: уже в 32 года (в 2000 году) он получил должность профессора в Университете Нотр-Дам.\n\n## Утопия, которой нет в реальной жизни ##\n\nБарабаши продолжает считать, что отсутствие масштабирования лежит в основе многих крупных сетей. В реальных сетях, помимо преимущественного присоединения, действует множество других механизмов, что и уводит их от чистой масштабируемости. Идеальный степенной закон – это утопия, которая никогда не будет видна в реальной жизни.\n\nБарабаши приводит такой пример. Перо и камень падают с разной скоростью, хотя по закону тяготения скорость должна быть одинаковой. Если бы ученые не знали о влиянии сопротивления воздуха, то сделали бы вывод о том, что гравитация неверна.\n\nПри этом дебаты ученых выглядят как непримиримый спор физиков и статистов. У каждой из сторон есть свои перспективы и свои ценные выводы. Клаузе отмечал, что его работы – это не атака, а лишь призыв к более детальному рассмотрению понятий и новых идей. Пока этого не случилось, общей теории сетей не существует.\n\n## Новое применение теории сетей ##\n\nБарабаши продолжает свои исследования, изучает многочисленные направления в этой области. Он готов делиться своими идеями и вдохновлять молодых ученых на важные открытия. Его работы помогли обнаружить скрытый порядок за различными сложными системами.\n\nВ своей книге «Формула. Универсальные правила успеха» Барабаши рассматривает секреты успеха и признания с точки зрения теории сетей. Оказывается, синтезом математических методов теории сетей возможно также просчитать успех и предсказать будущее. По словам Барабаши, в бизнесе понимание взаимосвязей внутри финансовой системы позволит избежать повторения экономического кризиса 2008 года.\n\nМногие научные деятели были вдохновлены работами Барабаши. Математики, физики и другие ученые создавали статьи с исследованиями в области сетевой науки. Популярные журналы описывали теорию сетей как универсальный закон природы. Исследователи утверждали, что архитектура без масштабирования может дать представление о многих фундаментальных вопросах мироустройства.\n\nБарабаши утверждает: «Удивительно простые и далеко идущие естественные законы управляют структурой и эволюцией всех сложных сетей, которые нас окружают».\n\n---","id":51}
{"Host":"https://habr.com","Path":"/en/post/132800/?mobile=no","Text":"Fakeroot, XCode и PackageMaker / Habr                         \n\n17-11-2011\nХочу поделиться своим опытом адаптации fakeroot для использования на маке в связке с XCode. Fakeroot запускает программы в особом окружении, которое эмулирует сессию супер-пользователя. Права root могут потребоваться при сборке инсталлятора с помощью PackageMaker. Исторически на UNIX-like системах, к которым относится и Mac OS X, сложился определенный подход к распространению софта. В основе любого пакета, будь то deb, rpm или макосевый pkg, лежит архив, который разворачивается непосредственно на целевую файловую систему. Например если пакет litware.pkg разворачивается в папку /usr/local/bin, а в архиве лежит исполняемый файл lit c маской доступа 0755 и владельцем/группой root:wheel, то в системе появится программа /usr/local/bin/lit (маска доступа 0755, владелец/группа root:wheel, т.е. такие же, как в архиве). Пакет создается с помощью специальных утилит (на Mac OS X это консольный packagemaker). Утилита помещает в пакет содержимое папки, которая указывается при запуске. Содержимое папки включаются в пакет как есть. В частности, чтобы изготовить правильный litware.pkg, нeобходимо выставить файлу lit владельцем root:wheel. К сожалению, для успеха данной операции необходимы права супер-пользователя. В Linux эту проблему обходят с помощью fakeroot. На самом деле программы запущены от имени текущего пользователя, но за счет перехвата ряда системных вызовов создается видимость работы от имени root. Например перехватываются chown и stat. Chown успешно меняет owner:group файла, при этом вместо реальной модификации этих полей на файловой системе (что потребовало бы настоящего root), информацию запоминает демон faked. Перехваченный stat незаметно модифицирует параметры файла, полученные от файловой системы, на основе сохраненной информации о предшествующих вызовах chown. Fakeroot-mini Fakeroot поддерживает MacOS X до версии 10.7 включительно. К сожалению, fakeroot отсутствует в обоих популярных менеджерах портов для Mac OS X (MacPorts, Fink). Для корректной работы необходима инсталляция. Кроме того, классический fakeroot неудобен в связке с XCode, так как нет возможности объединить все скрипты, требующие fakeroot, в одну fakeroot сессию. Разные fakeroot сессии совершенно независимы: изменения, сделанные в рамках одной сессии, не видны в другой. На основе кода fakeroot я сделал модифицированную версию fakeroot-mini со следующими ключевыми особенностями: Не требует инсталляции. Распространяется единственная динамическая библиотека. Эту библиотеку можно положить непосредственно в репозиторий что снимает необходимость в установке дополнительного софта для сборки проекта. Хранит информацию в расширенных аттрибутах файловой системы. Это решение устраняет проблему независимых fakeroot сессий. Принцип работы остался без изменений — в запускаемые программы внедряется динамическая библиотека, которая перехватывает системные вызовы. Внедрение библиотеки в процесс — это штатная возможность динамического загрузчика, которая активируется с помощью переменной окружения DYLD_INSERT_LIBRARIES: /usr/bin/env DYLD_INSERT_LIBRARIES=<имя-библиотеки> command [args] Демонстрационный XCode проект По ссылке можно скачать демонстрационный проект для XCode. Проект собирает простое консольное приложение + инсталлятор. Настройки проекта: DEPLOYMENT_LOCATION=YES DEPLOYMENT_POSTPROCESSING=YES INSTALL_OWNER=root INSTALL_GROUP=wheel STRIP_INSTALLED_PRODUCT=NO CHOWN=$(SRCROOT)/chown LIBFAKER=$(SRCROOT)/libfakermini-1.0.0.dylib PACKAGE=$(BUILT_PRODUCTS_DIR)/litware.pkg DEPLOYMENT_LOCATION=YES заставляет XCode раскладывать файлы по установочным путям. Например для программы lit указан установочный путь /usr/local/bin/lit. С включенной настройкой DEPLOYMENT_LOCATION XCode поместит lit по пути $(DSTROOT)/usr/local/bin/lit. Значение $(DSTROOT) настраивается, по-умолчанию это /tmp/<имя-проекта>.dst. Во временной папке будет построено следующее дерево файлов: /tmp/litware.dst/ └── usr ├── local │ └── bin │ └── lit └── share └── man └── man1 └── lit.1 DEPLOYMENT_POSTPROCESSING=YES включает дополнительную обработку файлов в $(DSTROOT). Среди прочего, устанавливается владелец $(INSTALL_OWNER) и группа $(INSTALL_GROUP) файлов. STRIP_INSTALLED_PRODUCT=NO отключает удаление отладочной информации в собранных бинарниках. Из-за ошибки в XCode, иногда при повторной сборке проекта отладочная информация копируется уже после того, как отработал strip и все удалил. CHOWN=$(SRCROOT)/chown указывает XCode использовать скрипт в папке с исходными текстами проекта вместо стандартной утилиты /usr/sbin/chown. Скрипт настраивает fakeroot-окружение и вызывает стандартный chown. Переменные LIBFAKER и PACKAGE определены для удобства, на поведение XCode они не влияют. Инсталлятор изготавливается с помощью следующего скрипта: /usr/bin/env \"DYLD_INSERT_LIBRARIES=${LIBFAKER}\" \\ /usr/sbin/mtree -f \"${SRCROOT}/mtree.spec\" -p \"${DSTROOT}\" -U && \\ rm -f \"${PACKAGE}\" && \\ /usr/bin/env \"DYLD_INSERT_LIBRARIES=${LIBFAKER}\" \\ /Developer/usr/bin/packagemaker --id org.example.litware \\ --root \"${DSTROOT}\" --no-recommend --target 10.5 \\ --out \"${PACKAGE}\" Сначала запускается mtree в fakeroot-окружении для настройки маски доступа и владельца/группы для папок в $(DSTROOT). Необходимо это потому, что хотя XCode и настраивает маску доступа и владельца/группу для $(DSTROOT)/usr/local/bin/lit, промежуточные папки (ex: $(DSTROOT)/usr/local/bin) не обрабатываются. По неизвестной причине, packagemaker не перезаписывает существующие файлы при генерации инсталлятора. Поэтому мы явно удаляем существующий файл инсталлятора в случае его наличия перед запуском packagemaker в fakeroot-окружении. Заключение Описанная технология позволяет создавать инсталляторы непосредственно в XCode, с минимумом настроек и без установки дополнительного софта. Ряд задач при изготовлении инсталлятора требует прав суперпользователя. Fakeroot позволяет успешно выполнить эти задачи непривилегированному пользователю. Информация может быть полезна разработчикам системных утилит и демонов для MacOS. Инсталлятор для обычных пользовательских приложений, как правило, можно собрать без fakeroot. Ссылки Демонстрационный проект для XCode Оригинальный fakeroot Fakeroot-mini Использование xcconfig файлов в XCode Перехват системных вызовов в Mac OS X\n","id":52}
{"Host":"https://habr.com","Path":"/en/post/356046/?mobile=no","Text":"Госдума приняла весь пакет «антитеррористических» законов Яровой-Озерова / Habr                        \n\n24-06-2016\nТеперь операторы связи, мессенджеры и соцсети будут обязаны хранить переписку и голосовые сообщения пользователей полгода Сегодня Госдума приняла во втором и третьем чтениях весь пакет антитеррористических законов, предложенных ранее председателем думского комитета по безопасности Ириной Яровой и главой комитета Совфеда по обороне и безопасности Виктором Озеровым. Поправки будут внесены в Уголовный и Уголовно-процессуальный кодексы, а также в 10 отдельных законов, пишет «РИА». Поправки изложены в двух законопроектах (первый, второй). Принятие пакета поправок может оказать сильное влияние на будущее телекоммуникационной сферы России. Это касается как компаний, работающих в этой сфере, так и рядовых граждан. К примеру, за публичные призывы к терроризму в интернете или оправданию терроризма вводится наказание в виде лишения свободы от пяти до семи лет или предусматривается штраф размером от 300 тысяч до 1 миллиона рублей. Личные данные абонентов и переписка Изначально авторы пакета поправок предлагали обязать операторов связи хранить все данные российских пользователей в России в течение трех лет. Это касается разговоров по телефону, текстовых сообщений и изображений. По оценкам экспертов, бюджет реализации этих предложений превысил бы годовой бюджет крупнейших операторов связи России. Система хранения данных пользователей только для одной из компаний, «Вымпелкома», обошлась бы где-то в $18 млрд. Сравнимые суммы затрат на создание соответствующей инфраструктуры ранее показали и другие компании. При этом годовая выручка операторов гораздо ниже предполагаемых затрат. Так, за прошлый год выручка «Вымпелкома» на российском рынке составила $4,6 млрд, «Мегафона» — $5,2 млрд, МТС — $6,4 млрд. Поэтому в поправки были внесены изменения. Теперь информацию о фактах приема и передачи звонков, текстовых сообщений, фотографий, звуков и видео операторы должны будут хранить на территории России в течение трех лет. Сами разговоры и переписку операторы тоже обязаны хранить, но уже не три года, а шесть месяцев. Кроме того, операторы связи обязаны предоставлять правоохранителям информацию о своих пользователях и оказанных им услугах связи. Эти же требования предъявляются к мессенджерам и социальным сетям. Здесь тоже есть некоторые послабления: срок хранения данных о фактах передачи сообщений сокращен с трех лет до одного года. А содержание переписки компании будут обязаны хранить до полугода. При использовании кодирования сообщений мессенджеры и соцсети обязаны предоставлять ключи для декодирования в ФСБ. В случае неисполнения требования гражданин будет обязан заплатить штраф в размере от 3 до 5 тысяч рублей; должностное лицо — от 30 до 50 тысяч рублей; юридическое лицо — от 800 тысяч до 1 миллиона рублей. ФСБ также освободили от необходимости компенсировать повреждения, устроенные в ходе спецопераций. Пока что порядок, сроки и объем хранения информации не установлен, этот вопрос должно решить правительство РФ. Разглашение гостайны Для средств массовой информации теперь предусмотрено отдельное наказание за разглашение сведений, «составляющих государственную или иную специально охраняемую законом тайну». Провинившееся издание будет обязано заплатить штраф в размере до миллиона рублей. До настоящего момента наказание за подобный проступок не было предусмотрено. Когда поправки вступают в силу? Новая норма о хранении переданных данных вступает в силу с 1 июля 2018 года. Вероятные сложности в ходе реализации поправок О том, что принятие предложения хранить данные и переписку абонентов может привести к росту тарифов на связь ранее предупреждал глава Минкомсвязи Николай Никифоров. «…технически это реализовать возможно, но крайне сложно и крайне затратно — речь идет об огромных суммах затрат. И поэтому соответствующие расчеты мы представим законодателям. Если законопроект в текущем виде принять, это приведет к драматическому росту цен на услуги связи. Я уверен, что этого не хотят ни депутаты, ни Государственная дума, ни наши граждане», — рассказал министр. Он также говорил о том, что хранить все данные абонентов пока невозможно, целесообразно сохранять лишь информацию о факте совершения звонков и отправке сообщений абонентам. По словам директора по связям с общественностью ОАО «Мегафон» Петра Лидова затраты операторов связи, скорее всего, будут возмещать обычные граждане. Он считает, что основную выгоду от реализации пакета поправок получат поставщики оборудования для систем хранения данных. А это, вероятно, американские или китайские компании, то есть деньги будут уходить за рубеж, а не в бюджет. Еще одна сложность — необходимость разработки надежной системы шифрования для хранимых данных пользователей. Об этом говорит директор Регионального общественного центра интернет-технологий Сергей Гребенников. По его словам, затраты на разработку и реализацию такой системы также будут огромными. Что касается раскрытия ключей шифрования — то, во-первых, не все компании в состоянии предоставить такие ключи. В том же мессенджере WhatsApp шифрование ведется на уровне пользователей, на сервере информация не хранится. Сама компания не сможет расшифровать данные пользователей по запросу полиции, если даже и захочет это сделать (например, по решению суда). Ключи шифрования может отказаться предоставлять Facebook, не сможет обеспечить выполнение требований ФСБ и Telegram, где также работает шифрование end-to-end». Вице-президент и технический директор MailRu Group Владимир Габриэлян также считает, что киберпреступники смогут без труда получить к хранимой информации пользователей мелких операторов. Небольшие компании просто не в состоянии обеспечить безопасность большого массива данных пользователей. Это может привести к ситуации, когда в сети за небольшую сумму можно будет купить записи телефонных разговоров абонентов мелких операторов связи.\n","id":53}
{"Host":"https://habr.com","Path":"/ru/post/220873/","Text":"habr.com\n== Самодельный кольцевой светодиодный осветитель для видео, фото и макро съемки\n\n\n25 апр 2014 в 20:39  Идея создания подобного девайса у меня в голове блуждала давно, однако макро, без особых усилий, мне удавалось снимать и со вспышками, а для фото кольцевой источник не был особо и нужен, плюс лень, не дающая сделать все заранее, поэтому работы над ним постоянно откладывались, а при появлении некоторой суммы денег переходили в русло поиска по магазинам или на Ибэе за вменяемые деньги, но те, которые мне были нужны либо стоили запредельных денег, либо первое и требовали доработки.\n\n Все случилось неожиданно, перед Новым годом, моя знакомая актриса и певица, попросила заснять ее конкурсное выступление в одном из клубов и сделать видео. Мой простенький и компактный источник видеосвета, со 160 небольшими диодами, который имелся и есть практически у всех занимающихся видеосъемкой операторов, в клубе однозначно не работает из-за мощной цветной подсветки, или иными словами с пары метров снять что-то можно, а вот метров с 10-15 можно и не снять. Пришлось срочно составить технические требования к характеристикам прибора и в интернете пересмотреть множество осветителей ими обладающих.\n\n На DX мной был найден достаточно компактный, легкий осветитель с литиевым аккумулятором, попросил продавца прислать точное описание из инструкции, потому как у китайцев на сайтах может быть написано все что угодно, в общем оказалось в описании были написаны, другие размеры внутреннего отверстия, а это означало что с моим рабочим объективом этот свет работать не будет. До съемки оставалась неделя, и нужно было или отказываться, или что-то делать … я выбрал последнее, и как потом оказалось был прав, получился удобный осветитель на все случаи жизни!\n\n Ниже на фото, то что получилось.Ниже приведу технические характеристики которые были для меня критичны при выборе:\n- Световой поток – не менее 1500Lm\n- Угол рассеяния – не менее 15 — 20 град. Этот угол на расстоянии в 7-10 метров позволяет полностью подсветить объект съемки, задача подсвечивать остальные части сцены – не моя. В дальнейшем оказалось возможным быстро, примерно за 5минут, изменить это значение на любое от 5 до 120, за счет смены линз, а больше 120 не позволяет получившийся корпус.\n- Цветовая температура – планировалось иметь плавную, от 2700K – до 6500K, но параметр CRI, как бы он не был высок для глаза, для камеры большого значения не имеет, плохо — все, и 2700K, и 4200K, и 5500K, и 6500K. Снимал одну и ту же тестовую шкалу Кодак для калибровки сканера и сравнивал с ее картинкой, снятой на солнце. Лучший результат оказался у набора диодов с разным люминофором, в результате цветовая температура источника по мнению камеры получилась 4000K. Хотя, если не придираться можно снимать и с одним типом люминофора.\n- Возможность плавно регулировать световой поток\n- Температура не цветовая – она должна быть гуманна не только для диодов, но и для рук, если держать осветитель, что означает менее 55-60 град.\n- Время работы без подзарядки не менее 1.5 часов\n- Вес и размер, аккумулятор и источник тока должны помещаться в поясную сумку “Nord Face” и не отрывать ее. Всего получилось 2кг с небольшим, но есть планы заменить на литий и скинуть еще 1.5 кг.\n\n\n Со светодиодной техникой у меня достаточно большой опыт работы, поэтому покопавшись среди имеющегося подобрал пару колец под светодиоды, колец на 18 диодов не нашел, а заказывать время уже не было, те что были, ушли на переделку круглых потолочных светильников с ламп накаливания на светодиоды, поэтому пришлось довольствоваться оставшимися кольцами на 12 диодов. \n\n Попробовал поставить на них 5 вт. диоды от Bridgelux, света получилось очень много, но и тепла не мало, с теплом было решаемо, а вот регуляторов света с ШИМ у меня не было, я их заказываю у русского продавца с Молотка из Китая ([http://molotok.ru/my_page.php?uid=14496341](http://molotok.ru/my_page.php?uid=14496341)), торгующим разным светом для дома, диодами и прочим, не могу сказать, что цены очень хорошие, конечно дешевле чем в Москве, но самое главное, человек понимает, что продает и может что-то редкое найти под заказ. В общем пришлось отказаться от 60W источника и поставить диоды по 3W, итого получилось 36W, световой поток примерно 2600Lm под линзами теряется до 15%.\n\n Камера их видит как-то вот так, поскольку устанавливает среднюю цветовую температуру от суммарного потока.\n\n Вторая проблема была подобрать корпус, в который это кольцо могло встать и главное этот корпус должен был стать радиатором для всего осветительного прибора, крепиться на бленду объектива и как следствие иметь малый вес и не нагревать объектив.\n\n Для этих целей как нельзя лучше подошла алюминиевая форма для выпечки, в которой сзади было вырезано отверстие под бленду, крепление на бленду было сделано из технического жесткого поролона черного цвета и сотового поликарбоната, свернутого в трубку по диаметру.\n\n Сам корпус используется в качестве радиатора, поэтому кольцо с диодами было закреплено на 3 болтах, для лучшего теплового контакта была использована термопаста, часть ее видна на фото, и поскольку толщина стенок около 1мм, то сзади есть еще одно кольцо, совпадающее по размером с тем на котором стоят диоды, из алюминия толщиной 3 мм, эти два кольца зажимают между собой корпус конструкции, для большей жесткости.\n\n Сверху наружная часть конструкции, находящаяся в светильнике обмотана пищевой фольгой, для лучшего отражения света, и для того что бы защитить края объектива от бликов или от проникновения туда света от источника, сам источник крепиться к держателю на 24 саморезах диаметра 1.8 мм и длиной 10 мм, край поликарбоната куда они вворачиваются залит термоклеем BOSH.\n\n Вопрос с питанием был решен следующим образом, в качестве аккумулятора энергии был выбран обычный аккумулятор от мотоцикла на 12V и емкостью 5.5Ah. Поскольку для 2 параллельных линеек из 6 диодов требуется примерно около 24V и ток в 1.5A, пришлось ставить повышающий DC/DC преобразователь с высоким коэффициентом преобразования, работающий на частоте 300kHz, в целом получилось сохранить около 93% энергии.\n\n Схема взята с сайта — [http://radiokot.ru](http://radiokot.ru), вся ее прелесть, в том, что половина компонентов находятся в старых компьютерных блоках питания, другая на материнках, в формирователях питания процессоров, они же кстати содержат ферриты, работающие на 300kHz.\n\n В дальнейшем между выходом и диодами, был поставлен радиоуправляемый (433MHz) ШИМ регулятор мощности с 3 предустановленными значениями 25%, 50%, 100% и имеющий плавную регулировку.\n\n Для его зарядки использую как стандартное ЗУ от розетки, так и возможность подключить его параллельно к такому же, основному в мотоцикле.В заключении\n\n После более полугода эксплуатации, видео снимает на отлично, мелочевку для интернет магазинов практически без теней, что упрощает обтравку, брал с собой в походы по пещерам, линзы менял на 5-8 градусные.\n\n Ниже несколько фото, сделанных для интернет магазина.\n\n Несколько фотографий, выполненных в пещерах, матовый рассеиватель был снят, использовались линзы с углами рассеяния 5-15 град.\n\n На следующем фото, осветитель снят с камеры, и за долгое время экспонирования руками (светом) прорисованы разные части кадра:\n\n Вот так выглядит эта конструкция на камере. Насколько помню стояла 1/4 мощности.\n\n На фото ниже свет, смешанный с естественным, проходящим через отверстие в своде пещеры.Цена вопроса\n- Кольцо PCB на 12 мощных светодиодов – US $6.20\n- 12 мощных светодиодов – US $12.00\n- Линзы и холдеры для 12 светодиодов — US $4.80\n- Кастрюлька для выпечки – US $2.70\n- Чёрная акриловая краска – US $1.50\n- Аккумулятор – (был)\n- Повышающий DC/DC преобразователь – US $18.50\n- Радиоуправляемый регулятор света с ШИМ – US $9.20\n- Итого: – US $54.90 или в рублях – 1970 руб.\n","id":54}
{"Host":"https://habr.com","Path":"/ru/post/482612/?mobile=no","Text":"Трогательный инженерный бизиборд, Новый Год и волонтеры / Хабр                \n\n31-12-2019\nЗачем нужен бизиборд, когда у папы много интересных инструментов? Опасные убраны, а пассатижами пусть ребенок играет, шуруповерт нажимает, ручки осциллографа крутит. Но наступил момент и сделан инженерный бизиборд. Таким каким он должен быть, в противоположность фабричному изделию – красивому, глянцевому, красочному, со всеми закругленными углами и краями. Разноцветному для глаз, но однородному и скучному на ощупь. С чего все началось Началось с объявления чуть больше года назад в социальной сети, что московскому психоневралогическому интернату (ПНИ) требуются бизиборды: Что это такое Что такое ПНИ трудно представить, но можно посмотреть в youtube. И если волонтеры пишут, что в ПНИ проблема дефицита сенсорных впечатлений, значит это ОЧЕНЬ большая проблема. Мы, взрослые, ходим по строительному гипермаркету и крутим в руках железки, щупаем трубки, проводим рукой по оплетке и т.д. Женщины в магазине трогают вообще всё. Я инженер, регулярно покупаю какие-то детали, чтобы на столе лежали, и я достаточное количество раз их потрогал и постепенно понял – смогу ли применить как задумал. Каждый в детстве лежал на кровати и бесконечно долго разглядывал и щупал ковер. Или, провинившись, стоял в углу и ковырял обои. Щупать – это способность и потребность человека. Когда мы с товарищем увидели бизиборды, которые планируется купить для ПНИ, решили сделать максимально трогательный бизиборд сами. Трогательный в прямом смысле. Куплены парные раскручивающиеся фитинги из пластика, никелированной стали, бронзы, чугуна оцинкованного и черного чугуна. Медные трубки, гофрированная трубка из нержавеющей стали, по которой со звоном долго спускается шайба, если ее поднять и отпустить. Самый красивый материал, конечно, никелированная сталь. Но красота не главное, главное контраст. Поэтому рядом с никелированной деталью висит деталь из черного чугуна, и трудно сказать – какой материал даст больший чувственный опыт. «Чугунные игрушки» из анекдота – не насмешка, а необходимость; ребенку важно почувствовать тяжесть и грубую фактуру материала. Все фитинги по-разному закручиваются, интересней других крутить сложные составные фитинги для труб ПНД. Гофра для унитаза удивляет диаметром, и ее можно растягивать. Пеньковая веревка впечатляюще колюча. Отрезной диск красивый, брутальный, и на ощупь грубый. Образцы керамики вдали выглядят обычно, но красивые, если рассматривать их многослойное покрытие вблизи, проводя по рельефу пальцами. Как это сделано За основу взят лист фанеры 1500×1000×10 мм. В строительном гипермаркете есть услуга пила в размер, к основному листу дополнительно нарезали ребер жесткости. С размером листа ошибся, лист чуть-чуть не влез в автомобиль, а гипермаркет уже закрылся и пришлось в ночь идти с этим листом домой пешком – не надо так. Ребра жесткости прикручены шурупами 16 мм – стянут оба листа, но не выйдут с другой стороны. Всюду, где можно, использован детский труд. Детский труд экономит деньги – не нужно покупать очередной конструктор. Вон папе купили новое кресло, иди распаковывай и собирай, нужна будет помощь — позовешь. И детский труд – это педагогично, ребенок вырастет умеющим не только работать руками, но и делегировать задачи. Дети учатся не тому, чему их учат взрослые, а тому что взрослые при детях делают. Покрасили на лестничной площадке, постелив полиэтилен. Закрепили детали тросами в ПВХ-оплетке. Стальные тросы в ПВХ-оплетке — прекрасный материал, только пришлось отдельно ехать в магазин за кусачками, которые этот трос смогут перекусить. На обратной стороне тросы затянули узлами, а узлы залили термоклеем. Термоклей имеет хорошую адгезию к ПВХ, залитый узел сам не распустится и выдернуть его с лицевой стороны стенда невозможно. Чем все закончилось В «наш» ПНИ уже успели закупить готовые бизиборды, меня переправили к волонтерам другого ПНИ. Там фотографии посмотрели и сказали – а давайте отправим ваш стенд в Азовский интернат для умственно отсталых детей. Сказали – и отправили. В новогоднюю ночь приехала еще одна бригада волонтеров, погрузили стенд в огромный прицеп к АЗЛК 2141 уже забитый коробками с подарками интернату, и в метель уехали. Очень романтично. Дальше я долго пытался получить какую-то обратную связь от волонтеров, но смог лишь узнать телефон интерната. Там звонку удивились, обрадовались, сказали, что не поняли, что это им привезли и отдали в уголок труда. И сказали, что примут любую помощь, и чтобы я приходил и помогал. Я посмотрел фотографии интерната, посмотрел на детей и увидел, что стенд отправили совершенно не туда. Ситуация в интернате по сравнению с ПНИ роскошная, проблемы запертости в своем теле и на своей кровати у детей нет. Жил бы рядом с интернатом – приходил бы по субботам, учил бы этих детей программировать Ардуино. Опыт преподавания робототехники в школе есть, справился бы и в интернате. Но между нами 1100 км. А стенд все же нужен был в ПНИ. Выводы Все сделано правильно, только нужно самому говорить с тем, для кого что-то делаешь. Самому изучать потребность, самому получать обратную связь и самому делать следующий шаг. Стенд делается относительно легко, технология «фанера + тросы в пвх + термоклей» рабочая, предлагаю использовать наш опыт.\n","id":55}
{"Host":"https://habr.com","Path":"/en/post/128943/?mobile=no","Text":"Хроники пентестера / Habr                                \n\n22-09-2011\nКак и несколько лет назад, веб-приложения по-прежнему остаются наиболее привлекательной мишенью для нарушителей всех мастей. Тут всегда хватит места и для матерых SEOшников, и для желающих нести свои мысли в массы путем подмены содержимого страниц, и, разумеется, веб-приложения являются первоочередной мишенью при преодолении внешнего периметра компаний, как в тестированиях на проникновение, так и в суровой жизни. Повсеместно низкая безопасность веб-приложений обусловлена множеством факторов: от качества разрабатываемого кода и выбранного языка программирования, до используемых конфигураций на стороне веб-сервера. Масло в огонь добавляет еще тот факт, что безопасность веб-приложений держится особняком относительно общей стратегии безопасности. Менеджеры структурных подразделений инициируют процессы развития бизнеса, которые в свою очередь, так или иначе, базируются на веб-технологиях. При этом служба управления информационной безопасностью в среднестатистической компании как бы закрывает глаза на то, что приобретаемое решение может содержать уязвимости. Более того, из опыта проводимых работ, больше половины ИБ подразделений в российских компаниях даже не подозревают, кто ответственен за внешний, официальный сайт компании, не говоря уже о том, кто занимается его безопасностью. Потому веб-приложения и взламывают на потоке. Проводимые в последнее время пентесты наглядно подтверждают эти доводы. Когда группа пентестеров используя уязвимости веб-приложений доходили до выполнения команд операционной системы, (помимо радости собственного превосходства)) создавалось обоснованное ощущение, что мы тут не одни. В одном из пентестов на сайте заказчика было найдено несколько критических уязвимостей, которые в перспективе могли позволить выполнять команды на сервере. Однако воспользоваться ими продолжительное время не получалось. Секунды превращались в минуты, минуты складывались в часы, цель была так близка, но все старания были безуспешны. Оглядевшись вокруг было замечено, что перед нами лишь один из сайтов большого хостинга. Учитывая, что на страницах исследуемого сайта и его соседей был замечен один и тот же зловред, вполне логично предположить, что злоумышленник до нас вряд ли стал бы заморачиваться с уязвимостями именно нашего анализируемого сайта. Наверняка он шел по пути наименьшего сопротивления и залился через соседей. Но ограничения при пентесте не позволяют повторить подобные подвиги. Между тем, старания все-таки не были напрасны и используя каскад уязвимостей сайт заказчика сдал свою оборону. Далее, вместо классического сбора информации, предстояло выяснить, каким образом зловред попал на страницы этого ресурса. Следующая история раскрывает тему использования уязвимостей нулевого дня при пентестах. Перед нами очередной оф. сайт другого заказчика на вполне себе безопасной системе управления сайтом Joomla. При поверхностном серфинге на глаза попадает уязвимый модуль, информация об уязвимости в котором еще не успела просочиться в паблик. Также известна информация, что на днях через указанный модуль проходила массовая заливка. И как это не странно, сплоитс у нас уже был на руках:)) Далее, как в голливудских фильмах. Вводим target, запускаем, заливаемся, радуемся)) После этого предстояло ответить на вопрос, «кто здесь»? И ответ не заставил себя долго ждать. Как бы шеллы с eval и base64 очень палевны. Последняя история, о которой я хочу сегодня рассказать, не менее показательна, чем остальные. Шутка ли, когда на одном из внешних сайтов всеми известного и любимого российского банка установлен бэкдор на языке наших поднебесных друзей. Более того, даты показали, что бэкдор был залит более года назад и на протяжении всего этого времени оставался незамеченным! Остроты всей истории придает также тот факт, что операционная система, на котором расположен веб-сервер, входит в состав корпоративного домена Active Directory банка. Как было выяснено позже службой управления ИБ самого банка, за все время был зафиксирован лишь один запрос, который собственно и установил бэкдор, т.е. в этот раз просто повезло. Кстати, бэкдор (аля веб-шелл) под Tomcat действительно оказался довольно наворочен (добавили себе в копилку), а установлен он был к слову через дефолты в интерфейсе администрирования Tomcat. Так как же защитится от подобных угроз? Во-первых, если вы планируете развернуть свой сайт на внешнем хостинге, поинтересуйтесь у хостера, какие действия он предпринимает для защиты сайтов своих клиентов. На эту тему вспоминается еще одна история. Анализировали мы как-то сайт, расположенный в одном из регионов нашей необъятной родины. Зафиксировали SQL-иньекцию и принялись ее раскручивать, и, буквально через полчаса, видим послание от хостера в свой адрес:) Но самое примечательное в этой истории это то, что хостер оперативно обеспечил виртуальный патчинг найденной нами уязвимости. С другой стороны, выбирая хостера, возможно не следует слишком много от него ожидать, но фундаментальные механизмы безопасности он должен обеспечивать при оказании своих услуг. Во-вторых — используйте стойкие, уникальные пароли везде, особенно на внешних веб-приложениях. Доступ к интерфейсам администрирования сайтом предоставляйте только доверенному списку IP-адресов. К слову, эти два простых правила вместе с минимальными привилегиями у пользователя, который взаимодействует с базой данных, помогает существенно снизить ущерб при наличии уязвимости типа внедрение операторов SQL. И наконец, защищаясь от атак нулевого дня помните, что чем больше защитных механизмов вы обеспечите, тем выше вероятность того, что вы убережете свой сайт от подобной напасти. Вероятно от целенаправленной атаки с глупыми уязвимостями в коде приложения защититься не получится, но от массовых атак защититься вполне реально. А это уже не мало! Так, что же требуется, чтобы защитить сайт от атаки нулевого дня при этом не прибегая к анализу его безопасности методом фаззинга или исходного кода? По моему видению должно быть реализовано как минимум следующее: — стойкие пароли к используемым идентификаторам во всех интерфейсах (ftp, раздел администрирования и т.п.). — все интерфейсы администрирования должны быть доступны только для доверенных IP-адресов; опционально, использование двухфакторной аутентификации везде, где это возможно. — минимизация привилегий во всех компонентах. — правильно настроенные таблицы разграничения доступа (начиная с файловой системы и заканчивая разграничением доступа средствами системы управления сайтом, если она предоставляет подобный функционал). — безопасные конфигурации веб-сервера и его среды (например, безопасные конфигурации PHP). — использование превентивных механизмов контроля (Web Application Firewall). — ограничение сетевого доступа со стороны веб-приложения к любым другим компонентам сети; размещение веб-сервера в изолированной сетевой среде (aka персональная ДМЗ). Придерживаясь перечисленных подходов вы существенно повысите безопасность своего веб-приложения. Желаю успехов в постоянной битве добра с добром по другую сторону :)\n","id":56}
{"Host":"https://habr.com","Path":"/en/post/518530/?mobile=yes","Text":"«Живи, вкалывай, вкалывай, вкалывай, сдохни! Репортаж с темной стороны Кремниевой долины» Отзыв о книге / Habr                        \n\n10-09-2020\n\"… Решения по разработке и распространению меняющих мир технологий нельзя оставить на произвол нескольких чересчур уверенных в себе богачей с дипломами Стэнфорда и глубоким презрением к истории, политике, языку и культуре, не говоря уже о страданиях бедных...\" Кори Пайн Интернет — это свободная и лишенная трения среда обмена, где все играют на равных, а лучшие идеи неизбежно пробиваются наверх. Если вы с этим не согласны, то вы — чудак и циник. Как и автор книги «Живи, вкалывай, вкалывай, вкалывай, сдохни!». Об авторе известно немногое: Кори Пайн (Corey Pein) проживает в Портленде (штат Орегон), журналист, вел новостной сайт в Лондоне, решил сменить карьеру и стал увлекаться информационными технологиями. Как и многие, кто пришел в техиндустрию после экономического кризиса 2008 года, учился программированию самостоятельно, читал техблоги на Hacker News. Но вскоре понял, что навыки которые он приобрел (писать приложение на Ruby on Rails, управлять виртуальным сервером) не приносят ему дохода. И тогда он принял решение, которое рано или поздно приходит в голову, человеку так или иначе связанным с разработкой цифровых технологий — поехать в Сан-Франциско, в Кремниевую долину, чтобы открыть свой стартап. Кремниевая долина это Мекка для разработчика, кодера, программиста. Столица миллиардеров Америки, фабрика инноваций и родина героев хайтека. В Голливуде у всех есть сценарий для фильма, в Кремниевой долине у всех есть идея для стартапа. Но в отличие от Голливуда, в Кремниевой долине не будет проблем с поиском инвесторов, акционеров и финансистов. Действительно, вы можете поучаствовать на семинарах и презентовать свой стартап на PowerPoint или выступить на meetUp с минутными питчами. Такие мероприятия там проводятся каждую неделю. Возможностей полно. Наберетесь смелости и дерзайте! Все достигают успеха, потерпеть неудачу может лишь дурак! Но автор потерпел неудачу и неоднократно. И он видел как терпят неудачу почти все такие же начинающие. И эта книга об этом. Первая половина книги повествует о мытарстах автора, о поиске и снятии жилья, о посещениях техтусовок. \"… Жилье стоило 85 долларов за ночь — меньше, чем в среднем по рынку, но все ровно больше, чем я мог себе позволить...\" Вы узнаете какие цели преследуют организаторы таких мероприятий для разработчиков, как блефуют презентующие, и что на самом деле стоит за бонусными напитками и едой в офисах Google, Apple и GitHub. Как появились такие компании как Uber и Yelp, кто их спонсирует и как они побеждают все судебные процессы. Чем отличается «трутни» от «быков» и какие еще классификации технарей бывают. Как устроена работа в Mechanical Turk (подробней есть статья Хабра тут) и Fiverr. Вторая половина посвящена корифеям мира цифровой индустрии и какие идеи они проповедуют. «Основатели стартапов — это капитал или рабочая сила? Марк Цукерберг — капитал. Но на каждого Цукерберга найдется сотня парней, которых уволили из их же стартапов. Это рабочая сила.» Узнаете как видят будущее основатели корпораций, чьи принципы они унаследовали и что хотят сделать с мировыми государствами. Почему о Стиве Джобсе и Илоне Маске пишут будто они рок-звезды, что делают с негативными отзывами Facebook, и при чем тут политики. Познакомитесь с Мэн-цзы Молдбагом и краснотаблеточной философией. Как добились успеха Питер Тиль и Винсент Серф. Как уклоняются от налогов Джефф Безос и Netflix. И много чего еще. Стоит ли читать? Несомненно. Напоминает романы «Рабы Майкрософт» и «J-Pod» Дугласа Коупленда. Но все же это документальная проза, а не художественная. Написано все простым языком, есть меткие и едкие подколы. Если у вас есть идея открыт свой стартап или поехать в Кремниевую долину (почему бы и нет!), то чтение этой книги будет как минимум полезным. Единственный возможно минус — это излишняя предвзятость автора к «технарям». При чтении может сложиться впечатление, что автор описывает лишь одну сторону медали. Но даже освещение лишь этой одной стороны дает пищу для множества размышлений. \"… Даже если наши общественные институты уцелеют в ближайшем будущем, очевидна тенденция к росту корпоративного контроля над всеми сферами жизни. Благодаря своей относительной популярности, огромным финансовым запасам и доказанной способности подрывать и поглощать другие индустрии, ведущие техкомпании готовы взять на себя непропорционально большую долю этого контроля. Ну а что до всех прочих — то нам придется вкалывать, вкалывать, вкалывать, вкалывать...\"\n","id":57}
{"Host":"https://habr.com","Path":"/en/company/1cloud/blog/437620/?mobile=no","Text":"Как облачные технологии меняют то, как и чем мы лечимся / Habr             \n\n26-01-2019\nЖелание спасти жизни и сохранить здоровье — один из наиболее важных двигателей прогресса. В информационный век появляется все больше новых возможностей для медицинских учреждений и развития науки. Многие из таких возможностей появились благодаря адаптации этой сферой облачных технологий. Расскажем об этом подробнее в сегодняшнем материале. / Flickr / National Eye Institute / CC BY Море данных Качество медицинских услуг напрямую зависит от уровня анализа данных, собранных на различных этапах исследовательской или практической деятельности в этой области. Согласно отчету IDC, ежегодный прирост медицинских данных составляет 48%, что тянет за собой новые требования мед. учреждений к объёму хранилищ и IT-инфраструктуре. За рост показателей в этой сфере «отвечают» сразу несколько факторов: Научная деятельность стремительно развивается. Частота появления новых научных публикаций, которыми руководствуются врачи и производители мед. оборудования, беспрецедентна. Ожидается, что к 2020 году объёмы медицинской информации будут удваиваться каждые 70 дней. Появляются новые способы сбора данных. Мобильные устройства для мониторинга и диагностики становятся источниками важной статистики, позволяющей ставить правильные и своевременные диагнозы. Например, Aruba, дочерняя компания HP, утверждает, что уже в этом году 87% американских провайдеров медицинских услуг будут использовать IoT-решения. Больше пациентов — больше данных. Невозможно забыть и про стареющее население развитых стран. Ожидаемая продолжительность жизни растет — не в последнюю очередь благодаря качественным и доступным медицинским услугам. Как следствие, нагрузка (в том числе информационная) на систему здравоохранения только увеличивается. Все это усложняет процессы обработки медицинских данных, а также их хранения и, что немаловажно, защиты. Например, прошлогоднее исследование Thales Group показало, что утечки данных — не редкость в медицинских организациях. Чем помогут облачные технологии Облачные технологии дают медицинским организациям шанс переосмыслить свои рабочие процессы и справиться с вышеописанными вызовами и проблемами. Посмотрим, какие изменения в сфере здравоохранения происходят благодаря новой инфраструктуре. Упрощается обмен медицинской информацией. Медицинским организациям приходится хранить огромное количество информации о пациентах: историю болезни, кардиограммы, рентгенологические снимки, снимки КТ и др. Чтобы справиться с таким объемом данных, все больше стран постепенно внедряют так называемые электронные медицинские карты (ЭМК), реализуемые на базе облачных технологий. Все данные о пациенте переводятся в цифровой формат и размещаются на удаленном сервере в облаке IaaS-провайдера. Там они оказываются надежно защищены с помощью алгоритмов шифрования и систем резервного копирования. При этом доступ к данным пациента могут получить авторизованные медработники в любой поликлинике страны. Таким образом, человеку не приходится заводить медкарту в каждом медучреждении, куда он обращается, а врачам проще работать с историей болезни. / Flickr / PxHere PD Облака также упрощают взаимодействие фармацевтических компаний с другими организациями. Например, Pfeizer уже использует облачную платформу для хранения конфиденциальной медицинской информации лабораторий-партнеров. Облачное хранилище с метриками и метаданными помогает им проводить 800 клинических испытаний медпрепаратов ежегодно. Появляются новые аналитические инструменты. Интеграция облачных аналитических систем с электронными каталогами данных пациентов позволит проводить больше исследовательских проектов. Бывший директор Кливлендской клиники Тоби Косгров (Toby Cosgrove) в недавнем интервью заявил, что именно автоматизированная обработка данных на облачных платформах станет ключом к повышению эффективности медицинских учреждений. Примером могут быть генетические исследования аутизма. Данные, собранные фондом Autism Speaks (более ста терабайт), обрабатываются на облачной платформе Google Genomics. Она использует решения вроде BigQuery для анализа больших данных на кластерах Apache Spark, Cloud Dataflow или Oracle Grid Engine. Исследователи утверждают, что такие проекты приведут к развитию технологий на стыке генетики и медицины. В США уже появился центр «персонализированной медицины», составляющий программу лечения на основе генетических данных пациента. Кстати, о лечении… Меняются подходы к диагностике пациентов. Облако идёт рука об руку с мобильными технологиями, позволяя дистанционно делать то, что раньше требовало личного присутствия. Для этого медучреждения размещают в облаке системы обмена медицинскими данными и даже решения для проведения видеоконференций. На базе последних часто строятся сервисы телемедицины. Пациенты теперь могут получать медицинские консультации не выходя из дома, что особенно помогает людям с ограниченными возможностями. Аналитики утверждают, что в ближайшую пару лет стоимость рынка телемедицины достигнет 40 млрд долларов. При этом разрабатываются технологии, которые должны помочь врачам с постановкой «удаленных» диагнозов. Израильская компания Beyond Verbal в сентябре 2016 года представила проект на базе системы ИИ, способной диагностировать заболевания по голосу больного. Её задача — распознать интонационные нюансы, которые человеку на слух не определить. Решение обучалось на двух миллионах аудиозаписей с голосами, говорящими на 40 различных языках. Другое направление развития — умные устройства интернета вещей. Инфраструктуру для этих инноваций предоставляют облачные решения. Они позволяют врачам удалённо мониторить состояние здоровья пациентов, снижая нагрузку на больницы и время реагирования на чрезвычайные события. Эксперимент, проведённый в Британии еще десять лет назад, показал 45% снижение смертности среди пациентов, использовавших устройства удалённого мониторинга. Развитие облачных технологий и интернета вещей привело к появлению большого медицинских гаджетов. В конце лета 2017 года компания Band-Aid выпустила «умный» пластырь, который позволит докторам получать данные о состоянии здоровья людей в сельской и ли труднодоступной местности, где, как правило, медицина развита слабо. Облачные технологии также упрощают жизнь пациентам при диагностике заболевания. В частности, теперь для получения результатов анализов не нужен личный визит к врачу — их можно узнать через интернет. Веб-порталы для пациентов становятся нормой, и, пусть по старой привычке, ими пользуются далеко не все, в будущем можно ожидать их массовую адаптацию. P.S. Пока мы можем наблюдать лишь первые шаги в сфере «облачной медицины» — со временем это направление будет развиваться все быстрее. И уже сейчас среди клиентов 1cloud.ru можно найти компании, занимающиеся предоставлением медицинских услуг, разработкой медицинского оборудования и препаратов. Примером может служить лабораторная служба HELIX. Многие технологические решения, которые сейчас лишь разрабатываются, в будущем станут индустриальными стандартами и окажут серьезное влияние на подходы к лечению. Где еще поможет облако: Как IaaS помогает франчайзи «1С»: опыт 1cloud Партизанский маркетинг: интервью с клиентом Как 1cloud упрощает жизнь 1С-разработчику\n","id":58}
{"Host":"https://habr.com","Path":"/en/post/281463/?hl=ru_RU&fl=ru,en","Text":"BYOD — удобство против безопасности / Habr             \n\n13-04-2016\nПовсеместное использование стратегии Bring Your Own Device (BYOD, использование персональных устройств в рабочих целях) во всех сферах деятельности позволяет ускорить бизнес-процессы, практически мгновенно получать актуальную информацию и упростить коммуникацию с коллегами. При видимом удобстве использования и мобильностью сотрудников возникает множество проблем и рисков информационной безопасности, о которых и пойдет речь в этой статье. Многие современные компании поставлены перед необходимостью искать баланс между мобильностью сотрудников и информационной безопасностью бизнеса, решая ряд новых задач, связанных с эффективностью управления персональными устройствами и обеспечением безопасности их применения. Личные ноутбуки Использование личных ноутбуков в рабочих целях, либо в качестве вспомогательного устройства — довольно распространённая практика. Тем не менее это одна из основных головных болей сотрудников ИТ/ИБ подразделений: устройство может содержать критичные данные, либо реквизиты доступа к ресурсам корпоративной сети, электронной почте и т.д. По понятным причинам контролировать содержимое таких устройств и обеспечивать их полноценную защиту крайне затруднительно и носит скорее рекомендательный характер. Да, существую компании, в которых политикой безопасности строго прописаны правила использования личной техники, вернее сказать запрет на их использование, но, тем не менее, в угоду удобству многие пренебрегают этими советами, несмотря на административные или иные меры. Личные устройства могут быть наиболее уязвимыми для целевых атак. Злоумышленникам гораздо проще атаковать «одинокий» ноутбук, используя те или иные атаки или методы воздействия, нежели устройство, находящееся под контролем специалистов, настроенное и поддерживаемое с должными мерами безопасности. Еще одно проблемой «домашних» устройств — в большинстве случаев современные пользователи работают с правами локального администратора, что упрощает возможность доставки на эти устройства вредоносного кода, например с помощью социотехнических атак. Про регулярное резервное копирование данных слышали все, но на практике все довольно печально: если нет контролирующего эти процессы норматива или регламента — пользователь устройства задумывается об этом очень редко, а делает еще реже. Типичным кошмаром для ИТ-отдела является и незащищенная информация, хранящаяся на личном ноутбуке, который можно потерять в аэропорту или в такси. Очень многие люди считают что пароль «на вход» обеспечивает надлежащие меры безопасности и относятся к шифрованию данных, как к чему то из области шпионских фильмов. Смартфоны Современные смартфоны и планшеты все меньше отличаются от ПК с точки зрения хранящихся на них корпоративных данных. Доступ к электронной почте, корпоративным документам, специализированным сервисам, деловые контакты и календари, заметки, планы и графики работ — это и многое другое может получить злоумышленник, завладев таким устройством, либо получив к нему доступ. Огромным фактором риска в случае утери или кражи устройства является невозможность мгновенно уведомить ответственных лиц, либо заблокировать доступ к устройству. Также, смартфоны и планшеты в большей степени подвержены атакам класса Man-in-the-Middle, т.к. контроль за эфиром в зоне передвижения владельца смартфона осуществить очень сложно, а заставить подключится мобильное устройство к «известной» точке доступа довольно легко. После подключения к точке доступа, в большинстве случаев без ведома и желания владельца можно совершать перехват и подмену трафика, а то и напрямую атаковать устройство (в случае с Android можно воспользоваться специальными модулями Metasploit Framework). Также, в случае Android-устройств велика вероятность заражения той или иной вредоносной программой. Это обусловлено не только тем, что таких устройств используется больше всего, но и внушающим опасения ростом числа уязвимостей в устройствах под управлением данной ОС. В случае рутованных/джейлбрекнутых устройств риск утери или кражи данных возрастает еще выше: это и установка приложения из неизвестных источников, неограниченные и слабоконтролируемые права — большинство пользователей не читает предупреждений и подтверждает практически любые запросы от приложений. Облачные хранилища Облачные технологии предлагают больше возможностей и удобства для доступа к корпоративным данным, но и одновременно с этим увеличивают риски утечки или кражи данных. Это обусловлено нерегулируемым доступом к сети, довольно слабой парольной политикой большинства пользователей, слабой подготовке к угрозам целевых атак, с применением социотехнических векторов. Более того, нативные облачные хранилища (gmail, icloud, onedrive и т.д.) личных мобильных устройств находятся вне сферы контроля ИТ/ИБ подразделений и с высокой долей вероятности могут быть скомпрометированы злоумышленниками. Решения по обеспечению безопасности Если нет возможности отказаться от использования личных мобильных устройств — необходимо включить эти устройства в политики безопасности компании: установить зоны ответственности за резервное копирование и техническое обслуживание устройств; использование VPN-соединений при использовании в публичных точках доступа; контроль установленных приложений, черные и белые списки; обеспечения контроля хранимых на устройстве критичных данных или сведений для доступа к ним; уведомление технического персонала о любых подозрительных случаях или инцидентах; регламентные проверки устройства; обеспечение осведомленности пользователей о текущих мобильных угрозах. Внедрение MDM Если носимое устройство принадлежит компании, его проще и эффективнее защищать используя общепринятые мировые практики защиты BYOD. В корпоративных мобильных устройствах доля смешивания личных и профессиональных данных мала, поэтому некоторые ограничения свободы действий пользователя оправданы и целесообразны. В этом случае баланс смещен в сторону защиты данных, нежели удобства использования. Для этих целей можно использовать как специализированные устройства (Blackberry), так и специальные превентивные меры по предотвращению утечек. Необходим план безопасности устройств, включающий в себя следующие шаги: Определить угрозы и элементы риска использования той или иной информации на носимом устройстве. Необходимо составить политику доступа к корпоративным данным вне периметра компании. Обеспечить дополнительные меры безопасности облачного хранения. Установить контроль приложений. Обеспечение надлежащей парольной политики. Установка и поддержание в актуализированным состоянии средств защиты. Реализовать меры по шифрованию данных. Установить возможность удаленного управления устройством. Обеспечить меры уничтожения информации в случае утери или кражи устройства. Меры по утилизации устройства или возврата в случае увольнения сотрудника. Внедрение административных мер нарушения политики BYOD. Все вышеперечисленные меры можно применять с использованием систем класса Mobile Device Management (MDM), которые позволяют удаленно (централизованно) управлять множеством мобильных устройств, будь то устройства, предоставленные сотрудникам компанией или собственные устройства сотрудников. Управление мобильными устройствами обычно включает в себя такие функции, как удаленное обновление политик безопасности (без подключения к корпоративной сети), распространение приложений и данных, а также управление конфигурацией для обеспечения всех устройств необходимыми ресурсами. MDM-решения — одно из средств реализации политики ИБ организации и, как любой другой инструмент, эффективны при условии использования по назначению и правильной настройки. Однако и это решение не является панацеей от всех угроз — возможность удаленного управления устройством только при наличии сети делает устройства уязвимыми к физическим атакам (при отключенной сети передачи данных или копированию памяти) — клонированию данных для анализа в специализированных средах или извлечения и возможной дешифровке данных, поэтому только соблюдение контроля доступа и состава данных на носимом устройстве может снизить риски утечки или кражи критичных данных или доступа к ним.\n","id":59}
{"Host":"https://habr.com","Path":"/en/post/410195/?mobile=no","Text":"ЦНИИмаш запатентовал лазерную систему опознавания «свой-чужой» для спутников / Habr                        \n\n15-02-2018\n3D-карта всех спутников и космического мусора на орбите Земли Система радиолокационного опознавания («Свой-чужой») — аппаратно-программный технический комплекс для автоматического отличения своих войск и вооружений от войск противника. Система традиционно используется в военном деле, начиная с племенной окраски первобытных племён, индейцев, использования татуировок. В современной армии каждое войско имеет форму установленного образца для различия своих и чужих солдат, а также систему паролей и отзывов. Но как осуществлять распознавание «свой-чужой» в космосе? Например, если радиолокационное опознавание по какой-то причине не работает. Или нужно определить принадлежность спутника по его обломкам? Интересное решение проблемы предлагают специалисты ЦНИИмаша — головного научного института госкорпорации «Роскосмос». Они предлагают при изготовлении спутников добавлять в материал корпуса или наносить на поверхность специальное светоотражающее покрытие, пишут «Известия». В химическом составе вещества будут закодированы страна-владелец, задачи космического аппарата, название спутника. Покрытие нужно наносить не только на корпус аппарата, но и на солнечные батареи. Считывание информации осуществляется лазерным лучом с наземной станции. Соответствующее оборудование можно изготавливать в Алтайском оптико-лазерном центре имени Титова, говорят авторы изобретения. По мнению специалистов, такая система позволит оперативно идентифицировать аппараты-нарушители на орбите и доказать виновность страны-хозяйки. В случае аварии какого-то спутника можно будет идентифицировать его даже по обломкам. «Предлагаемое изобретение может служить основой для разработки проекта международного соглашения об опознавательной маркировке космических средств. Это позволит повысить уровень доверия и транспарентности в космической деятельности, — рассказали «Известиям» в ЦНИИмаше. — По мере созревания политических и экономических условий это может стать перспективной инициативой России и средством повышения надёжности космической техники». ЦНИИмаш уже запатентовал новую систему опознавания. По статистике Бюро по орбитальному мусору NASA, на околоземной орбите находится более 21 000 фрагментов космического мусора размером более 10 см. Количество фрагментов от 1 до 10 см оценивается примерно в 500 000, число объектов менее 1 см превышает 100 000 000 штук. Крупные объекты рутинно отслеживает Сеть космического наблюдения США (и предупреждает МКС о необходимости манёвра в случае пересечения траекторий с крупным объектом). Объекты до 3 мм можно зарегистрировать наземными радарами. Основные источники космического мусора — взрывы спутников и столкновения. До 2007 года основным источником были обломки от взрывов верхних ступеней старых ракет, оставшихся на орбите с остатками топлива, в том числе жидкого топлива под высоким давлением. Но затем ситуация изменилась. Умышленный подрыв китайцами своего метеорологического спутника Fengyun-1C в 2007 году, а также случайное столкновение российского и американского спутников связи в 2009 году резко ухудшили ситуацию. Треть всего современного мусора на орбите появилась в результате этих двух событий. Большинство рукотворных фрагментов находится на высоте до 2000 км, а максимальная концентрация наблюдается на высоте 750−800 км. Здесь их средняя скорость составляет около 7−8 км/с, но относительно других космических аппаратов средняя скорость составляет 10 км/с. Ключевые элементы МКС способны нормально выдержать столкновение с обломками космического мусора размером до 1 см на такой скорости. И можно предположить, что такие столкновения регулярно происходят. По крайней мере, обшивка станции «Мир» после 15-ти лет на орбите была испещрена следами от столкновений с маленькими фрагментами космического мусора и метеороидами. Наибольший урон понесли солнечные панели, но это не повлияло на миссию и функциональность станции, считает NASA. Станция «Мир», снятая экипажем корабля «Атлантис», 19 сентября 1996 года Чем выше орбита — тем дольше космический мусор останется в космосе. Например, на высоте 800 км он может находиться десятилетиями, а на высоте более 1000 км — столетиями. Для примера, у автомобиля Tesla Roadster на ракете Falcon Heavy вероятность упасть обратно на Землю составляет всего 6% в ближайший миллион лет. После запуска глобальной спутниковой группировки SpaceX и других компаний количество спутников и космического мусора увеличится на порядок, так что распознавать их только оптическими средствами с помощью телескопов станет сложнее. Вот здесь и пригодится лазерная система идентификации по химическому составу корпуса. Впрочем, научный руководитель Института космической политики Иван Моисеев высказал мнение, что запатентованный способ слишком сложен и вряд ли будет реализован на практике. По его мнению, для борьбы с космическим мусором нужно внедрять другие системы: например, обязательно оснащать все спутники системами сведения с орбиты.\n","id":60}
{"Host":"https://habr.com","Path":"/en/post/162423/?mobile=no","Text":"Виртуальный сервер с Ubuntu 11.04, Software RAID и его восстановление / Habr                       \n\n12-12-2012\nПривет Хабр. Хотел бы описать решение проблемы с Software RAID на Ubuntu Server 11.04 с которой я столкнулся неправильно перезагрузив сервер. Пару дней назад, работал я, писал код на php, сервер офисный сильно не грузил. Вообще у нас принято как серверный так и клиентский код писать на собственных машинах и версировать при помощи git, разве что база данных mysql иногда бывает общей, с того самого сервера. А если надо, то git push в помощь. Для многих разработок на сервере настроены vhosts обновляемые из git и доступные из просторов интернетов. Перезагрузив какую-то страницу с сервера, я почуял, что что-то не так, часть страницы загрузилась, а дальше все… Ситуацию усугубляло то, что коллега подошел и сказал что у него перестал работать доступ на сервер по smb, а у меня еще и отвалилось соединение по ssh. Стало ясно, что повис не только apache. «Не проблема», подумал я, «ведь у нас виртуализация, перезагружу vm и дело в шляпе». Да, да именно так. Стоит себе физический сервер, на Ubuntu Server 11.04, внутри которого под qemu запущен еще один Ubuntu Server 11.04, на котором настроены все нужные сервисы. Почему так? Решение было принято более опытным коллегой, который к сожалению уволился, а я не особо силен в системном администрировании. Опущу небольшую часть истории про смену пароля, которого я конечно же не знал :) Подключился я к серверу физическому, и там: virsh: list Ок, сервер running, id 1. К терминалу не цепляется (с учетом небольшой паники, я забыл про vnc, но на тот момент он мне не очень бы помог, хотя и был настроен для гостевой OS). virsh: reboot 1 error: this function is not supported by the hypervisor: virDomainReboot Не ок, но что поделаешь: virsh: destroy 1 start 1 Ждемс. К терминалу все еще не цепляется. ssh соединение не работает. Вобщем сервер не стартует. Многократные попытки destroy/start ни к чему не привели. Отчаявшись, я решил посмотреть на конфигурацию гостевой OS. А там: <graphics type='vnc' port='5901' autoport='no' listen='0.0.0.0'/> Обрадовался и полез смотреть на это безобразие по vnc. Все дальнейшее выполняется внутри гостевой OS. А там: The disk drive for /some/mounted/folder is not ready yet or not present Continue to wait; or Press S to skip mounting or M for manual recovery После первого нажатия S, я понял что все похоже очень плохо. После импульсивного нажамания S 100500 раз удержания S в течение секунды, OS продолжил загрузку, но mysql, apache и многие другие демоны не запустились, так как папки вроде /var/lib/mysql оказались непримонтированы. Преодолев логин, я попытался понять, куда все пропало (бекапы у нас есть, вот только очень уж не хотелось весь остаток недели заниматься восстановлением). Присутствие в /etc/fstab и в /dev/ странных записей вроде /dev/md/1_0 меня насторожило. Гугл подсказал, что это части Software RAID массива. Внутри Ubuntu, Ubuntu, внутри Ubuntu Software RAID… Вот. Частей оказалось 5. Гугл подсказал, что fsck и mdadm мне в помощь: fsck –nvf /dev/md/1_0 … fsck –nvf /dev/md/5_0 Просим fsck ничего не менять, выводить много мусора интересной информации в консоль и проверить все, даже если файловая система не помечена как поврежденная. Из 5, на 3 оказались ошибки/повреждения ФС. Дальше я рискнул, и попросил fsck все исправить: fsck –vf /dev/md/1_0 … fsck –vf /dev/md/1_0 В тоже время mdadm для всех устройств говорил: mdadm --detail /dev/md/1_0 … Raid Level : raid1 … /etc/fstab говорил: … /dev/md/1_0 /var/www ext3 defaults,noatime 1 2 … Исправления прошли на ура. Осталось понять как заставить все это собраться опять. Перезагрузка не помогла. Сам по себе массив не собрался. Оказалось что названия и маппинг устройств вида /dev/md[xxx] в /dev/md/[yyy] менялись при каждой перезагрузке (в /dev/md/ создаются символические ссылки на /dev/md[xxx]). Поэтому устройства прописанные в /etc/mdadm.conf системой не находились и автоматически не монтировались. На этом этапе я перестал задаваться вопросом «Как же это раньше работало?», и решительно стал искать какой-то способ связать прописанное в данном файле с тем что я видел в /dev/md/. И таки нашел: mdadm --detail /dev/md/123_0 … UUID : 4e9f1a60:4492:11e2:a25f:0800200c9a66 … less /etc/mdadm.conf ARRAY /dev/md/1_0 level=raid1 metadata=0.90 num-devices=2 devices=/dev/sda5,/dev/sdb5 UUID=4e9f1a60:4492:11e2:a25f:0800200c9a66 Связь найдена (UUID), дело за малым. Назначить найденным в /etc/fstab старым mount point’ам, новые устройства из списка /dev/md[xxx], что и было сделано: mount –a #Монтирует все описанное в /etc/fstab Перезапустив mysql, apache и прочее, увидев, что содержимое /var/www вернулось и вообще все цветет и пляшет, я таки успокоился и пошел пить кофе. Как оказалось, сервер не дожил 4 дней до года аптайма. Однако нельзя сказать, что проблема решена на 100%. Непонятно поведения при перезагрузке, но теперь уж есть список манипуляций которые надо произвести чтобы все опять заработало. Вопрос к сообществу, а сталкивался ли кто-нибудь с подобным? На этом вопросе я закончу свой рассказ. Советы, вопросы и комментарии приветствуются. PS: После вышеописанных работ, у меня возникло желание избавиться от такой кхм, странной конфигурации дисков, но виртуализацию оставить. Заодно, будет повод развить навыки настройки сервера и выпросить у начальства апгрейд памяти для сервера, к которому по аппаратной части нареканий нет (Dell PowerEdge tower).\n","id":61}
{"Host":"https://habr.com","Path":"/en/post/429894/?amp","Text":"Использование камеры Fish eye на Raspberry Pi 3 с ROS — часть 2 / Habr             \n\n31-12-2018\nДобрый день уважаемые читатели Хабра! Это вторая часть рассказа об использовании fish eye камеры на Raspberry Pi 3. Первую часть можно найти здесь. В этой статье я расскажу о калибровке fish eye камеры и применении камеры в детекции объектов с помощью пакета find_object_2d. Кому интересно, прошу под кат. Калибровка камеры fish eye с использованием camera_calibration Здесь я описываю процедуру калибровки на основе официального мануала на портале ros.org. Для выполнения калибровки нам нужен пакет camera-calibration. Мы можем его установить с помощью apt: sudo apt-get install ros-kinetic-camera-calibration Нам нужен будет шаблон checkerboard. Скачаем шаблон с официального мануала на ros.org и распечатаем. Для удобства я его наклеил на фанерную доску: Давайте запустим программу калибровки: rosrun camera_calibration cameracalibrator.py --size 8x6 --square 0.108 image:=/usb_cam/image_raw camera:=/usb_cam Мы получим картинку: Переместим немного шаблон и подождем пока шаблон не будет выделен в кадре (на шаблоне не появятся цветные линии с точками). Переместим шаблон еще немного в сторону. Чтобы успешно выполнить калибровку нам нужно выполнить серию перемещений шаблона перед камерой из стороны в сторону так чтобы шаблон попал во все угловые позиции в поле зрения камеры (слева, справа, сверху и снизу). Справа от окна изображения с камеры в окне программы находится панель регистрации с тремя прогрессбарами: X фиксирует перемещение шаблона в направлении лево/право (горизонтальное) в поле зрения камеры Y фиксирует перемещение шаблона в направлении верх/низ (горизонтальное) в поле зрения камеры Size фиксирует приближение / удаление шаблона от камеры и наклон по отношению к камере. Skew фиксирует наклон шаблона влево, вправо, вверх и вниз (скос). Таким образом для успешной калибровки важно чтобы шаблон оказался в разных углах кадра, занимал весь кадр а также был наклонен влево, вправо, вверх и вниз. Калибровка fish eye камеры на Raspberry Pi может занять довольно много времени, поэтому запаситесь терпением. У меня процедура калибровки заняла 20 минут. Когда калибровка будет завершена, кнопка Calibrate должна активироваться (подсветится цветом): Также мы можем увидеть результаты калибровки в терминале: Если вы удовлетворены результатом, нажмите кнопку COMMIT. Окно программы закроется и вы увидите в терминале сообщение «writing calibration data to ...». Проверим, что был создан указанный файл: ll ~/.ros/camera_info/head_camera.yaml -rw-rw-r-- 1 vladimir vladimir 592 Apr 14 14:02 /home/vladimir/.ros/camera_info/head_camera.yaml Калибровка завершена. Теперь полученные данные калибровки можно использовать в алгоритмах визуальной локализации и SLAM в ROS. Детекция объектов с помощью find_object_2d Установить пакет достаточно просто. Устанавливаем его из репозитория apt в Ubuntu 16.04 для ROS Kinetic: sudo apt-get install ros-kinetic-find-object-2d source /opt/ros/kinetic/setup.bash Запустим ROS master и rqt_image_view: roscore roslaunch usb_cam usb_cam-test.launch С помощью следующей команды запустим узел детектора: rosrun find_object_2d find_object_2d image:=/usb_cam/image_raw Откроется окно программы детекции: Здесь мы увидим поток с камеры и результат детекции характерных признаков на объектах. Для начала проведем тренировку детектора на наших объектах. Поместим первый объект перед камерой: Нажмем правой кнопкой на левой панели Objects в окне и у нас откроется опция Add objects from scene. Выберем эту опцию и откроется окно добавления объекта: Выбрав наилучшую позицию для объекта, нажмем кнопку Take Picture чтобы сделать снимок объекта: Нам потребуется выделить объект на снимке. Используем курсор мышки для выделения объекта: Нажмем на кнопку Next чтобы вырезать объект на снимке и перейдем к следующему шагу. После обрезки изображения мы получим полное число обнаруженных на объекте характерных признаков. Остается только нажать кнопку End чтобы добавить объект в базу обученных объектов детектора. Здесь мы видим последний шаг процедуры добавления объекта: В итоге мы обучили детектор на одном объекте. Теперь можно попробовать детекцию объекта в сцене: Сделаем вывод позиции обнаруженного объекта в терминал: rosrun find_object_2d print_objects_detected Вывод будет таким: Object 1 detected, Qt corners at (259.387238,103.530960) (448.684052,79.495495) (282.419050,240.049667) (458.438938,199.656717) --- Object 1 detected, Qt corners at (255.340408,104.615120) (451.348079,75.302353) (284.672425,230.382223) (452.696585,197.625600) --- Object 1 detected, Qt corners at (253.440521,102.973320) (447.226440,76.793541) (278.530854,238.918013) (454.377219,197.526599) --- Выведем список топиков: rostopic list В списке появилось два новых топика: /objects и /objectsStamped. Выведем информацию об обнаруженных объектах: rostopic echo /objects layout: dim: [] data_offset: 0 data: [1.0, 266.0, 177.0, 0.7527905702590942, 0.060980819165706635, 0.00022385441116057336, 0.3012462854385376, 0.8929792046546936, 0.0008534671505913138, 334.9065856933594, 182.55294799804688, 1.0] --- Здесь второе и третье значения (266.0, 177.0) представляют ширину и высоту объекта. Все остальные значения в поле data представляют 3x3 матрицу гомографии (используется для вычисления позиции и ориентации объекта, а также значения масштаба и сдвига). Как показывают наблюдения, find_object_2d плохо справлется с детекцией объектов со слабой тектурой или без текстуры (textureless). Кроме того детектор неэффективен при детекции объекта под большим углом к объекту (если мы наблюдаем объект сбоку), или на большом расстоянии до объекта. После завершения работы с детектором, find_object_2d нам предложит сохранить добавленные объекты на диск. На этом пока все. Всем удачи и до новых встреч!\n","id":62}
{"Host":"https://habr.com","Path":"/en/post/451012/?mobile=no","Text":"Случайные перестановки и случайные разбиения / Habr                        \n\n08-05-2019\nЯ много лет читаю курсы по комбинаторике и графам для студентов-математиков и computer scientists (как это по-русски, компьютерных научников?), раньше в Академическом университете, а теперь в СПбГУ. Программа у нас построена так, что эти темы проходят как часть «теоретической информатики» (другие темы в ней — алгоритмы, сложность, языки и грамматики). Не могу сказать, насколько это оправдано метафизически или исторически: всё же комбинаторные объекты (графы, системы множеств, перестановки, клетчатые фигуры и др.) начали изучали задолго до появления компьютеров, и сейчас последние хотя и важная, но далеко не единственная причина интереса к ним. Но так посмотреть на самых спецов по комбинаторике и по theoretical computer science — это удивительно часто одни и те же люди: Ловас, Алон, Семереди, Разборов и далее. Наверно, есть на то свои причины. На моих уроках часто очень нетривиальные решения сложных задач предлагают чемпионы олимпиадного программирования (их перечислять не буду, кому любопытно посмотрите топ codeforces.) В общем, думаю, что некоторые вещи из комбинаторики могут быть интересны сообществу. Говорите, если что так или не так. Пусть вам надо построить случайную перестановку чисел от 1 до , чтобы все перестановки были равновероятны. Это можно сделать многими способами: например, сначала выбрать случайно первый элемент, потом из оставшихся второй и так далее. А можно поступить иначе: выбрать случайно точки в отрезке , и посмотреть как они упорядочены. Заменяя наименьшее из чисел на 1, второе на 2 и так далее получаем случайную перестановку. Легко видеть, что все перестановок равновероятны. Можно и не в отрезке выбирать точки, а, например, среди чисел от 1 до . Тут возможны совпадения (для отрезка тоже возможны, но с нулевой вероятностью, так что нас они не волнуют) — с ними можно бороться по-разному, например, дополнительно переупорядочивая совпадающие числа. Или взять N побольше, чтобы вероятность совпадения была маленькой (хорошо известен случай, когда , а есть число учеников в вашем классе, и речь о совпадении двух дней рождения.) Вариация этого метода: отметить случайно точек в единичном квадрате и посмотреть, как упорядочены их ординаты относительно абсцисс. Другая вариация: отметить в отрезке точку и посмотреть, как упорядочены длины отрезочков, на которые он разбился. Ключевой момент в этих подходах — независимость испытаний, по результатам которых строится случайная перестановка. Андрей Николаевич Колмогоров говорил, что теория вероятностей это теория меры плюс независимость — и это подтвердит всякий кто имел дело с вероятностью. Покажу, как это помогает, на примере формулы крюков для деревьев: Пусть — подвешенное за корень дерево с вершинами, растущее вниз как на картинке. Наша цель — вычислить число нумераций вершин дерева числами от 1 до таких, что для каждого ребра число в его верхней вершине больше чем в нижней. Одна из таких нумераций приведена на средней картинке. Ответ формулируется с помощью размеров крюков . Крюком вершины назовём поддерево, растущее из этой вершины, а его размером просто число вершин в нём. Длины крюков написаны на правой картинке рядом с соответствующими вершинами. Так вот, число нумераций равно делить на произведение размеров крюков, так для нашего примера Можно доказывать ту формулу по-разному, например индукцией по числу вершин, но наш взгляд на случайные перестановки позволяет провести доказательство вообще без вычислений. Оно лучше не только элегантностью, но и тем что хорошо обобщается на более тонкие вопросы о нумерациях с предписанными неравенствами, но об этом не сейчас. Так вот, возьмём n различных вещественных чисел и расставим их случайно в вершины дерева, в каждую по одному числу, все перестановок равновероятны. Какова вероятность того, что для каждого ребра число в верхней вершине ребра больше числа в его нижней вершине? Ответ: , и от набора чисел он не зависит. А раз не зависит, то давайте считать числа тоже выбранными случайно — для определённости, в отрезке . Вместо того, чтобы сначала случайно выбирать числа, а потом случайно расставлять их в вершины дерева, мы можем просто случайно и независимо выбрать число в каждой вершине: их перестановка окажется случайной автоматически. Таким образом, это вероятность того, что для случайных независимых чисел , выбранных по одному для каждой вершины дерева , выполняются все неравенства вида для всех рёбер , — верхняя вершина ребра, а — нижняя. Сформулируем эти условия в равносильном виде, но немного иначе: для каждой вершины должно происходить такое событие — обозначу его : число — максимальное среди всех чисел в вершинах поддерева-крюка . Заметим, что это вероятность события . В самом деле, в крюке имеется вершин и максимальное по крюку число сопоставлено каждой из них с равной вероятностью . Таким образом, формула крюков может быть сформулировано так: вероятность того, что происходят сразу все события , равна произведению вероятностей этих событий. Причин на то может быть много разных, но тут работает первая, приходящая в голову: эти события независимы. Чтобы это понять, посмотрим, например, на событие (соответствующее корню). Оно состоит в том, что число в корне больше всех остальных чисел в вершинах, а прочие события касаются сравнений между собой чисел, написанных не в корне. То есть касается числа и множества чисел в остальных вершинах, а все остальные события — порядка чисел в вершинах, отличных от корня. Как мы уже обсуждали, «порядок» и «множество» независимы, поэтому событие не зависит от прочих. Спускаясь далее по дереву, получим, что все эти события независимы, откуда и следует требуемое. Обычно формулой крюков называют формулу для нумераций не вершин в дереве, а клеток в диаграмме Юнга , возрастающих в направлениях координатных осей, и крюки там больше похоже на крюки чем для деревьев. Но эта формула доказывается сложнее и заслуживает отдельного поста. Раз уж пришлось к слову, не могу не рассказать о модели случайной диаграммы Юнга. Диаграмма Юнга это вот такая фигура из единичных квадратиков, длины её строк возрастают снизу вверх, а длины столбцов слева направо. Количество диаграмм Юнга площади обозначается , эта важная функция ведёт себя интересно и необычно: например, она растёт быстрее любого многочлена, но медленнее любой экспоненты. Поэтому, в частности, генерировать случайную диаграмму Юнга (если мы хотим, чтобы все диаграммы площади имели равную вероятность ) дело нетривиальное. Например, если добавлять клеточки по одной, выбирая место добавления случайно, разные диаграммы будут иметь разные вероятности (так, вероятность однострочечной диаграммы получается равной .) Выходит занимательная мера на диаграммах, но не равномерная. Равномерную можно получить так. Возьмём число , для наших целей лучше всего годятся числа в районе . Для каждого рассмотрим геометрическое распределение на целых неотрицательных числах со средним (то есть вероятность числа равна ). Выберем согласно нему случайную величину (есть много способов как это организовать). При больших скорее всего 0. Посмотрим на диаграмму Юнга, в которой строк имеют длину при каждом . Я называю это методом кораблей, потому что общая площадь иногда равна . Если не равна, то повторяем эксперимент. На самом деле равна она достаточно часто, если умно выбрать . Предлагаю читателю самостоятельно доказать, что все диаграммы данной площади равновероятны и оценить число шагов.\n","id":63}
{"Host":"https://habr.com","Path":"/en/company/sberbank/blog/417791/?mobile=no","Text":"Как мы съездили на New York Developer Week / Habr            \n\n22-07-2018\nОколо месяца назад в США прошла конференция для разработчиков New York Developer Week (NYDW), где мы выступали с докладом «High reliable, high loaded architecture of front-office system in bank» (Построение отказоустойчивых, высоконагруженных фронтальных систем банка). В посте делимся нашими заметками о конференции. А следующий пост посвятим докладу, который там представили. ТРИ ПРИЧИНЫ ВЫСТУПИТЬ НА NYDW Мы уже несколько раз на самых разных мероприятиях и конференциях в России рассказывали об особенностях архитектуры Единой Фронтальной Системы Сбербанка и в какой-то момент поняли, что нам будет полезна обратная связь от коллег из других стран. NYDW идеально подошла для этого. Во-первых, в ней принимают участие разработчики из компаний-лидеров отрасли. Например, после нас с докладом про в OpenTracing и лучшие подходы к логированию для микросервисов выступали инженеры из Uber, а в соседних аудиториях можно было послушать специалистов из Microsoft, Google или IBM. Во-вторых, хотя New York Development Week достаточно большая отраслевая конференция для разработчиков, но ее все же нельзя отнести к числу крупнейших. В этом году по нашим ощущениям в мероприятии приняло участие около 1500 инженеров и разработчиков, что, например, меньше, чем на топовых российских конференция РИТ++ или HighLoad. В-третьих, это был просто прекрасный шанс подтянуть свой английский и получить новые полезные контакты, чем мы и воспользовались. ТАКСИ И КВАДРОКОПТЕР НА ТРОИХ Как и у многих, наше знакомство с «Большим Яблоком» началось с легендарных желтых такси. И сразу же нас ждало два не слишком приятных сюрприза. Как известно, цены на услуги таксистов в США никак нельзя назвать низкими – дорога в 25 км от аэропорта до отеля обошлась в 55 долларов. Второй сюрприз ждал нас буквально на заднем сиденье. По правилам, езда на переднем кресле с водителем такси в США запрещена. Пассажир всегда размещается только сзади. При этом, для безопасности от водителя его отделяет прозрачная пластиковая стенка, из-за которой места сзади становится еще меньше. В итоге, дорога из аэропорта в город больше напоминала поездку в «загоне» полицейской машины. Июньский Нью-Йорк порадовал хорошей погодой: температура не опускалась ниже 25 градусов, а ветер с моря постоянно поддерживал ощущение свежести. Правда, искупаться не удалось – вода в океане прогревается слабее и не поднялась выше 20 градусов. В городе, как в кино, много небольших уютных кафе, а также разъезжают целые фургоны с мороженным. Хотя мы прилетели к самому началу конференции, день открытия мы посещать не стали. Дело в том, что докладов в тот день не было, а участвующие компании активно занимались хантингом посетителей: практически любой желающий мог прийти на площадку мероприятия в первый день и попытаться трудоустроиться в Google или любую крупную IT-компанию. Для нас этот вопрос был абсолютно не актуальным и вместо этого мы потратили день на знакомство с городом. При этом, чтобы собрать контакты перспективных программистов, партнеры мероприятия зачастую идут на небольшие хитрости. Например, по инициативе одной из компаний была организована бесплатная лотерея, призом в которой был новый квадракоптер. Чтобы принять участие в розыгрыше, достаточно было оставить свою визитку с контактами. 25 МИНУТ НА СЦЕНЕ Основные активности на конференции пришлись на второй и третий дни. Специально для регистрации мы приехали заранее к 8.45 утра, но первыми не оказались. К этому времени на входе уже была очередь из нескольких десятков участников. При этом, около половины программистов явно были не из США. Например, в перерыве между докладами мы познакомились с коллегами из Китая и Венесуэлы; несколько раз в коридорах была слышна русская речь. В этот раз конференция проходила в Бруклинском Экспо Центре, который похож на ангар, который представляет собой огромную стеклянную коробку, оформленную изнутри в стиле лофт. Помимо огромной зоны для регистрации, в здании было несколько залов, крупнейших из которых вмещает около 500 слушателей. Остальные залы меньше – примерно на 100 человек. Всего на конференции было представлено около 30 докладов. Основные темы – машинное обучение, блокчейн, искусственный интелект, web-разработка. А вот о мобильной разработке в этом году почти не рассказывали. Эта тема лишь немного поднималась в докладе Cloudflare про оптимизацию взаимодействия между клиентскими и серверными приложениями Среди выступающих в первый день запомнились выступление ведущего инженера Google Вадима Марковцева, который представил исследование открытого исходного кода с помощью алгоритмов машинного обучений. Так же удивление вызвал большой ажиотаж вызвал мастер класс использования GitKraken от спонсора конференции, компании Axosoft. Еще запомнилось выступление технического директора компании Kong Inc, с докладом «Blowing up the Monolith» посвященным практикам перехода к микро-сервисной архитектуре. Однако он отметился не столько ярким докладом, сколько примером того, как делать не следует. Хотя он неплохо говорил на английском, почему-то он решил выступать без микрофона и из-за этого его почти не было слышно. Как результат – уже через несколько минут слушатели начали покидать зал. Наше выступление было запланировано на второй день. Для доклада нам было выделено 25 минут. Выступление прошло без проблем. Еще до приезда в Нью-Йорк мы несколько раз репетировали и выступили как хорошие актеры, которые отлично дополняют друг друга. Аудитория приняла наше выступление с интересом. Зал на 100 человек был заполнен примерно наполовину, а после выступления последовало несколько вопросов от слушателей. Например, спросили, планируем ли мы перейти на контейнеризацию, спрашивали про наше видение о возможности автоматизации части процессов. Несколько слушателей подошли позднее чтобы обменяться контактами и с просьбой прислать текст доклада. В целом, NYDW оправдала наши ожидания. У нас впервые появился навык выступления на зарубежном мероприятии на иностранном языке. Вооружившись опытом выступления на такой площадке, можно готовиться к презентации на других еще более крупных конференциях разработчиков практически в любой точке мира. Авторы репортажа и компаньоны в поездке: ведущий руководитель направления по развитию IT-систем Михаил Пересыпкин главный архитектор Единой Фронтальной Системы Роман Шеховцов начальник отдела фронтальных систем международного блока Юрий Спорынин\n","id":64}
{"Host":"https://habr.com","Path":"/ru/company/garmin/blog/117089/?mobile=no","Text":"Готов ли коммуникатор заменить навигатор? / Хабр                                                  \n\n08-04-2011\nС каждым годом коммуникаторы на рынке отвоевывают новые позиции, вытесняя устаревающие устройства. Редко в быту можно увидеть калькуляторы, будильники, телефонные книги. Не пришла ли очередь и GPS-навигаторов? Об этом мы поразмышляем в этой статье. Почти все коммуникаторы, независимо от платформы, создаются с встроенными GPS-приемниками. Любой обладатель коммуникатора может получить координаты своего положения, запустив соответствующее приложение. Зачем тогда нужны навигаторы? И в чем функциональная разница между коммуникатором и навигатором? Прием сигнала и расчет положения Первые отличия можно выявить на уровне приема сигнала. Производители телефонов-коммуникаторов очень ограничены в пространстве, поэтому чаще всего используют миниатюрные антенны, расположенные в верхнем торце аппарата. Производители GPS-навигаторов имеют больше возможностей для крепления антенны, зачастую они выносят ее за корпус, улучшая тем самым прием GPS сигнала. Условия приема сигнала скажутся на чистоте и интенсивности сигнала, а значит, на качестве данных. Можно много говорить о «математике», позволяющей улучшить решение, но по факту позиционирование GPS-навигаторами точнее и стабильнее, нежели гибридными аппаратами. Навигаторы, как отдельные устройства, пассивны с точки зрения электромагнитного излучения, коммуникаторы, как гибридные, – нет. Нельзя сказать, что навигация невозможна с телефоном, находящимся в режиме разговора или обмена данными, но пространственное решение будет хуже, чем, если бы использовалось устройство без возможности передачи данных. Эргономичность и эффективность Немаловажным отличием являются размеры экранов. Коммуникаторы в этом плане довольно ограничены – с одной стороны, экран должен быть большим, а с другой – прибор должен умещаться в руке. Таким образом, навигация происходит на экране не более 4.5 дюймов. Для навигаторов размер экрана определяется назначением – туристу достаточно небольшого экрана, а спортсмену он может быть не нужен – существуют же, например, шагомеры с GPS — для штурмана корабля, где необходима обзорность, нужны специальные картплоттеры – большие экраны для рисовки электронных морских карт. Аккумулятор коммуникатора редко рассчитан более чем на 3-4 часа в режиме навигации. Навигаторы в этом плане менее прихотливы – большинство из них работает от батареек АА, либо от аккумуляторов повышенной емкости. Исключение составляют автонавигаторы и устройства, работающие на прочих видах транспорта, рассчитанные на частые зарядки от «прикуревателя», однако с лихвой компенсирующие этот недостаток размерами и яркостью экранов. IPX7 -аббревиатура значит, что прибор можно кратковременно погружать в воду. Для навигаторов это распространенный стандарт, гарантирующий работу устройства в самых сложных условиях. Даже в городе, наблюдая как капли дождя падают на экран коммуникатора, первой мыслью у меня было убрать его куда-нибудь, где посуше, что уж говорить о том, чтобы выходить на маршрут под проливным дождем. Среди навигаторов, есть даже модели способные плавать (к примеру, Garmin GPSMAP 78s). Карты, данные и интернет GPRS – а такое ли это благо, как принято считать? Действительно – что может быть удобнее? – Вы в незнакомом месте, включаете телефон, а он подгружает из сети карты, описания, фотографии. Это удобно, но есть и обратное обстоятельство – если вдруг у вас нет возможности подключиться к интернету, вы рискуете не только остаться без карт, но и без навигации вовсе. Многие коммуникаторы, компенсируя конструктивные недостатки антенн, закачивают эфемериды (данные о том, где находятся спутники на настоящий момент) из сети, а если сети нет, то позиционирование занимает настолько долгое время, какого хватает для того, чтобы найти киоск с газетами и купить карту города. С навигаторами поездки лучше планировать заранее. Заранее определить цель поездки, купить карты, загрузить их. В этом случае, пользователь может быть уверен на 100%, что в месте назначения он получит пространственную информацию именно с той подробностью, которая ему была предложена. Программное обеспечение и ресурсы прибора Управление пространственной информацией это очень ресурсозатратный процесс, поэтому при навигации в коммуникаторах часто встречаются такие явления, как «подвисания карты». Это является следствием того, что процессор коммуникатора не справляется со всеми задачами, которые запущены в данный момент. В этом плане процессоры навигаторов находятся в более выигрышном положении, так как на них возложена единственная роль – навигация, и все ресурсы устройства направлены на ее решение. Разрабатывая GPS-приемники для определенной целевой аудитории, производители могут учесть ее пожелания, такие как альтиметр, шагомер, датчик пульса, датчик расхода топлива. Коммуникаторы, рассчитанные на массового потребителя, не могут подобным похвастать. Выводы и заключения Выводы можно сделать следующие: Коммуникатор с функцией навигации полезен там, где – 1. Не нужно точного позиционирования более 10 м. 2. Где условия приема сигнала не нарушены окружающей обстановкой. 3. Где не нужен моментальный большой обзор карты. 4. Где не нужен миниатюрный датчик GPS. 5. Где всегда под рукой электросеть и есть возможность подзарядки. 6. Где сухо, тепло и чисто. 7. Где есть постоянное GPRS-покрытие. 8. Где пользователь не раздражается зависанием экрана. Можно сделать вывод, что коммуникаторы идеальны для летних прогулок по городу, экскурсий, поиску достопримечательностей. Коммуникаторы на сегодняшний день способны заменить в рюкзаке туриста карманный атлас, путеводитель и записную книжку. Навигаторы становятся нишевым продуктом, приобретая все больше модификаций для обеспечения задач спорта, туризма, автонавигации, судовождения, картографирования. Навигатор становится атрибутом профессионала, окончательно откидывая уже исчезающую приставку «бытовой», и очередь в музей до них еще подойдет не скоро.\n","id":65}
{"Host":"https://habr.com","Path":"/en/companies/sberbank/articles/650259/","Text":"Нейролингвистика, робототехника и видеоигры: сборник статей «Наука в Сбере-2021» / Habr                         \n\nТехнологические прорывы часто случаются в результате совместной работы научного сообщества и бизнеса. Поэтому помимо хороших айтишников, мы приглашаем работать в Сбер учёных и инженеров. Они публикуются в научных журналах и проводят доклады на международных конференциях, которые мы решили собрать в **сборник «** **Наука в Сбере-2021** **»** . Сегодня, в День российской науки, мы расскажем про этот сборник и сделаем мини-обзор некоторых статей.\n\n## Что такое сборник «Наука в Сбере-2021»? ##\nСборник даёт представление о передовых направлениях деятельности нашей компании. В нём мы собрали информацию об опубликованных работах исследователей Сбера, представили ведущих партнёров, рассказали о нашей Научной премии  и некоторых результатах прогнозных исследований (разделы «Видение-2035», «Сбер-Космос-Сбер», «Метавселенная», «Квантовая перспектива»).\nВ разделе со статьями даны краткие аннотации опубликованных в 2021 году статей и докладов. Они связаны с ИИ, современной медициной, нейролингвистикой (в частности, с популярным бенчмарком Russian SuperGLUE 1.1 и языковой моделью RuGPT3-XL ), робототехникой, нейроинтерфейсами и многими другими сегодняшними научными достижениями.\nВ завершающем разделе сборника под названием «Научная жизнь» собраны данные о статьях и докладах, которые будут опубликованы в первые месяцы 2022 года, чтобы дать читателю представление об актуальных направлениях работы исследователей Сбера.\n## Обзор некоторых статей из сборника ##\n### Выявление текстов, сгенерированных ИИ, на основе анализа топологических данных ###\n_Авторы: Л. Кушнарёва, Д. Чернявский, В. Михайлов, Е. Артёмова, С. Баранников, А. Бернштейн, И. Пионтковская, Д. Пионтковский, Е. Бурнаев._\nПоследние достижения в области нейронных систем обработки информации, в частности модели генерации текста (TGM), продемонстрировали впечатляющие возможности создания текстов, очень близких к человеческим по беглости, связности, грамматической и фактической правильности. Обширные TGM в стиле GPT (GPT — нейронная сеть, наделавшая шума в 2020 году как самая сложная, объёмная и многообещающая модель по работе с текстовыми данными) достигли выдающихся результатов в большом количестве задач нейролингвистики (NLP, Natural Language Processing), используя методы нулевого, одноразового и нескольких кадров и даже превосходя современные подходы к точной настройке. Однако такие модели могут быть использованы для создания фейковых новостей, обзоров товаров и даже контента экстремистского и оскорбительного содержания.\nБыло сделано много попыток разработать детекторы искусственного текста, начиная от классических методов машинного обучения и функций на основе подсчёта, заканчивая продвинутыми генеративными моделями глубокого машинного обучения (порождающие модели, generative model). Несмотря на выдающуюся производительность этих методов в различных областях, им всё ещё не хватает интерпретируемости и устойчивости по отношению к невидимым моделям. Искусственные тексты пока сложно отличить от написанных людьми.\nВ этой статье представлен новый метод обнаружения искусственного текста, основанный на анализе топологических данных (TDA), который, по мнению авторов, недостаточно изучен в области NLP. Авторы предлагают три новых типа интерпретируемых топологических функций на основе TDA. Результаты показывают, что TDA — это многообещающее направление в отношении задач NLP, особенно тех, которые включают поверхностную и структурную информацию.\n_Читайте полную версию научной статьи «Выявление текстов, сгенерированных_\n\n _ИИ, на основе анализа топологических данных», кликнув по_ _ссылке_  _или отсканировав QR-код:_\n\n### О методах компьютерной лингвистики в оценке систем искусственного интеллекта ###\n_Автор: Т. О. Шаврина_\nОсновным инструментом оценки уровня систем искусственного интеллекта выступают языковые тесты. Они являются самым доступным способом обучения ИИ и одновременно обладают высокой вариативностью, необходимой для формулировки интеллектуальных задач. В статье автор обозревает актуальную методологию обучения и тестирования интеллектуальных систем, рассматривает золотые стандарты текстовых задач (бенчмарки) в методологии General Language Understanding Evaluation (GLUE), а также обсуждает теоретические основы и конкретные реализации теста для ИИ-систем Russian SuperGLUE. Автор считает, что дальнейшее сближение практик машинного обучения и науки о языке способно заполнить лакуны в оценке ИИ-систем, в методах их эффективного обучения и в автоматическом анализе текста.\n_Найти статью «О методах компьютерной лингвистики в оценке систем искусственного интеллекта» можно, кликнув по_ _ссылке_  _или отсканировав QR-код:_\n\n### Одновременная локализация и построение карты на основе случайных признаков Фурье ###\n_Авторы: Е. Капушев, А. Кишкун, Г. Феррер, Е. Бурнаев._\nС прошлого века вероятностная оценка состояния была основной темой в мобильной робототехнике, как часть проблемы одновременной локализации и отображения (Simultaneous Localization and Mapping, SLAM). Восстановление положения робота и карты окружающей его среды по данным датчиков является сложной задачей, поскольку неизвестны как карта, так и траектория, а также соответствие между наблюдениями и ориентирами. Область методов оценки и отображения траекторий с дискретным временем хорошо развита. Однако представления в дискретном времени ограничены, потому что их нелегко адаптировать к неравномерно распределённым позам или асинхронным измерениям по траекториям.\nОдним из наиболее эффективных инструментов для аппроксимации гладких функций является регрессия гауссовского процесса (GP). Регрессия GP — это байесовский подход, в котором предполагается, что предварительное распределение по функциям является гауссовским процессом. В этой статье представлен алгоритм, основанный на приближении GP со случайными функциями Фурье (RFF) для SLAM без каких-либо ограничений. Авторы разработали этот метод на основе гауссовских процессов и случайных признаков Фурье для одновременной локализации и построения карты.\nПреимущества RFF для SLAM с непрерывным временем заключаются в том, что мы можем рассматривать более широкий класс ядер и в то же время поддерживать вычислительную сложность на достаточно низком уровне, работая в пространстве функций Фурье. Компромисс между точностью и скоростью можно регулировать с помощью количества функций. На наборе синтетических и реальных задач показано, что подход лучше всего работает в случаях очень шумных данных.\n_Полную версию статьи «Одновременная локализация и построение карты на основе случайных Фурье признаков» читайте, пройдя по_ _ссылке_  _или отсканировав QR-код:_\n\n### Игродром: что нужно знать о видеоиграх и игровой культуре ###\n\nАвтор этой книги Александр Вертушинский является одним из ведущих российских представителей направления game studies. Дисциплина занимается поисками ответов на вопросы «Что такое видеоигры и какое место они занимают в жизни человека?»\nИгра в разных формах присутствует в жизни каждого человека с ранних лет. Она может отражать социальные тенденции, технологические успехи и даже становиться искусством. Этот продукт современной культуры стал значимой частью нашей повседневности, поэтому сейчас самое время задаться вопросом, что собой представляют видеоигры и что они значат для нас? Книга представляет собой философское осмысление этапов развития игровой индустрии, анализ её сформировавшегося языка и места в современном культурном пространстве.\n_Бесплатно прочитать отрывок книги «Игродром: что нужно знать о видеоиграх и игровой культуре» и купить её можно по_ _ссылке_  _или QR-коду:_\n\n## Заключение ##\nМногие работы, представленные в сборнике «Наука в Сбере-2021», подготовлены сотрудниками нашей компании вместе с партнёрами по исследовательской работе из ведущих российских вузов и научных центров. Практически половина представленных работ (47%) опубликована или принята к публикации в журналах высшей категории Q1, а также представлена на конференциях уровня A/А\\*, а остальные нашли своего читателя в отечественных или узкоспециальных изданиях. Мы надеемся, что вам будет интересно ознакомиться с ними и составить своё мнение об уровне научной деятельности Сбера.\n_Скачать сборник «Наука в Сбере-2021» в формате PDF можно по_ _ссылке_ _._","id":66}
{"Host":"https://habr.com","Path":"/ru/post/112599/?mobile=yes","Text":"Знакомство с OCR библиотекой tessnet2 (язык C#) / Хабр                 \n\n26-01-2011\nБуквально на днях у меня появилась необходимость распознать простой текст на картинке и совсем не было желания реализовывать свой алгоритм, т.к. знаком с теорией и знаю, что это не такое простое дело, поэтому сразу решил изучить сначала рынок готовых библиотек. Буквально несколько запросов в гугл и я понял, что ничего более подходящего мне как библиотека tessnet2 невозможно найти. Постоянно читаю хабр и знаю, что тут есть уйма статей посвященных теории OCR и очень удивился, что нет ничего о библиотеке tessnet2. tessnet2 основана на Tesseract OCR Движок Tesseract OCR был одним из 3-х лучших движков представленных в 1995 году на UNLV Accuracy test. В период между 1995 годом и 2006 годом он был немного доработан, но, вероятно, это один из наиболее точных OCR движков, который доступен с открытым исходным кодом. Код, который доступен будет читать бинарные, серые или цветное изображение и выводить текст. Чтение TIFF построено так, что будут читаться несжатые TIFF изображения или могут быть добавлены Libtiff для чтения сжатых изображений. Как использовать Tessnet2: 1. Загружаем библиотеку, добавляем ссылку (reference) на Tessnet2.dll в .NET проекте. 2. Загружаем нужный нам язык (лично мне необходим английский) (tesseract-2.00.eng.tar.gz) и складываем в папку tessdata. Папка tessdata обязательно должна быть рядом с исполняемым файлом нашего приложения. Для того, чтобы прочитать текст с картинки достаточно такого текста: Bitmap image = new Bitmap ( \"eurotext.tif\" ); tessnet2.Tesseract ocr = new tessnet2.Tesseract(); ocr.SetVariable( \"tessedit_char_whitelist\" , \"0123456789\" ); // If digit only ocr.Init( @\"c:\\temp\" , \" eng \" , false ); // To use correct tessdata List <tessnet2.Word> result = ocr.DoOCR(image, Rectangle.Empty); foreach (tessnet2.Word word in result) Console .WriteLine( \"{0} : {1}\" , word.Confidence, word.Text); * This source code was highlighted with Source Code Highlighter . Я был очень рад результату, поэтому сразу вспомнил о том, что несколько месяцев назад прикручивал сервис для разгадывания каптч для одного проекта, сразу скажу, что ничего хорошего из этого не вышло, там нужна была скорость, но её не удалось там получить, т.к. подобные сервисы не способны её обеспечить, да и результат как правило плачевный, оно и понятно, т.к. платят там от 1 доллара за 1000 правильно введённых каптч, что мягко сказать ужасно. Поэтому эксперимента ради я решил поиграть с данной библиотекой на том примере. Исходными данными для нас будет являться каптча, на которой нужно произвести простейшие действия над двумя числами и получить ответ. Звучит довольно просто, но вот проблема ещё в том, что все символы разных цветов и имеется динамический фон, порой даже мне (человеку) сложно понять сходу, что там написано. Сразу привожу результаты работы программы, после чего я расскажу как это всё работает: На скриншотах чётко видно, что библиотека не может ничего разгадать из-за кучи линий, порой мешает и фон, который был убран не целиком. Поэтому я разработал свой небольшой алгоритм для чистки картинки, ничего в нём грандиозного нет, я просто отступаю несколько пикселей от края и пробегаю по прямоугольнику и собираю там цвета, также собираю цвета после первой цифры и перед знаком равно (последнее это больше хак, но т.к. статья посвящена другому, то оставил так). Всё что мне надо сделать потом – это закрасить все цвета, которые попали ко мне в коллекцию и не являются белым цветом. Из всех алгоритмов наиболее полезным может быть только алгоритм закрашивания области на Bitmap`е: void FloodFill( Bitmap bitmap, int x, int y, Color color) { BitmapData data = bitmap.LockBits( new Rectangle(0, 0, bitmap.Width, bitmap.Height), ImageLockMode.ReadWrite, PixelFormat.Format32bppArgb); int [] bits = new int [data.Stride / 4 * data.Height]; Marshal.Copy(data.Scan0, bits, 0, bits.Length); LinkedList<Point> check = new LinkedList<Point>(); int floodTo = color.ToArgb(); int floodFrom = bits[x + y * data.Stride / 4]; bits[x + y * data.Stride / 4] = floodTo; if (floodFrom != floodTo) { check.AddLast( new Point(x, y)); while (check.Count > 0) { Point cur = check.First.Value; check.RemoveFirst(); foreach (Point off in new Point[] { new Point(0, -1), new Point(0, 1), new Point(-1, 0), new Point(1, 0)}) { Point next = new Point(cur.X + off.X, cur.Y + off.Y); if (next.X >= 0 && next.Y >= 0 && next.X < data.Width && next.Y < data.Height) { if (bits[next.X + next.Y * data.Stride / 4] == floodFrom) { check.AddLast(next); bits[next.X + next.Y * data.Stride / 4] = floodTo; } } } } } Marshal.Copy(bits, 0, data.Scan0, bits.Length); bitmap.UnlockBits(data); } } * This source code was highlighted with Source Code Highlighter . Для тех кому интересно самому поэкспериментировать прикрепляю исходный код. Итог Мы познакомились с довольно интересной библиотекой tessnet2, проверили её работу в реальных условиях, добились довольно неплохих результатов разгадывания для сложных картинок (каптч), конечно ошибки есть, но их количество ничтожно мало, тем более для данного вида каптч можно добавить проверку с помощью регулярного выражения и Вы точно будете знать, что разгаданный текст соответствует нужному формату.\n","id":67}
{"Host":"https://habr.com","Path":"/en/post/261105/?mobile=no","Text":"Как оптимизировал работу с MongoDB с помощью устаревшего api или о чем молчит её спецификация… / Habr           \n\n24-06-2015\nОднажды столкнулся с задачей: mongoDb использовался как кэш/буфер между backend на Java и frontend на node.js. Все было хорошо, пока не появилось бизнес требование перебрасывать большие объемы за короткое время через mongoDb (до 200 тыс. записей не более чем за пару минут). Для чего не так важно, важна что задача такая появилась. И вот тут уж пришлось разбираться во внутренностях монги… Раунд 0: Просто пишем в монгу с Write Concern = Acknowledged. Самый банальный и простой способ в лоб. В этом случае монго гарантирует что все записалось без ошибок и вообще все будет хорошо. Все отлично пишется, но… при 200 тыс. умирает на двадцать и более минут. Не подходит. Путь в лоб вычеркиваем. Раунд 1: Пробуем Bulk write operations с тем же Write Concern = Acknowledged. Стало лучше, но не сильно. Пишет минут за десять-пятнадцать. Странно, вообще-то ожидалось большее ускорение. Ладно, идем дальше. Раунд 2: Пробуем поменять Write Concern на Unacknowledged и до кучи использовать Bulk write operations. Вообще, не лучшее решение, так как если что-то в монге пойдет что не так, мы никогда об этом не узнаем, так как она сообщит только что данные до неё дошли, а вот записались ли они в базу или нет неизвестно. С другой стороны, по бизнес требованиям данные это не банковские транзакции, единичная потеря не так критична, а если в монге все будет плохо, мы и так узнаем из мониторинга. Пробуем. С одной стороны записалась за всего минуту это хорошо (без Bulk write operations полторы минуты тоже неплохо), с другой стороны возникла проблема: сразу после записи java дает отмашку node.js и когда она начинает читать, данные приходят то целиком, то вообще не приходят, то половина читается, половина нет. Виной тут асинхронность — при таком Write Concern, монга ещё пишет, а node.js уже читает, соответственно клиент успевает прочитать раньше, чем запись гарантировано закончится. Плохо. Раунд 3: Начали думать, идея писать Thread.sleep(60 секунд) или записывать какой-нибудь контрольный объект в монгу, который показывал что все данные загрузились, выглядит очень криво. Решили посмотреть почему Bulk write operations ускоряют так плохо, ведь по идее Write Concern должен замедлять последнею запись при Bulk write operations, а не вообще все. Как-то нелогично получается, что ожидание записи последней порции требует столько времени. Смотрим код драйвера монги на Java, натыкаемся на то пакеты bulk операций ограничены неким параметром maxBatchWriteSize. Debug показывает что этот параметр у нас всего 500, то есть на самом деле весь bulk режется запросами только по 500 записей, поэтому и такие результаты, Acknowledged каждый раз ждет полной записи этих 500 записей, прежде чем послать новый запрос и так четыре тысячи раз при максимальном объеме, а это дико тормозит. Раунд 4 Пытаемся понять откуда берется этот параметр maxBatchWriteSize, находим что драйвер монги делает запрос getMaxWriteBatchSize() к серверу монги. Возникла мысль увеличить этот параметр в конфиге монги и обойти это ограничение. Попытки найти этот параметр или запрос в спецификации дали нулевой результат. Ладно, ищем в инете, находим исходные коды на C++. Этот параметр — банальная константа, зашитая жестко в коде исходников, то есть увеличить его никак не возможно. Тупик. Раунд 5 Ищем в инете ещё варианты. Вариант заливать через сотню параллельных потоков решили не пробовать, банально можно за DDos'ить собственный сервер с монгой (тем более что монга и сама умеет параллелить приходящие запросы). И тут нашли такую команду как getLastError, суть его ждать пока все операции сохраняться в базу и вернуть код ошибки или успешного окончания. Спецификация усилено пытается убедить, что метод устарел и его использовать не нужно, в драйвере монги он отмечен как depricated. Но пробуем отправляем запросы с Write Concern = Unacknowledged и Bulk write в ordered режиме, а потом вызываем getLastError() и да, за полторы минуты записали все записи синхронно, теперь клиент начинает чтение именно после полной записи всех объектов, так как getLastError() ждет окончания последней записи, при этом пакеты не тормозят друг друга. Ко всему прочему, если произойдет ошибка, мы об этом узнаем с помощью getLastError(). То есть мы получили именно быстрый Bulk write с Acknowledged, но ожиданием только последнего пакета (ну или почти, обработка ошибок вероятно будет хуже, чем у настоящего режима Acknowledged, вероятно эта команда не покажет ошибку произошедшую только в первых пакетах, с другой стороны вероятность что первый пакет упадет с ошибкой, а последний пройдет успешно — не так велика). Итак о чем молчит спецификация монги: 1. Bulk write операция не очень-то bulk и жестко ограничена потолком 500-1000 запросов в пакете. Update: на самом деле, как я сейчас обнаружил, все-таки упоминание о потолке в 1000 операций появилось, не было упоминания о магической константе больше года назад в версии 2.4, когда и проводился анализ, 2. Увы, но механизм с getLastError был в чем-то более успешным и новый механизм Write Concern пока не полностью его заменяет, точнее можно использовать устаревшую команду для ускорения работы, так логичное поведение «ждать успешную запись только последнего пакет из большого ordered bulk запроса» в монге так и не реализовано, 3. Проблема Write Concern=Unacknowledged даже не в том что данные могут потеряться и ошибка не возвращается, а в том что данные записываются совсем асинхронно и попытка клиента сразу обратиться к данным легко может привести к тому что он не получит данных или получит лишь часть их (важно, если команду на чтение отдавать сразу после записи). 4. У монги производительность запросов сильно страдает от такого ограниченного bulk'а и Acknowledged Write Concern реализован не совсем правильно, правильно ждать окончания записи именно последнего из пакетов. P.S. В целом, получился интересный опыт оптимизации нестандартными методами, когда в спеках нет всей информации. P.P.S. Так же советую посмотреть мой opensource проект [useful-java-links](https://github.com/Vedenin/useful-java-links/tree/master/link-rus) — возможно, наиболее полная коллекция полезных Java библиотек, фреймворков и русскоязычного обучающего видео. Так же есть аналогичная [английская версия](https://github.com/Vedenin/useful-java-links/) этого проекта и начинаю opensource подпроект [Hello world](https://github.com/Vedenin/useful-java-links/tree/master/helloworlds) по подготовке коллекции простых примеров для разных Java библиотек в одном maven проекте (буду благодарен за любую помощь).\n","id":68}
{"Host":"https://habr.com","Path":"/en/company/southbridge/blog/532134/","Text":"Kubernetes 1.20 — и что же слома.../починили на этот раз? / Habr              \n\n09-12-2020\nRelease Logo c сайта kubernetes.io Поздравляем с выходом версии 1.20. Третий релиз в 2020 году, в котором 11 фич объявили stable, 15 перевели в beta и добавили 16 новых в alpha-стадии. Судя по Release Logo, котики уже захватили мир и даже не скрывают этого. Давайте посмотрим, какие блюдца с молоком они заботливо положили нам под ноги. Конечно же, начать придется с громкой новости об отказе от dockershim. С одной стороны, дело это не быстрое, и есть еще целый год перед окончательным выпиливанием кода, но, с другой стороны, можно ожидать, что баги при взаимодействии с docker будут исправляться с меньшим приоритетом, чем раньше. Второе важное изменение, которое может сломать ваш продакшен, заключается в том, что в версии 1.20 починили баг с таймаутами для Probe типа exec. В описании контейнеров в манифесте подов есть специальный раздел, описывающий каким образом можно узнать, что приложение живое и готово обрабатывать входящие запросы. С помощью HTTP/TCP-запроса или запуска (exec probe) какого-либо процесса внутри контейнера (например, rabbitmqctl status) и анализа кода завершения. Суть проблемы в следующем: В манифесте есть поле timeoutSeconds:, которое задает максимальное время ожидания ответа на запрос или время выполнения процесса внутри контейнера. Ошибка была в том, что при выполнении exec probe этот параметр не учитывался и процесс внутри контейнера мог работать бесконечно долгое время, пока сам не завершится. Ошибку исправили, и в версии 1.20 все стало прекрасно. Но не для всех. Если вы используете exec probe и указывали timeoutSeconds: в своих манифестах — самое время проверить, укладывается ли время выполнения вашей пробы в таймаут. А вот если вы не указывали timeoutSeconds:, то у меня для вас не очень хорошая новость. Значение по умолчанию для этого параметра — 1 секунда, если ваша проба выполняется дольше, то успешной она явно не будет. И как всегда — вишенка на торте. Если вы используете docker, то при наступлении таймаута процесс внутри контейнера может остаться работать дальше. И если это readinessProbe, то со временем количество таких процессов может превзойти все ваши смелые ожидания. И к другим новостям: Если вы пользовались alpha feature API Priority and Fairness (APF) в своем мульти-мастер кластере 1.19 — выключайте ее перед обновлением. В 1.20 она переведена в статус beta, но в процессе перевода отломали совместимость с alpha. После обновления она будет включена по умолчанию. Мастеров больше не будет! Метка node-role.kubernetes.io/master и taint node-role.kubernetes.io/master:NoSchedule, которые по умолчанию ставил kubeadm на узлы с запущенными компонентами control-plane, объявлены deprecated и скоро будут убраны. Вместо них будут использоваться метка node-role.kubernetes.io/control-plane и taint node-role.kubernetes.io/control-plane:NoSchedule. В kubeadm усилили проверки входящих параметров serviceSubnet и podSubnet. Сеть для сервисов теперь имеет ограничение на размер и не должна аллоцировать более 20 бит. В переводе на человеческий это означает, что размер сети для IPv4 должен быть /12 или меньше (/13, /14 и т.п.), а для IPv6 — /108, но лучше будет использовать /112. Сеть для подов теперь проверяется на совместимость с параметром --node-cidr-mask-size, размер сетей, выделяемых на узел должен быть меньше, чем сеть для подов. Из API сервера убрали возможность беспарольного входа. Теперь не получится восстановить потерянный доступ к кластеру, просто зайдя на мастер и добавив в ключи запуска API сервера ключ --insecure-port 8080 IPv4/IPv6 dual stack был немножко переписан в части реализации сервисов. Один сервис теперь может иметь как IPv4, так и IPv6 адрес (при включении \"IPv6DualStack\" feature gate). Но это привело к несовместимым изменениям в API. Изменился манифест Kind: Service — вместо одного поля ipFamily теперь будет целых три: ipFamilyPolicy, ipFamilies, clusterIPs. Пишут, что большинству пользователей ничего не надо будет делать, сервисы останутся только IPv4, пока не включен соответствующий feature gate. TODO: Посмотреть, что там с Headless Service, которым в манифесте указываем clusterIP: None Команда для работы с сертификатам (проверки, генерации или продления kubeadm alpha certs) переименована в kubeadm certs. Старый вариант вызова еще работает, но в следующих релизах его уберут. Анонсировали фичу GracefulNodeShutdown — при ее включении kubelet будет самостоятельно завершать все запущенные поды, при выключении узла. Соблюдая все обычные gracefulShutdown таймауты и вызывая preStop хуки. Команда kubectl debug повышена до беты, и в ее описании появилось много различных возможностей. Запуск дополнительных эфемерных дебаг контейнеров в поде, создание копий пода и замена команды запуска. Единственная тонкость — новое название команды конфликтует с плагином debug, и теперь вместо плагина будет выполнятся встроенная команда. Если вы хотите продолжать пользоваться плагином, то его придется переименовать. В процессе починки CVE-2020-8559 немножко усилили безопасность, что может привести к поломке работы бэкендов расширения API, таких как metrics-server или prometheus adapter. Выпущен новый CronJob controller v2. Видимо, чинить баги старого контроллера больше не могут, и решили выпустить новый, с новыми багами. Пока в стадии альфа, так что включать его надо с помощью специального feature gate.\n","id":69}
{"Host":"https://habr.com","Path":"/ru/post/146167/","Text":"habr.com\n== Перевод из любой системы счисления в любую чисел большой длины\n\n\n20 июн 2012 в 14:41  Недавно решал задачи по криптографии, и возникла необходимость переводить очень большие числа из одной системы счисления в другую. С двоичной, восьмеричной, десятичной и шестнадцатеричной системой справляется и стандартный калькулятор ОС. Но он не рассчитан на числа большой длины. А мне как раз необходимо работать с числами длиной **>1000** знаков. Для этих целей решил написать небольшой консольный конвертер, позволяющий работать с числами любой длины и любой системы счисления от 2 до 36.Требования:\n\n • Конвертер должен работать с числами любой длины. • Конвертер должен работать в любой системе счисления от 2 до 36. • Конвертер должен уметь работать с файлами.Реализация:\n\n Писать решил на языке C++. Люблю этот язык, да и перевести исходники в другой язык из C++ не составляет особого труда.Написал следующий класс:\n\nclass Converter{\nprivate:\n//Вектор содержит исходное число\n\tvector<int> a;\n//Исходная система счисления\n\tint iriginal;\npublic:\n//Конструктор, содержит 2 параметра: строка исходного числа, исходная система счисления\n\tConverter(string str, int original){\n\t\tthis->iriginal = original;\n\t\t//Заносит числа исходного числа в вектор\n\t\tfor ( int i=0; i < str.length(); i++ ){\n\t\t\tthis->a.push_back(charToInt(str[i]));\n\t\t}\n\t}\n\t//Переводит символ в число, вместо некорректных символов возвращает -1\n\tint charToInt(char c){\n\t\tif ( c >= '0' && c <= '9' && (c - '0') < this->iriginal ){\n\t\t\treturn c - '0';\n\t\t}else{\n\t\t\tif ( c >= 'A' && c <= 'Z' && (c - 'A') < this->iriginal ){\n\t\t\t\treturn c - 'A' + 10;\n\t\t\t}else {\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\t//Переводит число в символ\n\tchar intToChar(int c){\n\t\tif ( c >= 0 && c <= 9 ){\n\t\t\treturn c + '0';\n\t\t}else{\n\t\t\treturn c + 'A' - 10;\n\t\t}\n\t}\n\t//Получает следующую цифру числа в новой системе счисления\n\tint nextNumber(int final){\n\t\tint temp = 0;\n\t\tfor ( int i = 0; i<this->a.size(); i++){\n\t\t\ttemp = temp*this->iriginal + this->a[i];\n\t\t\ta[i] = temp / final;\n\t\t\ttemp = temp % final;\n\t\t}\n\t\treturn temp;\n\t}\n\t//Возвращает true - если массив состоит из одних нулей и false в противном случае\n\tbool zero(){\n\t\tfor ( int i=0; i<this->a.size(); i++ ){\n\t\t\tif ( a[i] != 0 ){\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\treturn true;\n\t}\n\t//Конвертирует исходное число в заданную систему счисления\n\tstring convertTo(int final){\n\t\tvector<int> b;\n\t\tint size = 0;\n\t\tdo {\n\t\t\tb.push_back(this->nextNumber(final));\n\t\t\tsize++;\n\t\t}while( !this->zero() );\n\n\t\tstring sTemp=\"\";\n\t\tfor (int i=b.size()-1; i>=0; i--){\n\t\t\tsTemp += intToChar(b[i]);\n\t\t}\n\treturn sTemp;\n\t}\n};\n Код получился достаточно простойДалее прикрутил его в проект:\n\n//Адрес файла, содержащего исходное число\n\tstring inputFile = argv[1];\n//Исходная система счисления\n\tint original = atol(argv[2]);\n//Требуемая система счисления\n\tint final = atol(argv[3]);\n\n//Строка, содержащая исходное число\n\tstring origNumber;\n\n\tifstream fin(inputFile.c_str());\n\tif ( fin.is_open() ){\n\t\tfin >> origNumber;\n\t}else{\n\t\tcout << \"File \" << inputFile << \" not open\" << endl;\n\t//Небольшой костыль - если неудалось открытьфайл, возможно вместо его ввели требуемое число\n\t\torigNumber = inputFile; \n\t}\n\tfin.close();\n\tConverter conv(origNumber,original);\n//Если не был задан файл для вывода, то результат отобразиться на экране\n\tif ( argc > 4 ){\n\t//Адрес файла для записи нового числа\n\t\tstring outputFile = argv[4];\n\t\tofstream fout(outputFile.c_str());\n\t\tif ( fout.is_open() ){\n\t\t\tfout << conv.convertTo(final);\n\t\t}else{\n\t\t\tcout << \"File \" << outputFile << \" not create\" << endl;\n\t\t\tcout << conv.convertTo(final) << endl;\n\t\t}\n\t}else{\n\t\tcout << conv.convertTo(final) << endl;\n\t}\n Код, конечно, далек от идеала, но зато все просто и понятно.Теперь можно протестировать.\n\n Конвертер готов, теперь осталось его испытать. Создаю файл, содержащий число, представляющее из себя тысячу девяток. Запускаю в консоли:\n\n Конвертер удачно создает файл output.txt, содержащий число, длиной 3322 символа. Теперь выведу его на экран, для этого достаточно не задавать файл для вывода.\n\n Так же можно задавать исходное число прямо в консолиВывод:\n\n Если нужно конвертировать что-то очень большое, то данный конвертер отлично для этого подойдет. Отсутствие интерфейса позволяет без изменения запускать написанный код на любой платформе. А консоль забывать нельзя, будь то windows или Linux… Скачать проект(VS 2008) можно [**здесь**](https://dl.dropbox.com/u/36713876/Calc.zip).","id":70}
{"Host":"https://habr.com","Path":"/en/company/ispsystem/blog/480446/?mobile=no","Text":"Пишем Grafana reverse proxy на Go / Habr                  \n\n17-12-2019\nОчень хотелось назвать статью «Proxy-сервис на Go в 3 строчки», но я выше этого. В действительности так и есть, основную логику можно уместить в трёх строках. Для нетерпеливых и тех, кто хочет увидеть самую суть: proxy := httputil.NewSingleHostReverseProxy(url) r.Header.Set(header, value) proxy.ServeHTTP(w, r) Под катом более подробный рассказ для новичков в языке Golang и тех, кому нужно создать обратный прокси в кратчайшие сроки. Разберём, для чего нужен прокси-сервис, как его реализовать и что под капотом у стандартной библиотеки. Обратный прокси Обратный прокси (reverse proxy) — это тип прокси-сервера, который получает запрос от клиента, перенаправляет его на один или несколько серверов и пересылает ответ обратно. Отличительная черта обратного прокси в том, что он является входной точкой для соединения пользователя с серверами, с которыми сам прокси связан бизнес-логикой. Он определяет, на какие серверы будет передан запрос клиента. Логика построения сети за прокси остается скрыта от пользователя. Обратный прокси (Reverse Proxy) Для сравнения, обычный прокси связывает своих клиентов с любым сервером, который им необходим. В этом случае прокси находится перед пользователем и является просто посредником в выполнении запроса. Обычный прокси (Forward Proxy) Для чего использовать Концепцию обратного прокси можно применять в различных ситуациях: — балансировка нагрузки, — A/B-тестирование, — кэширование ресурсов, — сжатие данных запроса, — фильтрация трафика, — авторизация. Конечно, область применения не ограничивается этими шестью пунктами. Сам факт возможности обработки запроса как до, так и после проксирования даёт большой простор для творчества. В этой статье разберём использование обратного прокси для авторизации. Задача Мы разрабатываем панель управления виртуализацией VMmanager 6. В один прекрасный день мы решили дать пользователям бóльшую свободу в мониторинге и визуализации данных кластеров. Для этих целей выбрали Grafana. Чтобы Grafana заработала с нашими данными, надо было настроить авторизацию. Сделать это несложно, если бы не три «но». У нас уже есть единая точка входа — сервис авторизации. Мы не хотим заводить и авторизовывать пользователей в Grafana. Мы не хотим давать пользователям доступ к Grafana напрямую. Чтобы соблюсти условия, решили поместить Grafana во внутреннюю сеть и написать обратный прокси. Он будет проверять права в сервисе авторизации и только после этого передавать запрос в Grafana. Идея Основная идея — переложить ответственность за авторизацию в Grafana на сервер обратного proxy (официальная документация). Grafana будет принимать любой запрос как авторизованный, если он содержит определённый заголовок. Перед тем, как подставить этот заголовок, мы должны убедиться в правах текущего пользователя на работу с Grafana. Цепочка вызовов «Grafana-proxy, или Туда и обратно» Реализация Функция main довольно стандартна. Мы стартуем http-сервер, который будет принимать подключения на 4000 порту и обрабатывать любой адрес “/”, с которым произойдет подключение. func main() { http.HandleFunc(\"/\", handlerProxy) if err := http.ListenAndServe(\":4000\", nil); err != nil { panic(err) } } Основная часть работы происходит в обработчике запросов. [полный код под катом] func handlerProxy(w http.ResponseWriter, r *http.Request) { fmt.Println(r.URL.Host) if strings.HasPrefix(r.URL.String(), \"/api\") { //Проверка прав в сервисе авторизации } url, err := url.Parse(fmt.Sprintf(\"http://%s/\", grafanaHost)) if err != nil { SendJSONError(w, err.Error()) return } proxy := httputil.NewSingleHostReverseProxy(url) fmt.Println(r.URL.Host) r.Header.Set(grafanaHeader, grafanaUser) proxy.ServeHTTP(w, r) } Пройдемся по параметрам. Основные переменные в примере я вынес в константы: grafanaUser = \"admin\" //Пользователь, под которым мы будем авторизовываться grafanaHost = \"grafana:3000\" //Адрес расположения grafana grafanaHeader = \"X-GRAFANA-AUTH\" //Header, наличие которого определяет успешную авторизацию Для примера этого вполне достаточно, на практике может потребоваться выполнять предустановку этих значений. Вы можете передавать их proxy как параметры командной строки, затем использовать flag или более продвинутые пакеты для их разбора. В контейнерной среде также часто используются переменные окружения для конфигурации сервисов, на этом пути вам поможет os.Getenv. Далее идет проверка авторизации: if strings.HasPrefix(r.URL.String(), \"/api\") { err := CheckRights(r.Header) if err != nil { SendJSONError(w, err.Error()) return } } Если запрос идёт на grafana.host/api, проверяем права текущего пользователя на использование Grafana. Проверка необходима, чтобы на каждый GET-запрос JS-скрипта или PNG-иконки не беспокоить точку авторизации. Статический контент мы будем проксировать без дополнительных проверок. Для этого передаем map с заголовками, в которых содержится сессия пользователя, в сервис авторизации. Это может быть обычный GET-запрос. Устройство сервиса авторизации здесь значения не имеет. Если данные авторизации не устраивают, закрываем соединение, возвращая ошибку. После проверок формируем объект базового пути: url, err := url.Parse(fmt.Sprintf(\"http://%s/\", grafanaHost)) С помощью стандартного пакета httputil, расширяющего пакет http, формируем объект ReverseProxy. proxy := httputil.NewSingleHostReverseProxy(url) ReverseProxy — это обработчик запроса, который примет входящий запрос, отправит его в Grafana и передаст ответ обратно клиенту. Он будет направлять все запросы по адресу “базовый путь + запрошенный url”. Если пользователь пришел по адресу proxy:4000/api/something, его запрос будет превращен в grafana:3000/api/something, где grafana:3000 — базовый путь, переданный в NewSingleHostReverseProxy, а /api/something — входящий запрос. Добавляем авторизационный заголовок для Grafana и вызываем метод ServeHTTP, который сделает основную работу по обработке запроса. r.Header.Set(grafanaHeader, grafanaUser) proxy.ServeHTTP(w, r) Под капотом ServeHTTP делает довольно много работы, например, обрабатывает заголовок X-Forwarded-For или 101 ответ сервера на смену протокола. Основная работа метода — отправить запрос на составной адрес и полученный ответ переложить в ResponseWriter. Результат Весь код доступен на github. Проверка Эмулируем нашу систему с помощью Docker. Создадим два контейнера — proxy и Grafana в одной сети. Точку авторизации создавать не будем, считаем, что она всегда отвечает утвердительно. Контейнер Grafana будет недоступен вне сети, контейнер с proxy доступен на определённом порту. Создаём сеть: docker network create --driver=bridge --subnet=192.168.0.0/16 gnet Поднимаем контейнер Grafana с настроенным режимом авторизации посредством заголовка: docker run -d --name=grafana --network=gnet -e \"GF_AUTH_PROXY_ENABLED=true\" -e \"GF_AUTH_PROXY_HEADER_NAME=X-GRAFANA-AUTH\" grafana/grafana Обращаю ваше внимание, что это демонстрационная и не окончательная конфигурация. Как минимум, необходимо установить пароль администратора и запретить автоматическую регистрацию пользователей. Поднимаем reverse proxy: docker run -d --name proxy -p 4000:4000 --network=gnet grafana_proxy:latest В браузере переходим на localhost:4000. Отлично, перед нами авторизованная Grafana. Dockerfile для сборки контейнера с proxy и более подробная инструкция по поднятию контейнеров есть на github.\n","id":71}
{"Host":"https://habr.com","Path":"/ru/post/437122/?mobile=yes","Text":"Naming things / Хабр                \n\n22-01-2019\nThere are only two hard things in Computer Science: cache invalidation and naming things. — Phil Karlton We, developers, spend more time reading code than writing it. It is important for the code to be readable and clear about its intent. Below are some advice based on my experience naming things. Meaning A name, be it a variable, a property, a class, or an interface, should reflect the purpose of why it's being introduced and how it's used. Use accurate names If one can not get an idea about usage and purpose without extra comments the name is not good enough. If immediate usage or purpose idea based on naming is wrong then the naming is unacceptable. The worst possible naming is when a method name lies to the one who reads it. Avoid meaningless names These are names like $i, $j, $k etc. While these are OK to use in cycles, in other cases they are wasting readability. A common source of such names is classic science where most formulas use one-letter variables so it is faster to write. As a consequence, you can not make sense of these formulas without an introductory paragraph explaining naming. Often this paragraph is hard to find. Since computer science education includes significant number of classic science disciplines, students are getting used to such naming and bring it to programming. Naming classes, interfaces, properties and methods Class name should be one or several nouns. There should be no verbs. Try avoiding \"data\", \"manager\", \"info\", \"processor\", \"handler\", \"maker\", \"util\" etc. as these usually an indicator of vague naming. Interfaces are usually either nouns or adjectives. Some teams, including PHP-FIG, chose to postfix interfaces with Interface. Some do it with I prefix and some use no prefix or postfix. I find all these acceptable in case you are consistent. Properties should be named with nouns or adjectives. For booleans use affirmative phrases prefixing with \"Is\", \"Can\", or \"Has\" where appropriate. Method names should contain one or more verbs as they are actions. Choose verb that describes what the method does, not how it does it. While it is not strictly necessary, it is a good idea to end derived class name with the name of the base class: class Exception {} class InvalidArgumentException extends Exception {} Consistency Use a single name for a single concept. Be consistent. A good example is using begin/end everywhere instead of mixing it with start/finish. Follow code style conventions When developing a project, a team must agree on code style and naming conventions they use and follow these. If a part of conventions is not accepted by some team members then it should be reviewed, changed and new rule should be set. For PHP the most common convention is currently PSR-2 and most internal project conventions are based on it. Verbosity Avoid reusing names Using same name for many concepts should be avoided if possible as it brings two problems: When reading, you have to keep context in mind. Usually that means getting to namespace or package declaration constantly. Searching for such names is a pain. Avoid contractions Do not use contractions. Common examples are: cnt iter amnt impl function cntBigWrds($data, $length) { $i = 0; foreach ($data as $iter) { if (mb_strlen($iter) > $length) { $i++; } } return $i; } $data = ['I', 'am', 'word']; echo cntBigWrds($data, 3); The code above when named properly becomes: function countWordsLongerThan(array $words, int $minimumLength) { $count = 0; foreach ($words as $word) { if (mb_strlen($word) > $minimumLength) { $count++; } } return $count; } $words = ['I', 'am', 'word']; echo countWordsLongerThan($words, 3); Note still that short explanatory names without contractions are better than long explanatory names so do not take verbosity to extreme ending up with names like processTextReplacingMoreThanASingleSpaceWithASingleSpace(). If the name is too long it either means it could be re-worded to make it shorter or the thing you are naming is doing too much and should be refactored into multiple things. Avoid acronyms Avoid acronyms and abbreviations except commonly known ones such as HTML. Elon Musk sent an email titled \"Acronyms Seriously Suck\" to all SpaceX employees in May 2010: There is a creeping tendency to use made up acronyms at SpaceX. Excessive use of made up acronyms is a significant impediment to communication and keeping communication good as we grow is incredibly important. Individually, a few acronyms here and there may not seem so bad, but if a thousand people are making these up, over time the result will be a huge glossary that we have to issue to new employees. No one can actually remember all these acronyms and people don't want to seem dumb in a meeting, so they just sit there in ignorance. This is particularly tough on new employees. That needs to stop immediately or I will take drastic action — I have given enough warning over the years. Unless an acronym is approved by me, it should not enter the SpaceX glossary. If there is an existing acronym that cannot reasonably be justified, it should be eliminated, as I have requested in the past. For example, there should be not \"HTS\" [horizontal test stand] or \"VTS\" [vertical test stand] designations for test stands. Those are particularly dumb, as they contain unnecessary words. A \"stand\" at our test site is obviously a test stand. VTS-3 is four syllables compared with \"Tripod\", which is two, so the bloody acronym version actually takes longer to say than the name! The key test for an acronym is to ask whether it helps or hurts communication. An acronym that most engineers outside of SpaceX already know, such as GUI, is fine to use. It is also ok to make up a few acronyms/contractions every now and again, assuming I have approved them, e.g. MVac and M9 instead of Merlin 1C-Vacuum or Merlin 1C-Sea Level, but those need to be kept to a minimum. I agree with him. Readability Code should be able to be read as easily as prose. Choose words that you would choose writing an article or a book. For example, a property named TotalAmount is more readable in English than AmountTotal. Hiding implementation details That is more about object oriented design but it affects readability much if implementation details are exposed. Try not to expose methods named like: initialize init create build Domain language Code should use the same names as used in the business or domain model automated. For example, if a travel business using \"venue\" as a general name for cafes, hotels and tourist attractions, it is a bad idea to use \"place\" in the code because you and your users will speak two different languages making it more complicated than it should. Such a language is often called \"The Ubiquitous Language\". You can learn more from \"Domain Driven Design Quickly\" mini-book by InfoQ. English Majority of programming languages use English for built-in constructs and it is a good practice to name things in English as well. It is extremely important for a developer to learn English at least on the basic level and, what is more important, to have good vocabulary that one can use for finding a good name. Some useful tools: thesaurus.com — for finding synonyms. wordassociations.net — for finding associations. References Microsoft Guidelines for Names TwoHardThings by Martin Fowler Domain Driven Design Quickly by InfoQ\n","id":72}
{"Host":"https://habr.com","Path":"/ru/articles/370015/","Text":"Бюджетное видеонаблюдение для прижимистых «чайников» / Хабр                                                         \n\nСкоро будет 7 лет с момента написания статьи \"\nВидеонаблюдение под Ubuntu для «чайников\n» (ZoneMinder)\". За эти годы она не раз корректировалась и обновлялась в связи с выходом новых версий, но кардинальная проблема, а именно — стоимость IP видеокамер, оставалась прежней. Её обходили оцифровывая аналоговые потоки и эмулируя IP камеры с помощью USB «вебок».\n\nСитуация изменилась с появлением китайских камер стандарта\nONVIF\n2.0 (Open Network Video Interface Forum). Теперь любую камеру отвечающую стандарту вы можете настроить с помощью\nONVIF Device Manager\n.\n\nБолее того, вы сразу можете увидеть адреса и параметры потоков вещания с камеры. Да, да. Теперь потоков, как минимум — 2, не считая звука. Один архивный — в максимальном качестве, другой — рабочий в меньшем разрешении.\n\n_\\* Все картинки кликабельны_\n\nЯ буду рассказывать на примере камеры\nMISECU\nIPC-DM05-1.0 Купил её в «чёрную пятницу» по цене 1059,15 руб. Сейчас они подняли цену и я бы скорее приобрел\nGADINAN\n. Что в прочем, одно и то-же. В любом случае, аппаратная часть моей камеры определяется как\n**hi3518e\\_50h10l\\_s39**\nне зависимо от того, какой логотип написан на коробке. Камера купольная, по факту представляет из себя шарик «на верёвочке» легко вынимаемый из гнезда-держателя. Если будете заказывать, обратите внимание, что блок питания надо покупать отдельно (DC 12V/2A). Я использовал БП от сгоревших китайских-же настольных часов. К сожалению, звука и управления позицией в камере нет. Для этих целей подойдет какой-нибудь беби-монитор типа\nэтого\nили\nэтого\n. Главное, что бы в названии было слово Onvif.\n\nПосле распаковки и включения надо выставить IP адрес каждой камеры (по умолчанию у всех жестко 192.168.1.10), чтобы они не конфликтовали между собой. Это можно сделать в ONVIF Device Manager или штатной утилитой General Device Manage которая идет в комплекте, на мини CD. Далее, выставляем временную зону, параметры отображения дат и имя для каждой камеры. Создаем пользователей с правами «только для просмотра».\n\nВеб-интерфейс камеры, программы CMS и интерфейс облака в браузере совершенно одинаковы, неудобны и требуют IE c ActiveX.\n\nБлаго, их можно с успехом заменить приложением XMeye установленным на Android или iOS. Но, прежде необходимо сделать нашу камеру видимой для облака. Для этого откройте порт по которому работает Onvif (8899) на вашем коммутаторе. В моём случае — это\n_NAT Setting-Virtual Server._\nЕсли камер несколько, то внутренний порт для каждого IP оставляете прежним, а внешний меняете на пару значений. Далее, камера сама постучится в облако и предъявит свой индивидуальный CloudID. Вам нужно будет только добавить его в свой профиль в облаке.\n\nСобственно, сама по себе камера уже может детектить движение, стримить видео и отправлять аллармы. Вкупе с облачным сервисом\nXMeye\n— это готовый сервис мониторинга.\n\nЕсли вам хочется иметь свой собственный регистратор с архивами, и вы любите Windows, то ставьте бесплатные\niSpy\n, или\nSecurOS Lite\n(до 32 камер) или бесплатную-же версию (до 8 камер)\nXeoma\n. Кстати, у последней есть версии для Mac OS X, Linux включая ARM и Android.\n\nС настройками не должно возникнуть проблем, так что можете дальше не читать. Остальная часть статьи написана для Linux.\n\nЯ был приятно удивлен обнаружив в\nZoneminder\nv.1.30.0 визард для настройки ONVIF камер. Он позволяет подключить к консоли любой из потоков идущих с камеры в зависимости от аппаратных возможностей и потребностей оператора.\n\nУстановка и настройка Zoneminder никогда не были лёгким занятием. Последняя версия вышла особо капризной и требует предварительной установки веб-сервера LAMP, с последующим выполнением ряда\nдополнительных действий\n. Поэтому, приведу старый «джедайский» способ подключения камеры для более старых версий:\n\n1. Определите адреса потоков через ONVIF Device Manager или Xeoma. У вас должно получиться что-то похожее:\n\n```\nrtsp://192.168.1.4/onvif1\n```\n\nили\n\n```\nrtsp://192.168.1.1*/user=****_password=****_channel=1_stream=1.sdp?real_stream\n```\n\nНе забудьте заменить звездочки (\\*) своими данными.\n\n2. Проверьте адреса в проигрывателе VLC.\n_Меню-Медиа-Открыть IRL_\n\n3. Добавьте новый монитор с параметрами:\n\n> Source Type — `Remote`\n>\n>\n\nRemote Host Path — `rtsp://192.168.1.1*/user=****_password=****_channel=1_stream=1.sdp?real_stream`\n\nЖелаю удачи.","id":73}
{"Host":"https://habr.com","Path":"/en/articles/727954/","Text":"Больше контроля над селектором :nth-child() с помощью синтаксиса of S / Habr                          \n\nЭта статья — перевод оригинальной статьи « More control over :nth-child() selections with the of S syntax »\nТакже я веду телеграм канал « Frontend по‑флотски », где рассказываю про интересные вещи из мира разработки интерфейсов.\n## Селекторы псевдоклассов :nth-child() и :nth-last-child() ##\nС помощью селектора псевдокласса :nth-child()  можно выбирать элементы в DOM по их индексу. Используя микросинтаксис An+B , вы получаете тонкий контроль над тем, какие элементы вы хотите выбрать.\n`:nth-child(2)` : Выберет второй дочерний элемент.\n\n`:nth-child(2n)` : Выберет все четные дочерние элементы (2-й, 4-й, 6-й, 8-й и так далее).\n\n`:nth-child(2n+1)` : Выберет все нечетные дочерние элементы (1-й, 3-й, 5-ый, 7-ой и так далее).\n\n`:nth-child(5n+1)` : Выберет 1-го (=(5×0)+1), 6-го (=(5×1)+1), 11-го (=(5×2)+1), ребенка.\nЧтобы интерактивно увидеть, как логика An+B влияет на выделения, используйте тестер :nth-child .\nНо можно сделать более творческий выбор, если опустить параметр `A` . Например:\n`:nth-child(n+3)` : Выберет каждый дочерний элемент, начиная с третьего (3-й, 4-й, 5-й и так далее).\n\n`:nth-child(-n+5)` : Выберет каждый дочерний элемент до 5-го (1-го, 2-го, 3-го, 4-го, 5-го).\nОбъедините несколько таких селекторов `:nth-child()` , и вы сможете выбирать диапазоны элементов:\n`:nth-child(n+3):nth-child(-n+5)` : Выберет каждый дочерний элемент от 3-го до 5-го (3-й, 4-й, 5-й).\nИспользуя `:nth-last-child()` , вы можете делать подобные селекторы, но вместо того, чтобы начинать считать с начала, вы начинаете считать с конца.\nЕсли вы хотите перейти на новый уровень, вы можете использовать :nth-child() для применения стилей к группе элементов, когда они достигают определенного размера (\"Quantity Queries\")  или стилизовать родительский элемент на основе количества его дочерних элементов .\n## Предварительная фильтрация с помощью синтаксиса of S ##\nНовое в CSS Selectors Level 4  - возможность опционально передавать список селекторов в `:nth-child()`  и `:nth-last-child()` .\n```\n:nth-child(An+B [of S]?)\n:nth-last-child(An+B [of S]?)\n```\nКогда указано of S, логика An+B применяется только к тем элементам, которые соответствуют заданному списку селекторов S. Это означает, что вы можете предварительно отфильтровать дочерние элементы, прежде чем An+B сделает свое дело.\n\n## Примеры ##\nНапример, `:nth-child(2 of .highlight)`  выбирает второй подходящий элемент, имеющий класс .highlight. Другими словами: из всех дочерних элементов с классом `.highlight`  выберите второй.\nЭто отличается от `.highlight:nth-child(2)` , который выбирает элемент, имеющий класс `.highlight` , **а также**  являющийся вторым дочерним элементом.\nВ демонстрационном примере ниже вы можете увидеть эту разницу:\nЭлемент, соответствующий `:nth-child(2 of .highlight)` , имеет розовый контур.\n\nЭлемент, соответствующий `.highlight:nth-child(2)` , имеет зеленый контур.\n\nCodePen\nОбратите внимание, что `S`  - это список селекторов, что означает, что он принимает несколько селекторов, разделенных запятой. Например, `:nth-child(4 of .highlight, .sale)`  выбирает четвертый элемент, который является либо `.highlight` , либо `.sale`  из множества дочерних элементов.\nВ демонстрационном примере ниже элемент, соответствующий `:nth-child(4 of .highlight, .sale)` , имеет оранжевый контур.\n\nCodePen\n## Зебра, пересмотренный вариант ##\nКлассическим примером использования `:nth-child()`  является создание таблицы с полосками зебры. Это визуальный прием, при котором в каждой строке таблицы чередуются цвета. Обычно это делается следующим образом:\n```\ntr :nth-child (even) {\n  background-color : lightgrey;\n}\n```\nХотя это хорошо работает для статических таблиц, это становится проблематичным, когда вы начинаете динамически фильтровать содержимое таблицы. Когда, например, вторая строка становится скрытой, в итоге остаются видимыми первая и третья строки, каждая из которых имеет одинаковый цвет фона.\n\nCodePen\nЧтобы исправить это, мы можем использовать `:nth-child(An+B [of S]?)` , исключив скрытые строки из логики `An+B` :\n```\ntr :nth-child (even of :not ( [hidden] )) {\n  background-color : lightgrey;\n}\n```\n\nCodePen\nДовольно круто, правда?","id":74}
{"Host":"https://habr.com","Path":"/ru/companies/pt/articles/714316/","Text":"Зеркалирование GitHub-проектов в 2023 году / Хабр                 \n\nПо ряду причин я решил зеркалировать  свои открытые GitHub-проекты   на другие платформы совместной разработки. Сделать это оказалось не так просто. В этой короткой статье описаны трудности, с которыми мне пришлось столкнуться, и итоговое рабочее решение.\n\nИллюстрация Джона Тенниела\n\nРанее я пользовался функцией зеркалирования  GitLab  , которая называется pull mirroring. Она с определенной периодичностью копировала код, теги и обсуждения в issues и pull requests из моих GitHub-репозиториев в GitLab. Но некоторое время назад я заметил, что мои GitLab-зеркала стали отставать от основных репозиториев. Я вошел в GitLab и увидел вот такой неприятный баннер:\n\nФункция pull mirroring исчезла из некоторых моих репозиториев. Включить ее бесплатно более невозможно:\n\nПокупка премиум-поддержки GitLab (Premium Tier) невозможна в России. GitLab запрещает использовать даже ее бесплатную пробную версию. Поэтому я стал искать другие решения для зеркалирования моих GitHub-проектов:\n\nРазвертывание своего отдельного сервера разработки GitLab или  Forgejo   мне не подходило: я не хотел размещать свои open-source проекты на изолированном сервере. Это значительно ограничило бы взаимодействие с сообществом и стало бы препятствием для контрибьюторов.\n\n Я проверил китайскую платформу  Gitee  , и мне не понравилась ее ограниченная поддержка английской локализации.\n\n Я посмотрел на  Radicle  , пиринговую сеть для разработки программного обеспечения. Но ее «мощные функциональные возможности, основанные на блокчейне», показались мне слишком уж радикальными.\n\n Затем я посмотрел на  Codeberg  . Эту платформу коллективной разработки поддерживает одноименная некоммерческая организация, продвигающая идеи свободного программного обеспечения (free software). Эти ребята мне нравятся гораздо больше, чем Microsoft, владеющая GitHub. Но в марте 2020 года они отключили функцию зеркалирования на серверах Codeberg из-за  нехватки ресурсов  . По их словам, «зеркалированные репозитории легко создаются, но потребляют ресурсы вечно» :(\n\n Я также проверил  SourceHut   (спасибо  paulmairo   за ссылку). Эта платформа мне не подошла, потому что:\n\n Она предоставляет только платные услуги.\n\n Процесс разработки на ней выглядит несовместимым с GitHub: на платформе SourceHut для работы с задачами и кодом проектов используется электронная почта.\n\n Я посмотрел на  Salsa   (спасибо  Mic\\_92   за идею). Это сервер совместной разработки сообщества Debian, который работает на программном обеспечении GitLab. Сначала я зарегистрировал там учетную запись. Несколько дней спустя администратор Salsa активировал ее, и мне удалось скопировать один из моих проектов из GitHub в Salsa. Но оказалось, что функция pull mirroring в Salsa тоже отключена, как и на  gitlab.com   в бесплатном режиме. Я  задал вопрос об этом   в их трекере, но, к сожалению, не получил никакого ответа.\n\n Затем я создал  зеркала   для своих GitHub-проектов на  GitFlic  . Это небольшая российская платформа коллективной разработки, которая сейчас активно развивается. Их служба поддержки разрешила мне создать публичные репозитории и настроить зеркалирование. Кроме того, они довольно оперативно отвечали на вопросы и давали комментарии по обнаруженным проблемам. К сожалению, в GitFlic нет CI и даже нет возможности копировать информацию из issues и pull requests с GitHub. Очевидно, зеркала на GitFlic не стали для меня окончательным решением.\n\nСхема зеркалирования GitHub -> GitFlic\n\nКстати, я бы предложил команде GitFlic назвать раздел Issues для проектов на платформе не «Проблемы», а «Задачи».\n\nПодводя итог, я не нашел ни одной популярной платформы для совместной разработки, которая могла бы предоставить полнофункциональные зеркала для моих проектов на GitHub. Поэтому я решил взглянуть на эту задачу с другой стороны: как я могу вручную создать резервную копию обсуждений в своих GitHub-проектах?\n\nПервая идея состояла в том, чтобы сделать эту информацию частью кода. Вскоре я нашел проект  gh2md  , который это позволяет. Инструмент gh2md выгружает содержимое GitHub issues и pull requests заданного проекта в Markdown-документ.\n\nПод капотом этот инструмент использует интерфейсы GraphQL, предоставляемые GitHub, поэтому для него мне пришлось сгенерировать личный токен доступа GitHub с минимальными привилегиями. Подробности о работе с такими токенами можно найти в  документации GitHub  .\n\nТокен доступа GitHub\n\nТеперь мои проекты содержат файл  `issues.md`    с резервной копией всех задач и обсуждений  . Можно было бы использовать gh2md в CI, чтобы обновлять  `issues.md`   автоматически. Но пока я воздерживаюсь от предоставления скриптам GitHub Actions полного доступа к репозиторию.\n\nВ определенный момент мне пришла мысль о CI-скрипте, который генерировал бы pull request с обновлениями файла  `issues.md`  . Но вскоре я понял, что это не очень умная идея: такой автоматически созданный запрос приведет к очередному срабатыванию CI-скрипта для обновления  `issues.md`  , что создаст еще один pull request, и так далее :) Я решил не изобретать самому себе проблемы и добавил ручное обновление  `issues.md`   в список релизных процедур для своих проектов.\n\nЕсть и другой способ: можно выгрузить задачи и обсуждения из GitHub в git-хранилище проекта. Это можно сделать с помощью специального трекера  git-bug   (спасибо  Сергею Бронникову   за ссылку):\n\nПроект git-bug\n\nВ итоге после создания резервной копии информации из GitHub issues и pull requests я решил создать копии своих проектов на Codeberg и затем просто выполнять операцию  `git push`   одновременно и для GitHub, и для Codeberg. Платформа Codeberg успешно выполнила разовую миграцию моих проектов из GitHub, даже скопировала все обсуждения и задачи. Так что это было бы идеальным решением для меня, если бы в Codeberg было включено зеркалирование… Увы!\n\nТогда передо мной возник вопрос: что делать с устаревающей информацией в Codeberg issues? В крайнем случае можно было бы вручную удалять и воссоздавать проекты на Codeberg, чтобы поддерживать актуальность, но такое решение мне совсем не нравилось.\n\nСхема зеркалирования GitHub -> Codeberg\n\nЯ стал изучать настройки репозиториев и нашел обходной путь: Codeberg позволяет использовать внешний трекер задач! Я задал ссылку на задачи в GitHub и формат нумерации, отключил запросы на слияние в Codeberg, и теперь зеркала моих проектов просто ссылаются на issues и pull requests в GitHub.\n\nНастройка репозитория Codeberg\n\nВот это уже более или менее рабочее решение. Если что-то пойдет не так с GitHub, то я пересоздам репозитории в Codeberg, включу в них внутренний трекер и запросы на слияние, после чего анонсирую для сообщества и дистрибутивов GNU/Linux, что разработка моих открытых проектов переведена на эту платформу.\n\nФинальная схема зеркалирования GitHub -> Codeberg\n\nСпасибо за внимание. Буду рад комментариям и предложениям.","id":75}
{"Host":"https://habr.com","Path":"/ru/news/697270/","Text":"Минцифры рассказало Хабру о введении 5G в России / Хабр                 \n\nЗа последние несколько месяцев от сотрудников Минцифры и представителей отечественных операторов вышло несколько слегка противоречивых заявлений по поводу перехода на 4G и 5G в России. Например, судя по заявлению  заместителя министра цифрового развития, связи и массовых коммуникаций РФ Бэллы Черкесовой, на 2024 год уже запланирован запуск 5G. При этом в докладе директора департамента регулирования рынка телекоммуникаций Минцифры Дмитрия Тура указано , что работа 3G в России продлится как минимум до 2027 года, а «Ростех» в целом собирается перенаправить  субсидии с развития 5G на разработки для LTE. Информационная служба Хабра направила в Минцифры уточняющие вопросы по ситуации, на которые ведомство согласилось ответить.\n**Какие реальные сроки начала перехода РФ с 3G на 4G и с 4G на 5G? **\nЗа последние годы практически во всём мире проявилась отчётливая тенденция на вывод из эксплуатации сетей мобильной связи третьего поколения (3G). Согласно прогнозу GSMA, на 2025 год доля соединений в сетях 3G сократится в 8 раз по сравнению с 2020 годом и даже станет меньше доли 2G почти в 2 раза.\nОператоры связи уже проводят постепенный перевод спектра и оборудования из 3G в LTE (4G). Сроки окончательного выключения сетей 3G определяются каждым оператором самостоятельно. Согласно нашим прогнозам, срок полного вывода сетей 3G для крупных городов оценивается не ранее 2025 года и начиная с 2027 года вне крупных городов.\nСети LTE уже развёрнуты всеми операторами и полноценно работают во всех субъектах РФ, сети 3G продолжают использоваться для передачи голоса и увеличения зоны покрытия. На горизонте 2030 года экономической целесообразности перехода от LTE к 5G нет, поскольку доля устройств с поддержкой LTE продолжит свой рост.\n**В рамках программы «Цифровая экономика» к 2022 году должно быть обеспечено устойчивое покрытие сетью 5G десяти городов-миллионников, к 2024 году — всех городов с населением более миллиона человек. Эти сроки всё ещё реальны? **\nДействительно, в программе «Цифровая экономика», которая была утверждена ещё в 2017 году, были установлены такие целевые показатели. Но затем данная программа утратила актуальность из-за утверждения национального проекта «Национальная программа «Цифровая экономика Российской Федерации» (утв. президиумом Совета при Президенте РФ по стратегическому развитию и национальным проектам, протокол от 04.06.2019 № 7), в котором задачи по внедрению 5G были пересмотрены.\nВ частности, по данному национальному проекту была поставлена и реализована задача организации пилотных проектов по созданию сетей связи 5G в Российской Федерации в 5 отраслях экономики, в том числе на территории не менее 1 города с населением более 1 млн человек к концу 2020 года.\nВместе с тем, мы прекрасно понимаем, что с учётом текущих реалий развёртывание в 2024 году полноценных сетей связи пятого поколения на базе российского оборудования во всех городах-миллионниках невозможно. Поэтому цели федерального проекта и корректируются исходя из реальной ситуации.\n**Как на текущий момент продвигается тестирование 5G в России? **\nОператоры связи продолжают разворачивать отдельные пилотные площадки, тестируют различные сервисы и технические варианты реализации. Следует отметить, что наибольшее количество проектов разворачивается в Москве и Санкт-Петербурге, в них используется оборудование в диапазонах 4,8-4,99 ГГц и 25,25-29,5 ГГц. До 2024 года предстоит серьёзная работа с российскими производителями для совместной доработки решений.\n**Все ли операторы заинтересованы в переходе на 5G? **\nЗаинтересованность в строительстве пилотных проектов обозначили все операторы большой четвёрки, а также некоторые региональные участники. Вместе с тем, надо понимать, что подобные проекты требуют серьёзных инвестиций, и с учётом принятых решений по использованию только отечественного оборудования переход будет долгим и поэтапным, исходя из экономической обоснованности.\n**Если сроки реальны, возможно ли, что придётся переходить с 3G сразу на 5G, или сразу после перехода на 4G снова обновляться? В недавнем материале газеты «Коммерсантъ»** **указано** **, что вы прогнозируете сохранение 3G по меньшей мере до 2027 года.**\nПереход на сети связи LTE (4G) мы считаем состоявшимся, при этом отдельные сегменты 3G сохранятся на ближайшие 3-5 лет. При этом работа сетей GSM продолжится, скорее всего, и после 2027 года. Эти сети необходимы для оказания голосовых услуг, поскольку 100% проникновение VoLTE не ожидается даже в среднесрочной перспективе и отдельные решения для межмашинной коммуникации также не поддерживают сети LTE.\nВместе с тем, в настоящее время существует задача по проведению исследований технологий уже 6G и 7G. И вполне возможно, что переход с 4G будет осуществляться именно на них.\n**Что сдерживает переход? Если вопрос в оборудовании, то помогает ли справиться с его дефицитом параллельный импорт? Или ожидается, что переход на 4G и 5G будет осуществляться исключительно на отечественном оборудовании? **\nВ соответствии с решением Совета Безопасности строительство коммерческих сетей связи 5G возможно исключительно на отечественном оборудовании, включённом в соответствующие реестры радиоэлектронной продукции Минпромторга России программного обеспечения Минцифры России. Будущие сети 4G по мере готовности также будут на отечественном оборудовании. Операторы связи до конца года заключат соответствующие «форвардные» контракты с российскими разработчиками.\n**Также есть вопрос от пользователей Хабра по поводу выделенных под 5G частот. Как указал Максут Игоревич, для 5G в России выделили 4,4-4,9 ГГц, однако в других странах иные частоты. Не приведёт ли это к тому, что на зарубежных устройствах, ввозимых в РФ, не будет поддерживаться 5G? Возможно ли, что диапазон пересмотрят?**\nВ части абонентских терминалов в целом существует практика ориентированности модели на определённые рынки. Один из наиболее популярных диапазонов — 3,4-3,8 ГГц — действительно задействован у нас для иных целей и не может быть выделен под 5G. Значительное количество моделей телефонов уже поддерживает диапазон 4,4-4,9 ГГц, мы надеемся на постепенное увеличение их ассортимента. По практике развития LTE поддержка диапазонов устройствами за 5 лет в значительной степени выравнивается.\nДля внедрения 5G в России также планируются и другие полосы радиочастот, в частности, 24,25 27,5 ГГц, и диапазон 700 МГц, где в настоящее время с разной степенью интенсивности работают сети телерадиовещания.","id":76}
{"Host":"https://habr.com","Path":"/en/company/plarium/blog/453310/?mobile=no","Text":"Обмен данными между React-компонентами с использованием библиотеки RxJS / Habr               \n\n24-05-2019\nПеред вами перевод статьи Chidume Nnamdi, опубликованной на blog.bitsrc.io. Перевод публикуется с разрешения автора. Появление библиотеки RxJS открыло массу новых возможностей в мире JS. Цель RxJS — достигать многого, используя небольшое количество кода. Прочитав эту статью, вы узнаете, как осуществлять обмен данными между компонентами приложения на React, применяя возможности RxJS. Совет: используйте Bit для организации React-компонентов и обмена ими. Это позволит вашей команде быстрее разрабатывать свои приложения. Просто попробуйте. React Components Collection Redux Обмен данными между несвязанными React-компонентами — это то, ради чего были созданы библиотеки управления состояниями. Существует множество шаблонов для управления состояниями, но наиболее известны два: Flux и Redux. Redux популярен из-за своей простоты и использования чистых функций. Ведь благодаря им можно не сомневаться, что применение редьюсеров не приведет к каким-то побочным эффектам. Работая с Redux, первым делом мы создаем централизованное хранилище данных: Далее связываем компоненты с этим хранилищем и при желании обновляем или удаляем состояние. Любые изменения, внесенные в хранилище, будут отражены в компонентах, связанных с ним. Таким образом, поток данных распространяется на все компоненты, независимо от степени их вложенности. Компонент, находящийся на энном уровне иерархической структуры, способен передавать данные компоненту самого высокого уровня. Последний, в свою очередь, может передавать данные компоненту 21 уровня. RxJS С появлением RxJS использовать библиотеки управления состояниями стало гораздо проще. Многим понравился паттерн «Наблюдатель» (Observer), предоставляемый RxJS. Мы просто создаем поток Observable и даем возможность всем компонентам прослушивать его. Если какой-то компонент добавляется к потоку, прослушивающие (или «подписанные») компоненты реагируют на обновление DOM. Установка Создаем приложение на React, используя create-react-app. Если у вас нет create-react-app, то сперва установите его глобально: npm i create-react-app -g Далее генерируем проект в React: create-react-app react-prj Переходим в директорию: cd react-prj Устанавливаем библиотеку rxjs: npm i rxjs У нас должен появиться файл, создающий новый экземпляр BehaviourSubject. Почему мы используем BehaviorSubject? BehaviorSubject — это один из Subject в библиотеке RxJS. Будучи дочерним компонентом Subject, BehaviorSubject позволяет множеству наблюдателей прослушивать поток, а также делает массовую рассылку событий этим наблюдателям. BehaviorSubject сохраняет последнее значение и передает его всем новым подписанным компонентам. Таким образом, BehaviorSubject: Позволяет осуществлять массовую рассылку. Хранит последние значения, опубликованные подписчиками, и делает массовую рассылку этих значений. В папке src находится файл messageService.js, экспортирующий подписчику экземпляр BehaviorSubject и объект messageService. Объект-подписчик создается в начале файла — так он доступен для любого импортирующего компонента. У объекта messageService имеется функция отправки, принимающая параметр msg: в нем содержатся данные, которые нужны для передачи всем прослушивающим компонентам. В теле функции мы вызываем метод emit. Он осуществляет массовую рассылку данных подписанным компонентам в объекте-подписчике. Предположим, что у нас есть следующие компоненты: ConsumerA; ConsumerB; ProducerA; ProducerB. В иерархической структуре они выглядят так: Компонент приложения передает сообщение ProducerA и ConsumerB. ProducerA отправляет данные ConsumerA, а сообщение от ConsumerB попадает к ProducerB. Компоненты ConsumerA и ConsumerB имеют индивидуальный счетчик состояния. В их методе componentDidMount они подписаны на один и тот же поток subscriber. Как только публикуется какое-либо событие, у обоих компонентов обновляется счетчик. У ProducerA и ProducerB есть кнопки Increment Counter и Decrement Counter, которые при нажатии выдают 1 или -1. Подписанные компоненты ConsumerA и ConsumerB подхватывают событие и запускают свои функции обратного вызова, обновляя значение счетчика состояния и DOM. Посмотрим на иерархическую структуру еще раз: ProducerB передает данные ConsumerA, хотя они абсолютно не связаны. ProducerA передает данные ConsumerB, не являясь его родительским компонентом. В этом вся суть RxJS: мы просто создали центральный узел потока событий и позволили компонентам прослушивать его. Когда какой-либо компонент генерирует события, прослушивающие компоненты тут же подхватывают их. Поиграть с приложением можно на stackblitz: https://react-lwzp6e.stackblitz.io Заключение Итак, мы увидели, как можно осуществлять обмен данными между React-компонентами, применяя RxJS. Мы использовали BehaviourSubject для создания централизованного потока данных, а затем позволили остальным компонентам подписаться на этот поток. Теперь, когда один из компонентов генерирует данные, прочие компоненты также получают их. Уровень компонентов в иерархической структуре неважен. Если у вас есть вопросы относительно этой темы или вы хотите, чтобы я что-то добавил, исправил или удалил, напишите об этом в комментариях, в электронном письме или в личном сообщении. Спасибо за внимание!\n","id":77}
{"Host":"https://habr.com","Path":"/en/companies/advertone_ru/articles/231589/","Text":"5 историй о слияниях и поглощениях технологических рекламных стартапов / Habr             \n\nРынок онлайн-рекламы воспринимается многими ИТ-специалистами, как недостаточно интересный: создание систем для показа баннеров и рекламных объявлений не кажется им привлекательным занятием. Некоторые инвесторы также не видят смысла во вложениях в подобные стартапы.\n\nТем не менее, данная отрасль является довольно продвинутой с точки зрения технологий, а компании, оперирующие на этом рынке, создают по настоящему инновационные решения, которые привлекают таких гигантов, как Google, Twitter и Yahoo. Сегодня мы поговорим о некоторых историях покупки, слияний и поглощений, рекламных стартапов.\n\n#### Google и Admeld ####\n\nРекламная биржа Google под названием Double Click Ad Exchange когда-то была отдельным SSP-провайдером, предоставлявшем платформу по оптимизации размещений рекламы под названием Admeld. Эта система «умеет» определять рекламный инвентарь, который продается хуже, и в режиме реального времени заменять его на более эффективные инструменты, что позволяет повысить ROI рекламных кампаний. Подобная функциональность привлекла в число клиентов таких известных паблишеров (площадки для размещения рекламы), как Discovery, FOX News, Huffington Post, IAC, Thomson Reuters и WWE.\n\nСумма сделки\nсоставила\n\\$400 млн. После внедрения поддержки технологии RTB, эффективность рекламной сети Google значительно повысилась — продажи рекламного инвентаря выросли с 8% в 2010 году до 68% в 2011.\n\n#### Приобретения Rubicon ####\n\nВедущий игрок рынка онлайн-рекламы Rubicon Project в своей истории активно прибегал к покупке технологий и целых стартапов — именно эта тактика позволила компании достигнуть значительного успеха.\n\nПосле своего основания в 2009 году и привлечения первых инвестиций в размере \\$33 млн, Rubicon, специализировавшаяся в то время на A/B тестировании рекламных сетей и форматов баннеров,\nприобрела\nстартап OthersOnline – он развивал технологию для составления профайлов пользователей сети (сумма не называлась). Клиентами этого проекта были паблишеры. Покупка была совершена с целью получения возможности по объединению информации от разных поставщиков данных и создания наиболее полного представления аудитории, которое могло бы заинтересовать рекламодателей.\n\nНа этом поглощения со стороны Rubicon не закончились — в 2010 году был\nкуплен\nпроект в области информационной безопасности Site Scout. Технологии стартапа были использованы для реализации решения, которое позволяет паблишерам избегать размещения на своих ресурсах зловредной рекламы (malvertising).\n\nЗатем компания\nпоглотила\nеще и популярную рекламну юсеть Fox Audience Network, обладающую уникальными технологиями показа дисплейной рекламы, а в 2012 году для выхода на мобильный рынок была совершена\nсделка\nпо приобретению рекламного стартапа Mobsmith.\n\nВ результате приобретения технологических компаний и интеграции их разработок в собственные системы, Rubicon стала одним из лидеров рынка онлайн-рекламы (по\nоценке\nForrester).\n\n#### Сделка между Acxiom и LiveRamp ####\n\nЗнаменитая глобальная корпорация, занимающаяся сбором и анализом данных для реализации более точно таргетированных рекламных кампаний корпоративных клиентов, в мае 2014 года за \\$310 млн\nприобрела\nстартап LiveRamp, разрабатывающий технологию для загрузки клиентских данных в приложения для цифрового маркетинга.\n\nAcxiom и LiveRamp вместе взятые обладают связями с 7000 глобальных заказчиков и технологических партнеров. В результате поглощения, объединенная компания сможет охватывать высокоточными рекламными объявлениями до 99% взрослого населения США с помощью различных каналов и на различных устройствах. В течение года с момента сделки, Acxiom и LiveRamp рассчитывают добиться таких же показателей за пределами в США — в Европе и Азиатско-тихоокеанском регионе.\n\n#### Twitter и покупка Mopub ####\n\nВ сентябре 2013 года сервис микроблогов Twitter\nкупил\nстартап в области мобильной рекламы MoPub. Цель сделки заключалась в расширении возможностей рекламной платформы серфиса микроблогов и создании системы автоматизированной покупки рекламы на ней по технологии RTB. Сделка стала одной из крупнейших в истории Twitter — по оценкам издания The Financial Times её сумма могла составить от \\$300 до \\$400 млн.\n\nНа этой покупке руководство соцсети не остановилось. В начале июля 2014 было\nобъявлено\nо приобретению компании TapCommerce, специализирующейся на мобильной рекламе. Данный сервис позволяет в режиме реального времени повторно привлекать клиентов, загрузивших какое-либо приложение на свой телефон. На платформе TapCommerce работает, к примеру, мобильная версия eBay. Сумма сделки не раскрывается, но по данным издания Re/code, она\nсоставила\n\\$100 млн.\n\n#### Двойная сделка Amobee ####\n\nВ июне 2014 года компания Amobee, занимающаяся мобильной рекламой и ранее\nкупленная\nза \\$321 млн сингапурским сотовым оператором SingTel объвила о двойном поглощении. Частью компании (и, соответственно, SingTel)\nстали\nтехнологические проекты Adconion (сумма сделки \\$235 млн) и Kontera (\\$150 млн).\n\nAdconion после нескольких изменений стратегии бизнеса, сфокусировалась на работе в нише рекламы в онлайн-видео (эта область довольно перспективна, недавно Facebook\nкупила\nвидеорекламный стартап LiveRail за сумму \\$400-500 млн), число клиентов, использующих её платформу превышает 2000. Стартап Kontera занимался разработкой технологий мобильной рекламы и аналитики.\n\nКак заявили представители Amobee, поглощение этих двух проектов было обусловлено растущей конкуренции на рекламном рынке, которая вынуждает игроков консолидировать активы, для получения возможности предоставления более широкого спектра услуг своим клиентам.\n\nВ 2013 год Amobee также\nкупила\nстартап Gradient X, разработавший платформу для мобильной RTB-рекламы\n\n#### Выводы ####\n\nОпыт зарубежных компаний свидетельствует о том, что рынок онлайн-рекламы, на протяжении нескольких этапов своего развития, пришел к необходимости повышения эффекивности работы систем. Часто эта цель достигается с помощью реализации сервисов с применением технологии аукциона в реальном времени (RTB), который позволяет рекламодателям использовать одну систему для покупки рекламы на различных площадках, а паблишерам — продавать имеющиеся виртуальные «рекламные площади» с наибольшей эффективностью.\n\nПри этом, нужды современных рекламодателей довольно широки, поэтому рекламные компании задумываются не только о повышении результативности размещений, но и о предоставлении сопутствующих услуг — автоматический контроль репутации (Brand Safety), противодействие зловредной рекламе (adware) и т.п. Часто реализовать эти проекты своими силами не представляется возможным, поэтому компании готовы платить большие деньги за покупки сервисов, обладающими подобными технологиями. Это (и суммы, которые западные гиганты платят за нужные компании), что ниша рекламных стартапов очен перспективна.\n\n**Посты и ссылки по теме:**\n\nИстория рынка RTB: этапы развития и появление основных игроков\n\nДивный новый мир: мобильные операторы, как крупнейшие поставщики рекламных данных\n\nРекламные войны: Поисковики против интернет-провайдеров\n\nA/B-тестирование рекламных сетей (сервис Advertone)","id":78}
{"Host":"https://habr.com","Path":"/en/post/357298/?mobile=no","Text":"Суд Миннесоты обязал Google предоставить полиции логи поисковых запросов жителей целого города / Habr             \n\n18-03-2017\nСовременные технологии помогают повысить уровень и качество жизни современного человека. Hi-tech появляется во всех сферах нашей жизни, включая и криминальную составляющую. Так, злоумышленники нередко используют высокотехнологичные гаджеты или софт, причем речь сейчас идет не о киберпреступниках, а об «обычных» преступниках. Но и правоохранители разных стран не спят, стараясь идти в ногу с прогрессом. У тех, кто защищает общество от преступлений, постепенно появляются новые устройства и методы, которые помогают раскрывать злодеяния быстрее и эффективнее, чем раньше. Один из простых способов — анализ поисковых запросов на компьютере или телефоне предполагаемого преступника. Многие злоумышленники не предпринимают мер предосторожности, планируя что-либо при помощи своего ПК, планшета или телефона. В процессе анализа этих устройств полицейским иногда удается получать важные улики. Помогает и анализ сохраненных провайдерами логов. Иногда правоохранители в судебном порядке требуют у технологических компаний предоставить определенные данные о своих пользователях. Именно это случилось в городе Эдина, Миннесота, США. Правда, здесь полицейские запросили информацию не о каком-либо жителе. Полицейское управление Эдины обратилось в суд с требованием к Google предоставить информацию о поисковых запросах жителей своего города. Забегая наперед, стоит заметить, что речь идет не вообще обо всех подряд запросах, а только тех, которые имеют отношение к определенному человеку. То есть запросах, в тексте которых содержится имя этого человека. Все началось с того, что один из банков штата получил в январе этого года звонок от своего клиента с фамилией Дуглас. Он попросил клерка перевести $28500 со своего счета в этом банке на счет в другом банке. Для подтверждения транзакции Дугласа попросили прислать по факсу копию своего паспорта. Как оказалось, присланная копия паспорта — фальшивка, а сам запрос сделал человек, который не имеет никакого отношения к счету. Банк известил о случившемся правоохранителей, которые и занялись расследованием происшедшего. Изучив все полученные улики, полицейские начали искать изображение, использованное при создании фальшивой копии паспорта, и нашли его при помощи Google Images. В поисковых сервисах Bing и Yahoo эта же фотография не отыскалась, так что полиция сделала вывод о том, что злоумышленник искал фото через Google. После этого у полицейских появилась мысль дать запрос Google с целью получить определенные поисковые ключевые слова, сделанные жителями города в определенное время. Соответствующий запрос был подан не компании, а суду. По закону, компании CША могут выдавать личные данные своих клиентов или сторонних лиц только по запросу полиции. Судья одобрил запрос, и Google обязали предоставить эту информацию за период в пять недель. Google должен дать правоохранителям IP, e-mail и данные учетных записей тех жителей города, кто искал фамилию Дуглас в указанном временном интервале. Вроде бы правильный ход со стороны полиции, но теперь правозащитники и специалисты по информационной безопасности бьют тревогу. В самом деле, что, если кто-то искал фамилию своего родственника или знакомого — ведь фамилия Дуглас достаточно широко распространена в США? Здесь даже в пределах небольшого города можно найти несколько Дугласов. И что будет делать полиция, когда получит нужную информацию? Пойдет по домам пользователей, кто вводил указанный поисковый запрос с целью ареста? В США прецедентное судебное право, и решение судьи может стать началом большого количества обращений правоохранителей в суд с целью получения аналогичной информации в других городах. Уже, конечно, не в отношении некоего Дугласа, а по другим делам. Поскольку дело разрабатывается, полиция не рассказывает о промежуточных результатах расследования. Пока что неясно, согласится ли Google выдать эту информацию, или откажется, мотивируя недостаточно четким постановлением суда. Тем не менее, вполне вероятно, что это дело может превратиться в нечто гораздо более масштабное и значительное, чем простой поиск человека по запросам, которые он когда-то сделал. В целом, технологические компании, на серверах которых хранятся данные тысяч или даже миллионов человек, не слишком охотно сотрудничают с полицией, считая, что частная информация должна оставаться неприкосновенной. Например, еще в октябре 2015 года представители Apple отказались выполнять требование полиции разблокировать телефон Джана Фенга (Jun Feng), признавшего себя виновным в продаже наркотического вещества. Сейчас на многих устройствах, включая телефоны, включено шифрование по умолчанию. Google и Apple, например, еще в 2014 году включили дефолтное шифрование данных, обрабатываемых iOS и Android. Понятно, что правоохранителям все это не слишком нравится. В том же 2014 году представители ФБР даже назвали позицию этих компаний антиобщественной. Директор ФБР Джеймс Коуми тогда заявил, что технологии стали инструментом в руках очень опасных людей, а законы отстали от технологического прогресса. «У нас есть юридическое право перехватывать и прослушивать коммуникации и получать информацию по судебному ордеру, но часто отсутствует техническая возможность делать это», — заявил Коуми. Станет ли текущая ситуация продолжением противостояния технологических компаний и полиции, пока неясно. Но уже понятно, что решить проблему в скором времени вряд ли получится.\n","id":79}
{"Host":"https://habr.com","Path":"/en/companies/pt/articles/308064/","Text":"Cisco и Fortinet подтвердили наличие уязвимостей, опубликованных хакерами The Shadow Brokers / Habr                                      \n\nВ конце прошлой недели (13 августа) в сети\nпоявилась\nинформация о том, что кибергруппировка The Shadow Brokers выложила на продажу архив данных, украденных у другой хакерской группы Equation Group. Последнюю многие наблюдатели и эксперты по безопасности связывают с АНБ, в частности предполагается, что именно ее представители работали над наиболее известным кибероружием — включая червей Stuxnet, Flame, Duqu и другими.\n\nКак утверждалось, среди украденных у Equation Group программ содержались уязвимости для продуктов Cisco, Juniper и Fortinet, а также нескольких китайских производителей сетевого оборудования. Теперь появились подтверждения того, что хакеры создали эксплоиты для уязвимостей в маршутиазаторах этих вендоров. Причем в продукции Cisco обнаружен 0-day, бюллетень безопасности для которого\nпоявился\nтолько 17 августа.\n\n### Что произошло ###\n\nГруппировку Equation Group связывают с проникновениями в компьютерные сети госучреждений, телекоммуникационных компаний и других структур в разных странах — например, «Лаборатория Касперского» в 2015 году\nпубликовала\nинформацию об атаках хакеров на цели в России, Ираке и Иране. Кроме того, считается, что именно эта кибергруппировка тесно связана с вирусом Stuxnet, который был разработан по заказу Агентства национальной безопасности США для борьбы с ядерной программой Ирана.\n\nВ свою очередь, хакеры из Shadow Brokers сумели атаковать ресурсы Equation Group и получить доступ «к полному государственному набору кибероружия». Похитители опубликовали\nсообщение\n, в котором заявили о том, что продадут лучшие из украденных программ покупателю, предложившему наибольшую цену.\n\nПомимо платной части архива, хакеры выложили в свободный доступ его небольшую часть. По заявлению Shadow Brokers, если им удастся собрать 1 млн биткоинов (~\\$568 млн), то для бесплатного скачивания будет выложена некоторая часть украденных шпионских программ.\n\n_Скриншот архива Shadow Brokers_\n\n### При чем здесь Cisco и другие вендоры ###\n\nВ качестве доказательства того, что хакерам действительно удалось похитить «кибероружие», представители Shadow Brokers опубликовали код, который по их утверждениям предназначается для обхода систем безопасности маршрутизаторов популярных вендоров.\nОпрошенные\nжурналистами эксперты, которые ознакомились с представленным кодом, сообщили, что код действительно функционален и может решать задачу манипуляции трафиком и его перенаправления в момент прохода через маршрутизатор.\n\nПо данным The Wall Street Journal, опубликованный фрагмент кода применим к маршрутизаторам компаний Cisco, Juniper и Fortinet, а также китайских вендоров Shaanxi Networkcloud Information Technology и Beijing Topsec Network Security Technology.\n\nПозднее представители Cisco\nподтвердили\n, что эксплоиты представленные в украденном архиве под названиями EPICBANANA, JETPLOW и EXTRABACON используют уязвимости в продуктах компании. Под угрозой оказались маршрутизаторы ASA, файрволлы PIX и Cisco Firewall Services Modules.\n\nПри этом, если одна из использующихся уязвимостей (\nCVE-2016-6367\n) была устранена еще в 2011 году, то патч для второй (\nCVE-2016-6366\n) 0-day ошибки\nпоявился\nтолько 17 августа. Баг связан с реализацией протокола SNMP и может дать атакующему root-права в системе, что позволит осуществить выполнение произвольного кода (RCE).\n\nСМИ\nсообщают\n, что эксплоит Extrabacon эксплуатирует 0-day-уязвимость, бюллетень безопасности для которой был опубликован совсем недавно. В коде эксплоита есть ограничение на его исполнение по версиям с 8.0(2) по 8.4(4). Как выяснили эксперты Positive Technologies, на более новых версиях — например, 9.2(2), скрипт при определенной доработке может успешно срабатывать.\n\nЭксперты по безопасности уже успели\nпроверить\nработоспособность представленных эксплоитов и\nподтверить\n, что они действительно могут быть использованы для осуществления кибератак.\n\n### Как защититься ###\n\nПоскольку полноценного патча для этой уязвимости пока не существует, для повышения уровня безопасности представители Cisco рекомендуют использовать\nправила Snort\n. В свою очередь, компания Fortinet выпустила собственный\nбюллетень безопасности\n, а исследователи\nопубликовали утилиту\nдля проверки наличия уязвимости в оборудовании этого вендора.\n\nЭксперты Positive Technologies рекомендуют установить все последние обновления безопасности, опубликованные вендорами, а также исполнить рекомендации, содержащиеся в бюллетенях безопасности. Кроме того, защититься от атак с использованием обнаруженных уязвимостей можно с помощью специализированных средств защиты. К примеру, система контроля защищенности и соответствия стандартам\nMaxPatrol 8\nуспешно обнаруживает обе описанные уязвимости в оборудовании Cisco.\n\nЭксперты Positive Technologies разработали сигнатуру для Suricata IDS, позволяющую обнаружить использование уязвимостей Cisco и предотвратить возможные последствия. Сигнатуру можно найти в официальном репозитории\ntwitter.com/AttackDetection/status/766413545672146945\n. Кромего того, были сгенерированы все доступные для эксплоита EPICBANANA пэйлоады и разработано правило для обнаружения атак с его использованием:\ntwitter.com/AttackDetection/status/766695730694291456\n.","id":80}
{"Host":"https://habr.com","Path":"/en/company/orienteer/blog/312844/?mobile=yes","Text":"Свой BaaS c моделированием предметной области, скриптами и многим другим за полчаса / Habr            \n\n17-10-2016\nСегодня расскажем и покажем как за полчаса поднять свой Backend as a Service (BaaS) с весьма интересными возможностями. BaaS — это веб-приложение, которое работает в облаке и предоставляет все необходимое для бизнес/мобильных приложений и сайтов (front-end). BaaS как минимум позволяет: Управлять пользователями и ролями Моделировать предметную область Получать доступ к данным через REST Управлять самими данными (база данных) Вот здесь уважаемый yurash собрал основные на 2012 год BaaS-системы. Сейчас их значительно больше, но в плане вопросов, которые задают люди о BaaS-платформах, статья актуальна. Кстати, если интересует текущий рынок BaaS (и BAP — business application platform), то могу поделиться в следующей статье. Что берем для создания BaaS Orienteer — конструктор бизнес-приложений (business application platform) с открытым исходным кодом. В своей основе Orienteer использует OrientDB — NoSQL мульти-парадигмная база данных с крутыми возможностями прямо из коробки. Docker Cloud — свободная платформа для развертывания в облаке контейнеров с приложениями Docker. На Хабре много статей про Docker. BaaS на основе Docker-Orienteer позволяет быстро создавать приложения и быстро вносить изменения в уже имеющиеся приложения. При этом у системы низкий порог вхождения: простейшее приложение может создать человек, не знакомый с базами данных и программированием. Связка BaaS-Docker может быть полезна и сервис-провайдерам, малым дизайн студиям и менеджерам in-house дата-систем. Что получим в итоге Конструктор модели данных Широкий список типов данных: от примитивных (INTEGER, STRING и т.п.) до комплексных (LINK, EMBEDDED, LINKMAP, EMBEDDEDMAP), SPATIAL Экранные формы и виджеты для создания объектов спроектированной модели Управление пользователями и ролями Серверные скрипты на JavaScript/SQL REST-интерфейс с динамическим добавлением/изменением/удалением своих функций Проектирование и исполнение бизнес-процессов BPMN на основе Camunda Модульная архитектура для гибкого расширения через java, если очень понадобится Развертываем Вот он наш план на ближайшие полчаса Регистрируемся на Docker Cloud Регистрируемся на Digital Ocean (AWS, Azure и т.п.) Связываем Docker Cloud и Digital Ocean: создаем свою ноду Создаем и запускаем свой сервис Orienteer Настраиваем под себя Регистрация на Docker Cloud Docker Cloud в плане процедуры регистрации не отличается чем-то особенным от других сервисов в интернете. Заходим на Docker Cloud Придумываем Docker ID (aka username). Рекомендую использовать только нижний регистр со знаком “-”: если в будущем захотите использовать Docker весьма тесно и будете выпускать свои образы для контейнеров, будет проще адресовывать Задаем email и пароль Подтверждаем email, и всё: вы зарегистрированы! Регистрация на Digital Ocean Идем на Digital Ocean. Рекомендую использовать эту реферальную ссылку, так как это даст вам дополнительные $10 Регистрация еще проще чем на Docker Cloud: задаете email и пароль При регистрации рекомендую использовать promo-code от Docker Cloud, который даст вам еще $20. Чтобы узнать его, зайдите в Cloud Settings на Docker Cloud. Вы увидите что-то типа этого: Подтверждаете email и всё: вы зарегистрированы Связываем Digital Ocean и Docker Cloud Идем на знакомую страницу Docker Cloud > Cloud Settings (см. картинку выше) В строке Digital Ocean нажимаем на перечеркнутую розетку Авторизуем Digital Ocean для Docker Cloud’а (то есть, вводим email и пароль докера) Готово Создаем свой сервис Orienteer Прежде всего надо создать как минимум одну ноду, на которой будем запускать наш BaaS. Для этого на Docker Cloud: Идем в Node Clusters и нажимаем Create В пункте Providers выбираем Digital Ocean Указываем регион Выбираем тип ноды. Рекомендую выбирать с не менее чем 1 Гб памяти Жмем Create. Вы увидите что-то вроде картинки ниже. Docker Cloud сам закажет, установит, настроит и запустит ноду на Digital Ocean Осталось совсем чуть-чуть: запустить свою копию Orienteer из образа Docker. Orienteer — гомогенное приложение: все свое носит с собой. Поэтому создавать Docker Stack не понадобится. Нужен лишь сам сервис. Идем в Services и нажимаем Create В предложенном списке сверху выбираем Public Services В блоке слева вводим критерий поиска: orienteer/orienteer Выбираем именно orienteer/orienteer и попадаем на такую страницу: Из всего множества настроек единственное, что обязательно необходимо настроить — это порты, доступные снаружи. Например, так: Жмем Create & Deploy Придется подождать, пока образ Docker будут скачан на нашу ноду и запущен. Обычно это занимает не больше 3 минут. В итоге вы увидите что-то как на картинке ниже. Жмем на ссылку и попадаем на наш собственный BaaS на основе Orienteer. Чтобы войти в систему используйте логин/пароль по умолчанию: admin/admin Настраиваем под себя Для настройки крайне рекомендую обратиться к официальной документации, а в частности к главе по созданию собственного бизнес приложения (в формате walkthrough). Настройку под конкретное ваше приложение можно условно разделить на следующие шаги: Создать модель данных Сконфигурировать модель ролей и пользователей Раздать права по ролям/пользователям Добавить необходимые скрипты исполняемые на серверной стороне Настроить интерфейс: виджеты, отчеты, графики, перспективы и т.д. Нет предела совершенству, поэтому доводить напильником можно до бесконечности: вводя свои виджеты, свои форматы экспорта, бизнес процессы, печатные отчеты, нотификации и т.д. Если есть вопросы по дальнейшей настройке под ваши нужды: всегда рады помочь!\n","id":81}
{"Host":"https://habr.com","Path":"/en/company/ibm/blog/265413/?mobile=no","Text":"LinuxONE: мейнфрейм от IBM, работающий только с Linux. Подробности проекта / Habr             \n\n25-08-2015\nКорпорация IBM, одновременно с заявлением о расширении стратегии использования на мейнфреймах технологий с открытым исходным кодом и других решений сообщества разработчиков Open Source, представила два новых мейнфрейма в линейке LinuxOne. Мейнфреймы позиционируются, как самые защищённые Linux-системы в своей отрасли. Плюс ко всему, это еще и самые мощные безопасные корпоративные сервера, которые адаптированы для работы с гибридными облачными технологиями и мобильными приложениями. IBM приняла решение открыть код мейнфреймов для сообщества разработчиков открытого программного обеспечения. В частности, была раскрыта технология мейнфрейма IBM, предназначенная для обнаружения проблем и предотвращения сбоев в работе системы предприятия, повышения производительности на всех платформах и обеспечения лучшей интеграции с более широкой сетью и облаком. Мейнфремы LinuxOne Как уже говорилось выше, запущено два сервера из линейки LinuxOne. Первый, получивший название LinuxOne Emperor, построен на базе мейнфрейма IBM z13 с его процессором Z13. Система предназначена для работы в крупных корпорациях, для решения большого количества бизнес-задач. Второй — Rockchopper, предназначается для среднего бизнеса. Он работает c процессором Z12. Конфигурация LinuxONE Emperor: • частота работы 8-ядерного Z13 — вплоть до 5 ГГц; • до 141 процессоров; • 10 ТБ совместно используемой памяти; • 640 процессоров ввода/вывода; • Поддержка вплоть до 8000 виртуальных машин. При тестировании система показала результат в 30 млрд RESTful (Representational State Transfer) в день, с использованием Node.js и MongoDB. Таких показателей нет ни у одной отдельной системы Linux. LinuxONE Rockhopper, открывающее список решений, разработано специально для заказчиков из стран с развивающейся экономикой, для которых важна скорость, безопасность и доступность мейнфреймов, но в меньшем корпусе. Отметим, что название мейнфрейма было выбрано не просто так – за основу взято название вида императорских пингвинов, самых больших пингвинов в мире. Императорский пингвин с уверенностью смотрит в будущее мейнфреймов Работать обе системы могут с таким ПО, как Apache Spark, MariaDB, PostgreSQL и Chef. Компания предоставит открытые программные продукты и отраслевые инструменты, в том числе Apache Spark, Docker, Node.js, MongoDB, MariaDB, PostgreSQL и Chef для работы с IBM z Systems, чтобы обеспечить заказчикам более широкий выбор и гарантировать гибкость при развертывании гибридных облачных сред. Canonical и IBM также объявили о запуске совместной инициативы, направленной на продвижение использования Ubuntu Linux на IBM z Systems, открыв для сообщества разработчиков Ubuntu доступ к возможностям мейнфреймов. «Пятнадцать лет назад IBM удивила многих представителей индустрии, установив Linux на мейнфреймах. А сегодня уже более трети заказчиков мейнфреймов IBM работают на Linux, – говорит Том Розамилия, старший вице-президент IBM Systems. – Этот анонс еще раз подтверждает приверженность IBM поддержке сообщества разработчиков Open Source. Объединение лучшей открытой системы с передовыми мощностями позволит заказчикам справляться с новыми мобильными рабочими нагрузками и эффективно управлять вычислениями на гибридных облаках. Учитывая успешный опыт использования Linux на мейнфреймах, мы продолжаем расширять границы возможностей для обычных серверов, у которых нет повышенных требований к безопасности и производительности». Компания предоставит новые открытые программные средства и отраслевые инструменты для работы с мейнфреймами Значительно расширив возможности для предприятий, IBM предоставила LinuxONE и IBM z Systems ключевые технологии открытого программного обеспечения, удовлетворяющие отраслевым стандартам. В их число вошли Apache Spark, Node.js, MongoDB, MariaDB, PostgreSQL, Chef и Docker. Эти технологии могут работать на мейнфрейме так же бесперебойно, как и на других платформах, не требуя при этом дополнительных навыков или преимуществ в показателях производительности. IBM стала инициатором создания виртуальной среды на мейнфреймах. Сегодня компания предлагает большой выбор решений для виртуализации, наделяя новые системы LinuxONE функциями виртуальной машины с помощью гипервизора KVM, который использует набор открытых стандартов, как и любой другой сервер Linux. SUSE, ведущий дистрибьютор Linux, обеспечит первоначальную поддержку KVM для мейнфреймов. Canonical и IBM также объявили о запуске совместной инициативы, направленной на продвижение использования Ubuntu Linux на IBM z Systems. Компания Canonical планирует распространять операционную систему Ubuntu для LinuxONE и z Systems, включив в нее третий дистрибутив Linux. SUSE и Red Hat уже поддерживают такой формат распространения. Canonical также планирует поддержать программное решение KVM для мейнфреймов. Компания присоединится к новому проекту Linux Foundation в условиях роста спроса на мейнфреймы со стороны разработчиков Open Source. Для сообщества разработчиков открытого программного обеспечения расширение доступа к технологиям мейнфреймов IBM стало крупнейшим единовременным открытием кода приложений для мейнфреймов со стороны IBM. Компания также разрешила доступ к исходному коду решений по предиктивной аналитике, которые обеспечивают непрерывный мониторинг для определения необычного поведения системы и минимизации его последствий. Такой код может быть использован разработчиками для создания аналогичной по функциям и по способностям реагирования системы. Вклад IBM поможет запуску нового проекта Open Mainframe Project, организатором которого выступила некоммерческая организация Linux Foundation, занимающаяся развитием экосистемы Linux. IBM совместно с Linux Foundation будет поддерживать Open Mainframe Project, участниками которого являются около десяти организаций из научной сферы, государственного и корпоративного сектора. Цель проекта – разработка, внедрение и продвижение Linux на мейнфреймах. «Технология Linux на мейнфреймах достигла критической массы, поэтому разработчикам, производителям, пользователям и академическим кругам необходима независимая площадка, где они могут работать совместно, чтобы продвигать инструменты и технологии Linux, тем самым привнося инновационные технологии в работу предприятий и индустрий, – отметил Джим Землин, исполнительный директор Linux Foundation. — Open Mainframe Project является ответом на ожидания пользователей Linux. Данный проект нацелен на поддержание развития экосистемы открытого программного обеспечения с целью повсеместного использования уникальных особенностей мейнфреймов для обеспечения безопасности, доступности и производительности». IBM предоставит бесплатный доступ к LinuxONE Developer Cloud В рамках объявления IBM также предоставит доступ к мейнфрейму для разработчиков открытого программного обеспечения. Беспрецедентный шаг компании призван способствовать созданию и продвижению инноваций в этой области. IBM запускает платформу LinuxONE Developer Cloud, к которой сообщество разработчиков получит свободный доступ. Облако выполняет функцию виртуального R&D механизма для создания, тестирования и запуска новых приложений, в том числе проверки совместимости с системами взаимодействия, мобильными и гибридными облачными приложениями. Колледж Марист и Школа информационных наук Сиракьюсского университета разместят облака, с помощью которых разработчики получат бесплатный доступ к виртуальному мейнфрейму IBM LinuxONE. В рамках программы IBM также создаст отдельную облачную платформу для независимых поставщиков программного обеспечения в центрах IBM в Далласе (США), Пекине (Китай) и Бёблингене (Германия). Она предоставит разработчикам доступ и бесплатный пробный период работы с ресурсами LinuxONE для подключения, тестирования и оценки производительности новых приложений для платформ LinuxONE и z Systems. Новые финансовые предложения для портфеля решений LinuxONE предусматривают гибкие условия в ценообразовании и объеме предлагаемых ресурсов. Это позволит предприятиям платить только за то, чем они действительно пользуются, и легко увеличивать мощности в условиях роста бизнеса. Новые системы LinuxONE доступны уже сегодня.\n","id":82}
{"Host":"https://habr.com","Path":"/en/post/111246/?mobile=no","Text":"Лабораторный источник постоянного напряжения из блока питания / Habr                         \n\n05-01-2011\nНесколько недель назад мне для некого опыта потребовался источник постоянного напряжения 7V и силой тока в 5A. Тут-же отправился на поиски нужного БП в подсобку, но такого там не нашлось. Спустя пару минут я вспомнил о том, что под руки в подсобке попадался блок питания компьютера, а ведь это идеальный вариант! Пораскинув мозгами собрал в кучу идеи и уже через 10 минут процесс начался. Для изготовления лабораторного источника постоянного напряжения потребуется: — блок питания от компьютера — клеммная колодка — светодиод — резистор ~150 Ом — тумблер — термоусадка — стяжки Блок питания, возможно, найдётся где-то не нужный. В случае целевого приобретения — от $10. Дешевле я не видел. Остальные пункты этого списка копеечные и не дефицитные. Из инструментов понадобится: — клеевой пистолет a.k.a. горячий клей (для монтажа светодиода) — паяльник и сопутствующие материалы (олово, флюс...) — дрель — сверло диаметром 5мм — отвертки — бокорезы (кусачки) Изготовление Итак, первое, что я сделал — проверил работоспособность этого БП. Устройство оказалось исправным. Сразу можно отрезать штекера, оставив 10-15 см на стороне штекера, т.к. он вам может пригодиться. Стоит заметить, что нужно рассчитать длину провода внутри БП так, чтобы его хватило до клемм без натяжки, но и чтобы он не занимал всё свободное пространство внутри БП. Теперь необходимо разделить все провода. Для их идентификации можно взглянуть на плату, а точнее на площадки, к которым они идут. Площадки должны быть подписаны. Вообще есть общепринятая схема цветовой маркировки, но производитель вашего БП, возможно, окрасил провода иначе. Чтобы избежать «непоняток» лучше самостоятельно идентифицировать провода. Вот моя «проводная гамма». Она, если я не ошибаюсь, и есть стандартной. С жёлтого по синий, думаю, ясно. Что означают два нижних цвета? PG (сокр. от \"power good\") — провод, который мы используем для установки светодиода-индикатора. Напряжение — 5В. ON — провод, который необходимо замкнуть с GND для включения блока питания. В блоке питания есть провода, которые я здесь не описывал. Например, фиолетовый +5VSB. Этот провод мы использовать не будем, т.к. граница силы тока для него — 1А. Пока провода нам не мешают, нужно просверлить отверстие для светодиода и сделать наклейку с необходимой информацией. Саму информацию можно найти на заводской наклейке, которая находится на одной из сторон БП. При сверлении нужно позаботиться о том, чтобы металлическая стружка не попала вовнутрь устройства, т.к. это может привести к крайне негативным последствиям. На переднюю панель БП я решил установить клеммную колодку. Дома нашлась колодка на 6 клемм, которая меня устроила. Мне повезло, т.к. прорези в БП и отверстия для монтажа колодки совпали, да еще и диаметр подошел. Иначе, необходимо либо рассверливать прорези БП, либо сверлить новые отверстия в БП. Колодка установлена, теперь можно выводить провода, снимать изоляцию, скручивать и лудить. Я выводил по 3-4 провода каждого цвета, кроме белого (-5V) и синего (-12V), т.к. их в БП по одному. Первый залужен — вывел следующий. Все провода залужены. Можно зажимать в клемме. Устанавливаем светодиод Я взял обычный зелёный индикационный светодиод обычный красный индикационный светодиод (он, как выяснилось, несколько ярче) . На анод (длинная ножка, менее массивная часть в головке светодиода) припаиваем серый провод (PG), на который предварительно насаживаем термоусадку. На катод (короткая ножка, более массивная часть в головке светодиода) припаиваем сначала резистор на 120-150 Ом, а к второму выводу резистора припаиваем черный провод (GND), на который тоже не забываем предварительно надеть термоусадку. Когда всё припаяно, надвигаем термоусадку на выводы светодиода и нагреваем ее. Получается вот такая вещь. Правда, я немного перегрел термоусадку, но это не страшно. Теперь устанавливаю светодиод в отверстие, которое я просверлил еще в самом начале. Заливаю горячим клеем. Если его нет, то можно заменить супер-клеем. Выключатель блока питания Выключатель я решил установить на место, где раньше у блока питания выходили провода наружу. Измерял диаметр отверстия и побежал искать подходящий тумблер. Немного покопался, и нашел идеальный выключатель. За счёт разницы в 0,22мм он отлично встал на место. Теперь к тумблеру осталось припаять ON и GND, после чего установить в корпус. Основная работа сделана. Осталось навести марафет. Хвосты проводов, которые не использованы нужно изолировать. Я это сделал термоусадкой. Провода одного цвета лучше изолировать вместе. Все шнурки аккуратно размещаем внутри. Прикручиваем крышку, включаем, бинго! Этим блоком питания можно получить много разных напряжений, пользуясь разностью потенциалов. Учтите, что такой приём не прокатит для некоторых устройств. Вот тот спектр напряжений, которые можно получить. В скобках первым идёт положительный, вторым — отрицательный. 24.0V — (12V и -12V) 17.0V — (12V и -5V) 15.3V — (3.3V и -12V) 12.0V — (12V и 0V) 10.0V — (5V и -5V) 8.7V — (12V и 3.3V) 8.3V — (3.3V и -5V) 7.0V — (12V и 5V) 5.0V — (5V и 0V) 3.3V — (3.3V и 0V) 1.7V — (5V и 3.3V) -1.7V — (3.3V и 5V) -3.3V — (0V и 3.3V) -5.0V — (0V и 5V) -7.0V — (5V и 12V) -8.7V — (3.3V и 12V) -8.3V — (-5V и 3.3V) -10.0V — (-5V и 5V) -12.0V — (0V и 12V) -15.3V — (-12V и 3.3V) -17.0V — (-12V и 5V) -24.0V — (-12V и 12V) Вот так мы получили источник постоянного напряжения с защитой от КЗ и прочими плюшками. Рационализаторские идеи: — использовать самозажимные колодки, как предложили тут, либо использовать клеммы с изолированными барашками, чтобы не хватать в руки отвёртку лишний раз.\n","id":83}
{"Host":"https://habr.com","Path":"/en/post/109820/?mobile=no","Text":"Генерируем OfficeOpenXML-документы за 5 минут / Habr             \n\n10-12-2010\nЧасто надо бывает из приложения на ASP.NET сгенерировать отчёт на сервере в OpenXML-формате. Есть несколько привычных способов сделать это: «Нашёл, слинковал, заюзал» – идём в Гугл, ищем библиотеку для генерации docx или xlsx, подключаем, разбираемся, генерируем. Это привычно, но долго. «Фу» – использовать COM. Это не рекомендуется, требует установленного Microsoft Office на сервере, не очень thread-safe, с x64 не дружит и вообще старомодно. «Ъ» – разобраться с форматом, собрать из XML и зазипать. Брутально. «Microsoft way» – об этом способе рассказывается под катом. Небольшое введение OfficeOpenXML – это то, в чём вы по умолчанию сохраняете документы, работая в Word и Excel: docx и xlsx. Файл представляет собой zip-архив. Его можно переименовать в zip, открыть архиватором и рассмотреть, что внутри: Отчёты в OOXML хорошо воспринимаются и редактируются привычными средствами. Я бы не рекомендовал в серьёзных приложениях ограничиваться именно этим форматом, но советую поддерживать его. Подготовка Нам понадобятся: Microsoft OpenXML SDK: www.microsoft.com/downloads/en/details.aspx?FamilyId=C6E744E5-36E9-45F5-8D8C-331DF206E0D0&displaylang=en (качать то, что больше) Microsoft Word Простейшее C#-приложение в Visual Studio Качаем OpenXMLSDKTool с сайта Microsoft и устанавливаем его: Поехали Запускаем Open XML SDK 2.0 Productivity Tool: Эта тулза очень простая и умеет делать две маленькие, но важные операции: Сгенерировать код по документу Сравнивать документы на уровне XML Но обо всём по порядку. Генерация кода Загружаем в программулину наш документ и кликаем «Reflect Code»: Слева мы видим структуру документа – те же файлы, что присутствуют в архиве, и представление их содержимого. Ноды в дереве можно выделять: справа видно содержимое ноды в виде XML и код, который может сгенерировать именно этот кусочек. На моём примере виден один абзац из тела документа. Оно как раз живёт в word/document.xml. Если выделить корень дерева (сам документ) – получим код для всего документа. Теперь давайте поиспользуем этот код Делаем проект в Visual Studio. Пусть это будет простое консольное C#-приложение Добавляем референс на сборку DocumentFormat.OpenXml: У меня она в GAC. Если вы не хотите её туда класть, можно добавить ссылку на сам файл. Отдельно скачать его можно там же, где был OpenXMLSDKTool, но по ссылке OpenXMLSDKv2.msi Добавляем референс на WindowsBase Добавляем файл «GeneratedClass.cs» Копируем туда код из тулзы, из окошка ReflectedCode Закрываем файл, сохранив его, переходим в Program.cs Пишем метод Main: new GeneratedCode.GeneratedClass().CreatePackage( @\"D:\\Temp\\Output.docx\" ); Запускаем Всё. Код для генерации документа готов. Документ будет выглядеть точно так же, как он выглядел перед тем, как вы сохранили его в Word. Быстро, не правда ли? Что внутри? Что же внутри сгенерированного класса? Во-первых, там один единственный открытый метод: public void CreatePackage( string filePath) { using (WordprocessingDocument package = WordprocessingDocument.Create(filePath, WordprocessingDocumentType.Document)) { CreateParts(package); } } Вот тут вставляется текст, который будет в документе: private void GenerateMainDocumentPart1Content(MainDocumentPart mainDocumentPart1) { Run run2 = new Run() { RsidRunProperties = \"00184031\" }; Text text2 = new Text(); text2.Text = \"Исчисление предикатов, по определению, философски выводит структурализм, изменяя привычную реальность.\" ; // о.О какую траву курил Яндекс? } Как видно из названий private-методов в коде, OpenXml-документ состоит из частей (part). Для генерации каждой части сделан отдельный метод. Наиболее любознательные, конечно же, ехидно улыбнувшись, вставили в документ картинку. Картинки хранятся прямо в этом файле, в виде base64, вот тут: #region Binary Data //... #endregion Завязываем бантики Рефакторинг картинок и замена статического контента на динамический оставим читателю в качестве упражнения. А вот метод, который генерирует не файл, а массив байтов – для отдачи клиенту из asp.net без временных файлов: public byte [] CreatePackageAsBytes() { using ( var mstm = new MemoryStream()) { using (WordprocessingDocument package = WordprocessingDocument.Create(mstm, WordprocessingDocumentType.Document)) { CreateParts(package); } mstm.Flush(); mstm.Close(); return mstm.ToArray(); } } Всё, код для генерации отчёта в формате docx готов. Осталось заменить контент на динамический. Мы же не делали всё это ради того, чтобы всё время отдавать одно и то же, ведь правда? И добавить на страничку ссылку «Скачать в формате Word». Сравнение документов Итак, мы сгенерировали код по документу. Добавили туда много данных, зарефакторили его, внедрили в production. И вот нам надо поменять шрифт и текст в отчёте. Как же это сделать? Кода много, искать в нём долго. Оказывается, всё очень просто, нам поможет фича сравнения документов: Положим рядом старый и новый документы Открываем Open XML Productivity Tool, выбираем «Compare files...»: Открываем файлы и жмём OK. Перед нами результат сравнения: На строчки с именами файлов можно тыкнуть и увидеть, в чём именно отличия: В MoreOprions выбирается, что игнорировать при сравнении. View Part Code показывает код той части, XML которой вы видите. Уж сопоставить XML и код труда не составит. Кстати, эту фичу ещё очень удобно использовать, если вы только знакомитесь с форматом OpenXML: добавляете что-то в документ и смотрите, что изменилось. Поможет тем, кто выбрал способ «Ъ», о котором говорилось в начале статьи. Факты С Xlsx катит. Точно так же, как с docx Если внутри Docx график или диаграмма – всё будет хорошо Это всего лишь strongly-typed обёртка над библиотекой System.IO.Packaging На сервере не нужно ничего, кроме этой библиотеки Никаких проблем с x64 Производительность на высоте Выводы Я считаю, что использование DocumentFormat.OpenXml для генерации отчётов в web-приложениях – правильный выбор. Полезная тулза из SDK позволит вам не тратить время зря. Что почитать Про OpenXML SDK: msdn.microsoft.com/en-us/library/bb448854(office.14).aspx Про OpenXML (если кто с ним не знаком): en.wikipedia.org/wiki/Office_Open_XML Удачи! Спасибо за внимание.\n","id":84}
{"Host":"https://habr.com","Path":"/en/post/134604/?mobile=no","Text":"Как получился Indie-Tracker / Habr                 \n\n14-12-2011\nЯ разрабатываю онлайн-сервис таск-трекинга для небольших команд разработчиков. Он будет очень простым и наглядным, с современным графическим интерфейсом. Чтобы начать им пользоваться, достаточно пройти по ссылке внизу поста и зарегистрироваться. Сейчас я расскажу, почему во время разработки программы я несколько раз изменял её концепцию. Какие ошибки я допустил и к чему пришел в конце. Зачем? Я перебрал достаточно много разных программ такого рода, но они мне не понравились. Они были слишком сложны и не понятны, отнимали много времени, а установку и настройку некоторых вообще не удалось довести до конца. Мне показалось, что эти программы сделаны только для менеджеров. И мне стало интересно написать подобный продукт, ориентированный на разработчиков. Первый эскиз Я успел поработать программистом в малых и в средних компаниях, а также фрилансером. В самой крутой команде было шесть программистов, один менеджер и еще один очень главный менеджер, плюс группа девочек-тестеров. Раскидывали задачи с помощью сложной и страшной системы Atlassian Jira: заходишь по ссылке, жмешь кнопочку и получаешь список своих задач. Ничего о ведении проектов я так и не понял. Но все же у меня возникло представление о системе ведения задач. В качестве пользователей я видел себя и маленькие мифические команды и решил, что распределения задач по исполнителям не нужно. Первый блин вышел таким: на основном экране программы был свернутый многоуровневый список. Он состоял из категорий, под-категорий и задач. Еще были мульти-задачи — я думал, что это круто. Выглядело это примерно так: Все элементы сворачивались и разворачивались, задачи по клику расширялись, показывая больше информации и кнопки для управления ими (в работу, завершить и т.д.). Категории показывали количество выполненных задач. Процесс работы с программой был прост: создаешь задачи, распихиваешь по папкам-категориям, смотришь на список и отмечаешь выполненные задачи. Плюсом программы была компактность: на небольшом экране можно быстро оценить состояние всего проекта. Но негде было посмотреть список “текущих” задач, то есть программа не отвечала на вопрос “что нужно сделать сегодня?”. Но тем не менее, так я видел продукт. Хорошо, что я не успел его таким доделать. Развитие интерфейса Первые причины для пересмотра концепции я нашел сам. Категории казались сложными, громоздкими и ненужными, программа не давала то, что нужно. Как я вообще это придумал? Я снова начал просматривать существующие аналоги. Отказался от мульти-задач и категорий, теперь задачи отображались простым списком. Добавил вверху колонки три кнопки, чтобы можно было прятать задачи определенного статуса, например, завершенные. Еще добавил новые колонки: комментарии к выбранной задаче, релизы — список важных задач, историю изменений. Теперь на виду всегда остаются только нужные задачи. Программа стала похожа на некий совместный рабочий стол. Она начала решать проблему составления ближайших планов проекта, окреп интерфейс пользователя. Исчезла возможность взглянуть на проект сверху, как это было раньше, но не думаю, что это было большой потерей. Программа стала годной к употреблению, и я продолжил её реализацию. К сожалению, на тот момент трекером никто не пользовался, даже я. Я только писал его, придумывал что-то новое и странное. Испытание в реальных условиях Последним местом работы была новая маленькая компания, занимающаяся одним небольшим, но успешным проектом. У нас было два художника, три программиста и дизайнер. Как в любой команде, здесь существовал процесс распределения задач. Но делали они это — смешно сказать — в google-таблицах. Моим трекером еще никто не пользовался, и я предложил компании использовать его для управления. Но он им не подошел, он вообще не соответствовал нуждам команды. Тогда, работая прямо внутри целевой аудитории своего проекта, я стал наблюдать за внутренними процессами, пытаясь понять, что же им было нужно. Задачи записывались в google-таблице, где на каждого человека была заведена своя колонка. Списки задач обновлялись примерно раз в неделю. Казалось бы, никакого функционала не было, почему они это использовали? А потому, что это было очень просто: кликнул в ячейку — ввел текст задачи. Или выделил и удалил. Никаких popup-окон, кнопок “ok” и т.д., только быстрый ввод и отображение. А самое главное — всего в одной таблице каждый человек мог видеть свою личную колонку, а “начальник” видел колонки всех остальных, и было ясно-понятно, кто что делает на этой неделе. Конечно, были в этом варианте существенные ограничения, но все они окупались тем, что самые важные и часто используемые функции выполнялись простейшим образом. Для “переманивания” клиентов им нужно было предложить нечто еще более простое и удобное, чем google-таблицы. В результате я стал многое переделывать, началась генерация идей. Иногда я что-то рисовал, объяснял и показывал на работе, получал комментарии и двигался дальше. Проект стал развиваться гораздо быстрее, чем раньше. У руководителей больше интереса было в просмотре задач по отдельному пользователю по нескольким проектам, управление итерациями и приоритетами. Другие идеи приходили от личного опыта работы в компании. Они касались удобства работы в трекере именно исполнителя задач. А самый неожиданный опыт пришел из сферы разработки игр, он касался пользовательского интерфейса. В результате, как мне сейчас кажется, мне удалось собрать кое-что интересное, что-то, что могло показать задачи по исполнителям, или задачи в разрезе по степени выполнения, причем переключение между этими режимами очень легкое, их даже можно смешать и придумать свой режим. При этом я постарался упростить работу с программой. Что получается в итоге — судите сами. Вывод Время, когда я разрабатывал трекер самостоятельно, конечно, принесло свои плоды, но больше технического характера. Сама основа продукта пришла во время наблюдения группы потенциальных пользователей. Но с другой стороны, если бы я не дал время для развития собственных идей, то получилось бы что-то совсем другое, может быть менее интересное. Я до сих пор работаю в этой команде, постоянно смотрю, насколько удобен мой продукт, иногда вдруг ловлю себя на мысли, что еще нужно добавить или изменить. Я считаю, что это один из лучших способов развития проекта. Подвал На данный момент проект находится в стадии развития, бета-версия доступна тут. Спасибо за внимание.\n","id":85}
{"Host":"https://habr.com","Path":"/en/post/424361/?mobile=no","Text":"Как геймификация улучшает пользовательский опыт / Habr           \n\n25-09-2018\nПривет, Хабр! представляю вашему вниманию достаточно свободный перевод статьи Ника Бабича. Создание продуктов, отвечающих пользовательским нуждам, фактически стало стандартным подходом к дизайну. Проектировщики, пытаясь улучшить UX, постоянно ищут новые техники и подходы. Один из недавно ставших популярным методов — геймификация. В этой статье я поделюсь парой советов о том, как эту технику можно применять в веб-дизайне. Что такое геймификация Когда мы слышим «геймификация», на ум первым делом приходит в том числе и гейм-дизайн. Хоть у геймификации и гейм-дизайна есть пара схожих черт, прямо они не связаны. Геймификация — это дизайн-техника, основывающаяся на использовании игровых механик вне игр. Грамотно реализованная геймификация улучшает пользовательское вовлечение и повышает конверсию. Как игровые элементы улучшают пользовательский опыт Геймификацию часто ассоциируют с PBL (points, badges, and leaderboards — баллы, бейджи, рейтинги). Легко предположить, что можно оживить пользовательский опыт, просто разнообразив его какими-нибудь бонусными баллами, но это не так. Геймификация — это не столько про баллы, сколько про мотивацию. Если у людей есть мотивация что-то делать, они будут заниматься этим куда больше. Люди играют в игры не из-за баллов, а из-за того, что это доставляет им удовольствие и бросает вызовы. Геймификация в веб-дизайне Геймификацию можно использовать, чтобы дать посетителям чёткий путь по продукту и ясные цели. Пользователь будет чувствовать себя настоящим игроком, начинающим по сайту целое приключение: взаимодействие превращается в историю, где посетитель («герой») преодолевает препятствия («вызовы») и пытается добиться цели («получить награду»). Это делает процесс взаимодействия с сайтом гораздо яснее и предсказуемее. Вызов и награда Люди любят вызовы. Они заставляют нас концентрировать свои усилия на результатах и доказывают, что мы можем решить любую проблему, с которой сталкиваемся. Вспомните, когда вы в последний раз покупали что-то в IKEA, привозили домой и собирали несколько часов. Весело ведь. Человеческая натура всегда толкает нас на то, чтобы принимать вызовы и доказывать, что мы можем с ними справиться. Похожим образом вызов, включенный в пользовательский поток задач, может стать затягивающим игровым элементом, мотивирующим людей действовать. Эта техника используется, чтобы мотивировать пользователя на выполнение заданий, которые иначе казались бы скучными (например, заполнение профиля). Процент в левом верхнем углу профиля в PayPal постоянно мотивирует пользователя добавить больше личной информации. Эффект вызова можно улучшить, добавив какую-нибудь награду. Если выполнение задач на сайте награждается, у пользователя будет больше мотивации принять вызов. ProductHunt просит пользователей выполнить несколько заданий, прежде чем дать им возможность присоединиться к обсуждению. Групповой квест Групповой квест — техника, работающая сходно с мультиплеером в играх. Чтобы победить, игроки объединяются в команды. Kickstarter — хороший пример проекта, использующего много гейм-дизайнерских техник. Таймер обратного отчёта, создающий ощущение срочности и дополненный last mile drive (прим. переводчика: техника геймификации, демонстрация того, что пользователь близок к цели, например с помощью прогресс-баров, как на скриншоте ниже), мотивирует пользователя к действию. Но самое важное — то, что пользователи могут выполнить краудфандинговое задание, только работая в команде. Kickstarter использует геймификацию для краудфандинга. Пошаговое обучение Пошаговое обучение — техника, помогающая учить пользователей пользоваться вашим продуктом. Программа обучения разделена на несколько шагов (например, уровней), и каждый шаг доступен только после выполнения предыдущего. Эта механика помогает сделать процесс первого взаимодействия с продуктом максимально простым и понятным. Когда компания ProdPad продумывала, как улучшить процесс знакомства с сервисом, ей пришла идея геймифицировать его. ProdPad превратила знакомство в целое приключение; каждый сделанный в системе обучения шаг (например, добавление названия продукта или каких-либо дополнительных данных) продлевает пробный период. Если визуализировать весь процесс взаимодействия, он становится для пользователей более понятным. У них будет возможность оценить требуемые усилия и спланировать время. Социальное влияние Люди — существа социальные. То, что мы делаем, часто основывается на том, что подумают другие. Это человеческое психологическое свойство можно использовать в разработке веб-опыта. Opower — компания, пытающаяся помочь людям уменьшить счета за электричество. Она обнаружили хороший способ менять человеческое поведение — вместо того, чтобы показывать абстрактные числа, Opower демонстрирует «лучшего» и «среднего» по затратам соседей. Когда люди это видят, им сразу хочется оказаться хотя бы на среднем уровне. Opower, используя социальное сравнение, мотивировала миллионы семей экономить на электроэнергии Развитие и достижения Эта техника основывается на том, что люди мотивируются, видя свой прогресс, и это толкает их на развитие своих способностей и достижение совершенства в какой-либо области. Хороший пример — Duolingo, геймифицировавший процесс обучения и превративший его в весёлый, увлекательный опыт. Каждый урок — вызов; когда пользователь выполняет задание, Дуолинго награждает его бейджем. Заключение Разумеется, пользовательский опыт в сути своей больше завязан на юзабилити и простоте, но есть дополнительный ингредиент, который сильно на этот опыт влияет, — удовольствие. Взаимодействие с интерфейсом должно быть не только удобным, но и приятным; этого вполне можно добиться с использованием геймификации.\n","id":86}
{"Host":"https://habr.com","Path":"/en/articles/152845/","Text":"Получение метаданных .NET на клиенте с использованием ajax / Habr             \n\nВсем, кто программирует в среде ASP.NET MVC, хорошо известно, насколько широко используются метаданные в .NET вообще и в MVC в частности. В MVC, атрибуты применяются как при генерации разметки, так и при валидации данных, полученных с клиента.\n\nПри использовании классической модели программирования сайтов это прекрасно работает. Но что, если Вы работаете с использование ajax и формируете html разметку динамически на клиенте? Вы хотите иметь метаданные модели (далее МДМ) на клиенте? Я — да!\n\nПрямой путь — это сформировать json, включив туда и данные, и МДМ. Мы всегда можем написать в контроллере что-то вроде:\n\n```\n public ActionResult GetData()\n {\n   return Json(new {\n       data = new { данные },\n       meta = new { мета }\n    });\n }\n\n```\n\nЭто просто, но не удобно при работе с моделью в браузере, да и выглядит как-то неприглядно.\n\nМне пришло в голову, что передавать МДМ следует так же, как это делает сервер или браузер, а именно в заголовках http. Давайте попробуем сделать следующее:\n\nСформируем объект мета.\n\nCериализуем его в строку json.\n\nСделаем енкодинг или конвертацию строки в base64 (это необходимо, так как заголовок http передается в ASCII).\n\nCоздадим заголовок http и именем “meta-data”.\n\nДалее я написал простой код, который реализует эту идею.\n\nДля начала определим модель и поставим атрибуты на свойства.\n\n```\npublic class Data\n{\n      [ReadOnly(true)]\n      [DisplayName(\"Номер\")]\n      public int Id { get; set; }\n\n      [DisplayName(\"Название\")]\n      public string Name { get; set; }\n}\n```\n\nНапишем простой метод действия, который будет возвращать клиенту данные и МДМ.\n\n```\npublic ActionResult GetData()\n{\n      var data = new Data { Id = 1, Name = \"Test\" };\n      var meta = ModelMetadataProviders.Current.GetMetadataForType(() => data, typeof(Data));\n      var metaForJS = meta.Properties.ToDictionary(\n             p => p.PropertyName,\n             p => new { displayName = p.GetDisplayName(), readOnly = p.IsReadOnly });\n      var jsonMeta = new System.Web.Script.Serialization.JavaScriptSerializer().Serialize(metaForJS);\n\n      // Uri.EscapeDataString -  использовано для простоты кода. В реальном приложении используйте base64.\n      Response.Headers.Add(\"data-meta\", Uri.EscapeDataString(jsonMeta));\n\n      return Json(data, JsonRequestBehavior.AllowGet);\n }\n\n```\n\nНаберем в адресной строке браузера\nlocalhost\n:67578/Hab/GetData и посмотрим на заголовок http.\n\n```\nHTTP/1.1 200 OK\nCache-Control: private\nContent-Type: application/json; charset=utf-8\nServer: Microsoft-IIS/8.0\nX-AspNetMvc-Version: 4.0\ndata-meta:5B%7B%22displayName%22%3A%22%D0%9D%D0%B0%D0%B7%D0%B2%D0%B0%D0%BD\n%D0%B8%D0%B5%22%2C%22readOnly%22%3Afalse%7D%2C%7B%22displayName%22%3A%22%D0%9D\n%D0%B0%D0%B7%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%22%2C%22readOnly%22%3Atrue%7D%5D\nX-AspNet-Version: 4.0.30319\n...\n\n```\n\nУбедимся, что все работает как надо, и перейдем к клиентской части кода.\n\n```\nfunction createField(name, value, meta) {\n    // Создаем поле с учетом метаданных\n    return \\$('<div/>').append(\n        \\$(\"<label/>\").attr('for', name).text(meta[name].displayName),\n        \\$(\"<input type='text'/>\").attr(\"name\", name).attr(\"readonly\", meta[name].readOnly).val(value)\n    );\n}\n\n\\$(function () {\n    \\$.getJSON(\"/Hab/GetData\")\n    .done(function (data, s, xhr) {\n        // получаем,  декодируем и создаем js объект  meta\n        var meta = \\$.parseJSON(decodeURIComponent(xhr.getResponseHeader(\"data-meta\")));\n        // создаем и выводим поля на экран\n        for (var p in data)\n            \\$('body').append(createField(p, data[p], meta));\n    })\n    .error(function(d,s){alert(s);});\n});\n\n```\n\nВ результате мы получаем два поля ввода с учетом метаданных модели на сервере.\n\nОсобенно элегантно данный подход выглядит при использовании на клиенте\nbackbone\n. Мы можем получить МДМ, переопределив метод parse:\n\n```\nvar model= Backbone.Model.extend({\n     ....\n     parse: function(data, xhr){\n        this.meta = \\$.parseJSON(decodeURIComponent(xhr.getResponseHeader(\"data-meta\")));\n        return data;\n     },\n     ......\n});\n\n```\n\nполучив МДМ в model.meta в виде объекта js, можем использовать МДМ во view для рендеринга модели\n\n```\nvar view = Backbone.View.extend({\n     render: functoin(){\n        var m =  this.model.toJSON();\n        this.\\$el.html(this.template(_.extend(m, { meta: this.model})));\n     }\n});\n\n```\n\nиспользуя для tempalte что то вроде:\n\n```\n<script >\n...\n<label><%-meta.name.displayName%></label>\n<input type='text' value=\"<%name%>\" <%-meta.readOnly ?\"readonly\":\"\" %> />\"\n...\n</script>\n\n```\n\nВ заключение добавлю, что описанный подход можно легко применить для валидации модели на клиенте.","id":87}
{"Host":"https://habr.com","Path":"/en/articles/60803/","Text":"Пуш ми, бум-бум, тач ми… Ajax Push Engine / Habr                         \n\nСегодня поговорим о\nComet\nи о\nserver push\nвообще.\n\nОбычные веб-приложения, как и сайты, работают по традиционной модели запрос-ответ-запрос, при этом, в силу особенностей HTTP-протокола и некоторых серверных реализаций обработчиков, приложение не хранит информацию между запросами, так что каждый вызов является независимым, а идентификация или сессионность обеспечиваются более высокоуровневыми средствами (например, всем известная реализация сессий в PHP). Кроме этого, запрос на новую информацию посылает всегда клиент, который заинтересован в получении самой актуальной версии данных. В приложениях, которым критично обновление, это часто становиться узким местом. У нас в одном из предыдущих проектов, было сразу несколько периодических AJAX-запросов на обновление данных. Хотя для такого случая есть варианты и вызова нескольких обработчиков на стороне сервера одним запросом от клиента.\n\nНо существует другой подход, когда сервер самостоятельно определяет, что есть новые данные (а он то узнает об этом самым первым) и доставляет их клиентской программе, которая не тратит время на запросы, а сама получает, когда что-то новенькое появилось. Для этого, правда, необходимо держать постоянным подключение к серверу, например, через двунаправленный сокет. И если в традиционном ПО с этим особых сложностей не наблюдается, что реализовать долгие постоянные соединения для веб-приложений достаточно непросто. Самым примитивным способом это делают через IFrame, однако это не единственная возможность, да и разработчики топовых библиотек и плагинов к ним постарались, так что поищите в своем любимом фреймворке, реализация Comet там должна быть (она точно есть в Dojo Toolkit, есть плагины для jQuery, встроенная возможность в GWT). Также можете почитать о различных способах реализации вот\nв этом наборе материалов\n.\n\nА вот как сделать сервер? Обычный вариант Apache + PHP слабо подходит, хотя, конечно, и на нем возможно, но решение будет далеко не оптимальным и не выдержит типичной нагрузки. Кстати, о нагрузке. Для Comet-приложений нагрузкой считается количество клиентов, которые могут быть обслужены одновременно, при этом имеется ввиду количество открытых соединений с клиентами, а не передача данных. И число таких соединений для обычных серверов должно достигать десятков тысяч, типичные цифры — 20 — 50 тыс. соединений параллельно. И вот тут связка Apache/PHP никак не поможет. Надо что-то другое.\n\nДля мира Java есть реализации для серверов приложений, например, Jetty (наверное, самая известная и стандартная реализация Comet, вот\nхорошая статья на русском\nот IBM), не так давно появился серверный фреймворк на базе\nплатформы Grizzle\n—\nAtmosphere\n, так что реализовать собственную логику и свой сервер для специфического приложения не составит труда.\n\nОстальным что делать? Им могу порекомендовать\nоткрытый проект APE или Ajax Push Engine\n. Это небольшой сервер, написанный на С, который компилируется в виде демона и слушает свой порт, по умолчанию, 6969, однако его можно настроить и на совместную работу с Apache. В отличие от других решений, APE это специализированный HTTP-сервер, который поддерживает GET/POST запросы, то есть, подключиться к серверу можно с любого языка и системы, лишь бы понимались HTTP-запросы. И это самодостаточно решение, в принципе, можно ограничится одним только APE, без дополнительных серверов (в Jetty реализация все же требует сервера приложений и веб-сервера). По заявлениям разработчиков, APE отлично работает под нагрузкой и способен держать одновременно до 100 тысяч соединений, в дальнейшем будет добавлена и горизонтальное масштабирование.\n\nСледует заметить, что по своей архитектуре, проект APE состоит из трех частей:\n\n**epoll-driven HTTP server**  — самая основа, сервер, который обрабатывает подключения и держит соединения, позволяя реализовать Server push любым подходящим для клиента образом, будь то **XHR long-polling**  или через iframe, а в будущем и нативные возможности браузеров будут утилизированы (например, web sockets)\n\n**APE JavaScript Framework**  — не менее важная часть проекта, клиентский скрипт на базе MooTools, который может интегрировать возможности сервера в любое клиентское AJAX-приложений. Также в нем реализована модульная система и расширения, так что поверх можно написать любую обертку для другой библиотеки или прозрачно встроить в ваше решение. Библиотека реализует имитацию сокетов и pipe, так что разработчик получает достаточно высокоуровневые абстракции, а если забраться еще выше, то ему доступна событийная модель и можно просто забыть о деталях реализации.\n\n**Система плагинов**  — сервер можно расширять, добавляя собственную функциональность  через подключаемые модули. Сейчас доступно только через С-модули, однако они предельно просты, в будущем ожидается серверный JavaScript, что станет по истине отличным решением (эх, ещё бы РНР туда прикрутить, хотя, конечно, можно через простенький C-плагин, но все же).\n\nНа стороне сервера, APE использует механизм epoll, а также реализацию хеш-таблицы (DJB Hash Algorithm). Это оборачивается, кроме максимальной производительности, ещё и сложностями в портировании — сервер хочет для сборки ядро Linux 2.6.19+ и libc6-dev, и пока заставить его собираться под Cygwin мне не получилось (стопор именно в плане epoll-механизма, встроенного в ядро).\n\nДля обмена информацией между сервером и клиентом, а также между клиентами (через сервер), в APE реализовано несколько методов. Первый — каналы (Channels), когда пользователи подписываются на определенные каналы, а когда на сервере появляется новая информация, ассоциированная с этим каналом, она передается всем клиентам, слушающим канал. Таким образом можно сразу разослать новые данные всем клиентам, которых может быть неограниченное количество. Также APE можно использовать как очередь сообщений, хранимых в памяти — каждый подключенный пользователь имеет одну или несколько очередей. Получателем сообщения может быть или канал, тогда сообщение получат все, или напрямую другой пользователь, либо внешний сервис. Каналы могут быть как интерактивными, так и доступными только для чтения.\n\nAPE берет на себя роль прокси-сервера, позволяя обмениваться сообщениями с внешними сервисами (например, вашим сервером приложения), а также получать от них данные и рассылать их в каналы или конкретным пользователям. Таким образом, можно построить систему, в которой фронт-ендом для пользователей будет APE, который обрабатывает подключения, а дальше работает уже бек-енд сервер, поставляющий и принимающий данные в виде сообщений по внутреннему протоколу APE, однако ничего не знающий о количество подключенных клиентов, ему надо только уметь общаться с APE-сервером.\n\nВзаимодействие между клиентом и сервером производится через\nспециальный протокол\n. Пользователи могут инициировать команды, например, послать сообщение или создать подключение, в ответ на которые сервер шлет RAW-данные, обычно это JSON. Однако модули сервера могут расширять встроенный набор команд, поэтому вы можете описать собственный набор действий, в зависимости от потребностей.\n\nХотя\nофициальная документация\nвсе еще в разработке (конечно, это вики), и не сообщает о доступных модулях, в репозитарии исходников доступны несколько базовых модуля, которые могут пригодится и как справочное пособие для разработки своих решений, так и для встраивания в уже существующую инфраструктуру. Например, libape-mysql позволяет подключаться к базе данных и получать оттуда новую информацию (однако, учтите, что версия модуля 0.01), а libape-chat покажет демо-версию простой чат-системы. Самым интересным обещает быть libape-spidermonkey, хотя пока о нем нет никакой информации, кроме исходника.\n\nПроект очень молодой, однако уже показывает свой потенциал, поэтому если вы планируете начинать какое-либо серьезное приложений, где требуется одновременная работа с множеством пользователей (чат или игровая система), или критична доставка данных, советую обратить внимание на APE.\n\n**P.S.**\nЕсли кому получится собрать этот сервер под Win32 систему — пожалуйста, поделитесь решением!","id":88}
{"Host":"https://habr.com","Path":"/en/post/133518/?mobile=no","Text":"Установка Fedora 16 в качестве полнофункциональной PV-guest системы в XenServer / Habr                       \n\n28-11-2011\nВ этой заметке хотелось бы рассказать, как можно установить Fedora 16 в качестве PV-гостевой машины в XenServer/Xen Cloud Platform. В принципе, описанный ниже способ установки не поддерживаемых напрямую дистрибутивов хорошо известен, однако при установке Fedora 16 возник ряд затруднений, с которыми пришлось немного повозиться. Готовое решение под катом. Шаг 1. Установка HVM Так как в XenServer нет теплейтов для Fedora, мы начинаем установку с создания абстрактной HVM-машины из теплейта ”Other install media”. Здесь все стандартно, задаем память, сеть, создаем диск, присоединяем образ с дистрибутивом, запускаем. На первом же появивщемся экране (Install or update...) нажимаем клавишу Tab и добавляем в командную строку загрузки ядра параметр nogpt. Иначе установщик разметит диск под gpt и в дальнейшем PV-загрузчик pygrub не сможет найти ядро и рамдиск установленной системы. Момент номер 2. По умолчанию, установщик создает раздел /boot в формате ext4, что опять же недоступно пониманию pygrub. Поэтому не забываем указать, что /boot необходимо отформатировать в ext2 В остальном настраиваем систему по своему вкусу. Шаг 2. Переделываем HVM в PV Перед тем, как изменить параметры запуска машины, необходимо внести пару изменений в grub: Традиционно, файл настроек grub.cfg находится в каталоге /boot/grub. Однако в Fedora 16 grub переехал в каталог /boot/grub2, и соотвктственно pygrub не может его найти. Чтобы это преодолеть делам линк: [root@localhost ~]# ln /boot/grub2/grub.cfg /boot/grub Несмотря на то, что pygrub в целом понимает формат grub2, некоторые конструкции приводят к его краху с весьма невнятными сообщениями. Конкретно в данном случае pygrub взрывается, встретив такую строчку в grub.cfg: set default=\"${saved_entry}\" меняем ее на set default=0 Все, теперь можно со спокойной совестью менять параметры машины: [root@xcp01 bin]# xe vm-list name-label=f16 uuid ( RO) : da8d401c-93b9-67a5-5275-39c6f76e67c9 name-label ( RW): f16 power-state ( RO): halted [root@xcp01 bin]# xe vm-param-set uuid=da8d401c-93b9-67a5-5275-39c6f76e67c9 HVM-boot-policy=\"\" [root@xcp01 bin]# xe vm-param-set uuid=da8d401c-93b9-67a5-5275-39c6f76e67c9 PV-bootloader=pygrub [root@xcp01 bin]# xe vm-disk-list uuid=da8d401c-93b9-67a5-5275-39c6f76e67c9 Disk 0 VBD: uuid ( RO) : 37301b8f-c490-c1b3-ee11-09ea749289af vm-name-label ( RO): f16 userdevice ( RW): 0 Disk 0 VDI: uuid ( RO) : 8259df56-47d2-494d-a1a8-437ce5388cf5 name-label ( RW): f16 sr-name-label ( RO): NetApp XCP NFS virtual-size ( RO): 32212254720 [root@xcp01 bin]# xe vbd-param-set uuid=37301b8f-c490-c1b3-ee11-09ea749289af bootable=true [root@xcp01 bin]# xe vm-start uuid=da8d401c-93b9-67a5-5275-39c6f76e67c9 [root@xcp01 bin]# Убеждаемся, что все нормально работает, устанавливаем XS Tools и получаем полноценную виртуалку со всеми вкусностями в виде live snapshots, live migration и прочая. P.S. Чтобы set default не вылезал в будущем, нужно поправить файл /etc/grub.d/00_header Аналогичная проблема вылезла при апгрейде Ubuntu 10.10 В Ubuntu 11.04, но там pygrub ломался на обработке submenu в grub.cfg. Я у себя просто вырезал генерацию submenu из скриптов /etc/grub.d\n","id":89}
{"Host":"https://habr.com","Path":"/en/post/350382/?mobile=no","Text":"Тестирование API сервисов и RSpec / Habr                        \n\n15-03-2018\nИногда бывает необходимость написать небольшой АПИ сервис, часто в виде прототипа. И часто этот прототип потом так и остаётся в первоначально написанном виде следуя принципу «работает — не трогай». Переписывание даже относительно маленького сервиса сопряжено с возможностью внесения ошибки или случайного незначительно изменения поведения, которое обнаружится далеко не сразу. На помощь тут приходит тестирование по методу черного ящика (функциональное тестирование). Написание тестов является важной частью процесса разработки, а время потраченное на написание тестов может быть гораздо больше, чем реализация тестируемого функционала. Предлагаю рассмотреть метод тестирования, когда тестируемый код (сервис) и авто тесты написаны на разных языках программирование. Данный подход позволяет писать тесты без зависимости от первоначально выбранной технологии, что позволяет достаточно легко «выкинуть» прототип и переписать требуемый функционал на других технологиях. Плюс это демонстрация того, что тесты не обязательно должны быть написаны на том же языке, что и тестируемый сервис. Для примера возьмём следующую задачу. Написать http API сервис со следующими методами: GET /ping — сервис должен всегда отвечать кодом 200 и текстом «OK». GET /movies — сервис отдаёт список фильмов, который в свою очередь получает из стороннего сервиса. Поддерживает фильтрацию через query параметр rating, если параметр не задан, использует значение по умолчанию. Нам понадобится: Rspec — фрэймворк для тестирования на Ruby Mockserver — для эмуляции ответа от стороннего сервера Go + echo — для написания прототипа АПИ сервиса Внимание: в данном тексте сознательно опущены любые детали по установке, настройке и использовании рассматриваемых инструментов Rspec в качестве фрэймворка для тестирования выбран так как синтаксис языка ruby позволяет писать достаточно лаконичные тесты с минимум утилитарного кода. MockServer — является очень мощным инструментом для эмуляции ответов сторонних сервисов, главная особенность — умеет запускаться как независимый http API сервис. Если вы используете другой стэк технологий, то почти наверняка сможете найти наиболее удобные для вас аналоги. Данные инструменты взяты исключительно ради примера. Шаги для установки и настройки ruby, java и golang я пропускаю. Начнём с Rspec. Для удобства желательно установить bundler. Список используемых гемов будет такой: gem \"rspec\" gem \"rest-client\" gem \"mockserver-client\" Mockserver имеет достаточно удобное REST API и клиенты для Java и JavaScript. Мы же воспользуемся ruby клиентом, на данный момент он уже явно не поддерживается, но базовый функционал доступен. Генерируем скелет приложения через команду rspec --init Затем создаём файл /spec/api_spec.rb: # /spec/api_spec.rb require 'spec_helper' require 'rest-client' require 'mockserver-client' RSpec.describe \"ApiServer\" do let(:api_server_host) { \"http://#{ENV.fetch(\"API_SERVICE_ADDR\", '127.0.0.1:8000')}\" } end Напишем тест для метода /ping (поместим данный участок кода внутри блока RSpec.describe «ApiServer») describe \"GET /ping\" do before { @response = RestClient.get \"#{api_server_host}/ping\" } it do expect(@response.code).to eql 200 expect(@response.body).to eql 'OK' end end Если сейчас запустить тест (через команду rspec), то он предсказуемо свалится с ошибкой. Напишем реализацию метода. package main import ( \"net/http\" \"github.com/labstack/echo\" ) func main() { e := echo.New() e.GET(\"/ping\", ping) e.Start(\":8000\") } func ping(c echo.Context) error { return c.String(http.StatusOK, \"OK\") } Скомпилируем и запустим наш АПИ сервис (например через go run). Для упрощения кода будем запускать сервис и тесты вручную. Запускаем вначале АПИ сервис, потом rspec. В этот раз тест должен пройти успешно. Таким образом мы получили простейший не зависимый тест, с помощью которого можно протестировать реализацию данного АПИ метода на любом языке или сервере. Усложним пример и добавим второй метод — /movies. Добавляем код теста. GET /movies describe \"GET /movies\" do let(:params) { {} } before { @response = RestClient.get \"#{api_server_host}/movies\", {params: params} } context '?rating=X' do let(:params) { {rating: 90} } let(:query_string_parameters) { [parameter('rating', '90')] } let(:movies_resp_body) { File.read('spec/fixtures/movies_90.json') } let(:resp_body) { movies_resp_body } include_examples 'response_ok' end describe 'set default filter' do let(:query_string_parameters) { [parameter('rating', '70')] } let(:movies_resp_body) { File.read('spec/fixtures/movies.json') } let(:resp_body) { movies_resp_body } include_examples 'response_ok' end end По условию задачи список фильмов необходимо получать из стороннего АПИ, для эмуляции ответа в сторонне АПИ используем mock server. Для этого зададим ему тело ответа и условие при котором он будет им отвечать. Сделать это можно следующим образом: setup mock include MockServer include MockServer::Model::DSL def create_mock_client MockServer::MockServerClient.new(ENV.fetch(\"MOCK_SERVER_HOST\", 'localhost'), ENV.fetch(\"MOCK_SERVER_PORT\", 1080)) end let(:query_string_parameters) { [] } let(:movies_resp_body) { '[]' } before :each do @movies_server = create_mock_client @movies_server.reset @exp = expectation do |exp| exp.request do |request| request.method = 'GET' request.path = '/movies' request.headers << header('Accept', 'application/json') request.query_string_parameters = query_string_parameters end exp.response do |response| response.status_code = 200 response.headers << header('Content-Type', 'application/json; charset=utf-8') response.body = body(movies_resp_body) end end @movies_server.register(@exp) end И реализацию хэндлера в сервисе АПИ: movies handler func movies(c echo.Context) error { rating := c.QueryParam(\"rating\") if rating == \"\" { rating = \"70\" } client := &http.Client{} req, _ := http.NewRequest(\"GET\", \"http://localhost:1080/movies\", nil) req.Header.Add(\"Accept\", `application/json`) q := req.URL.Query() q.Add(\"rating\", rating) req.URL.RawQuery = q.Encode() if resp, err := client.Do(req); err != nil { panic(err) } else { return c.Stream(http.StatusOK, \"application/json\", resp.Body) } } Для запуска тестов теперь необходимо уже запускать три процесса: проверяемый сервис, mock server и rspec. go run main.go java -jar mockserver-netty-5.3.0-jar-with-dependencies.jar -serverPort 1080 rspec Автоматизация данного процесса является отдельной задачей. Стоит ещё обратить внимание на итоговый размер кода сервиса и тестов для него. Покрытие тестами минимального сервиса на 30 строк требует почти в три раза больше строк кода в тестах, с объёмным кодом на установку моков, но без учета автоматизации запуска и фикстур ответов. С одной стороны это порождает вопрос рациональности тестирования, с другой стороны, данное соотношение в целом является стандартным и показывает, что хорошие тесты — это как минимум половина работы. И их независимость от первоначальной выбранной технологии может стать большим плюсом. Однако, нетрудно заметить, что таким образом крайне затруднительно тестировать состояние БД. Одно из возможных решений данной проблемы — добавление приватного АПИ для изменения состояния БД или создания слепков БД (фикстур) для разных ситуаций. Gist с листингом Обсуждение, плюсы, минусы и критика — ждём в комментариях\n","id":90}
{"Host":"https://habr.com","Path":"/ru/companies/selectel/articles/660021/","Text":"Какую технику выбрасывают испанцы: 4 фотоаппарата и неплохой объектив с онлайн-барахолки / Хабр                                                         \n\nПривет, Хабр! Если вы помните мои посты про испанские барахолки, то это один из них. Правда, как я рассказывал в одном из постов, я переехал в другой регион Испании, и стоящей барахолки поблизости не нашел. В предыдущем городе была очень классная, где я каждую субботу что-нибудь да покупал. Ну да не беда, переключился на онлайн, где время от времени покупаю что-то для собственных нужд, коллекции или ремонта, для развития соответствующего скилла.\n\nНа прошлой неделе мне повезло — купил сразу четыре фотоаппарата за 40 евро (состояние их мне было неизвестно — рабочие или нет). Все это было куплено у одного продавца, который работает на точке сбора нерабочей электроники — испанцы туда сносят все барахло, ну а сотрудники берут себе то, что понравилось. Некоторые продают в онлайне, и вот я клиент одного такого продавца. Под катом — результаты изучения покупок.\n\nБез долгих вступлений покажу, что купил.\n\n### Panasonic DMC-LS2 ###\n\n5 МЕГАпиксельная фотокамера LUMIX DMC-LS2, которая работает от двух батарей АА, оборудована оптическим стабилизатором изображения MEGA O.I.S. и 3х оптическим трансфокатором.\n\nВыпущена она была в 2006 году. Рассчитывать на невероятные снимки, сделанные ею, не приходится, но, в целом, вполне рабочая «лошадка». У нее, по словам производителя, 13 композиционных режимов съемки для любой ситуации. Есть вспышка, корректировка цвета и т.п.\n\nХарактеристики:\n\nОбъектив LUMIX DC VARIO.\n\n Процессор обработки изображения Venus Engine Plus.\n\n 2,0’’ ЖК-экран с технологией TRM (улучшает видимость изображения при съемке вне помещения).\n\n Скоростная последовательная съемка MEGA BURST.\n\n Экранное меню на русском языке.\n\n Встроенная память 14 Мб.\n\n Совместимость с Картой Памяти SD/MMC.\n\n Переключатель режимов съемки: Простая, Макро, Программная, Сцены 1, Сцены 2, Видео, Просмотр.\n\n Экранная информация о режимах съемки.\n\n 4х цифровой зум.\n\nЯ пытался вставить в нее 4 ГБ SD-карточку, но работать с ней она отказывалась. Форматировал в FAT32, FAT, но нет — не хотела и все тут. Вставил карту объемом 1 ГБ и все заработало.\n\nПроверил основные функции — без проблем все работает.\n\nСтоимость ее на местном «Авито» — от 20 до 30 евро, так что в случае чего можно и продать. Но пока думаю отдать дочке — пусть играется.\n\nРезюме: полностью рабочая камера, которую еще можно использовать. Если продать, покроет половину стоимости покупки.\n\n### Nikon F65 c объективом Nikon 28-80 mm/F 3.3-5.6 AF G NIKKOR ###\n\nДревний фотик от 2001 года, который на удивление, до сих пор остается в строю. Сужу по количеству обзоров на YouTube, которые делают современные фотографы-влогеры. Понятно, это больше баловство, но упоминаний этой камеры, сделанных за последние несколько лет, просто куча. Видимо, чем-то она полюбилась сообществу.\n\nСразу проверить работоспособность не мог, поскольку в ней используются элементы питания CR2, которых у меня не было. Но потом порылся в запасах, нашел-таки элементы питания, проверил — и все заработало, камера в рабочем состоянии. Фотопленки у меня нет, но объектив фокусируется, затвор щелкает, вспышка сверкает. С этим все хорошо.\n\nЕсть небольшой ЖК-дисплей, который отображает полезную для фотографа информацию. В целом, это фотик начального уровня, который, впрочем, имеет довольно много функций. Среди них — автоматический режим AUTO а также пять сюжетных программ (спорт, портрет, макросъемка, пейзаж, ночная съемка). В Nikon F65 предусмотрены также четыре режима отработки экспозиции — P, S, A и М (программный автоматический режим, режим приоритета выдержки, режим приоритета диафрагмы, ручной режим).\n\nОбъектив Nikon 28-80 mm/F 3.3-5.6 AF G NIKKOR оказался рабочим. Его стоимость на онлайн-барахолке около 40 евро, на Amazon б/у видел за 75. В целом, один этот объектив перекрывает стоимость всей посылки.\n\n«Тушка» стоит на той же барахолке 25-50 евро, видимо, есть ценители, которые покупают этого «динозавра» для каких-то своих целей.\n\nРезюме: рабочая камера, которую можно использовать. Она одна перекрывает стоимость посылки, а если с объективом, то посылка окупает себя раза в 2.\n\n### Nikon Coolpix p510 ###\n\nЦифровая фотокамера, которая была выпущена в 2012 году. Тогда ее расхваливали за кратность оптической трансфокации, которая равна 42. Кроме того, максимальное фокусное расстояние — 1000 мм в эквиваленте для 35-мм камер. Ну и плюс здесь еще есть GPS-модуль, для записи геотегов. Сейчас геолокацию для фоток многие отключают, ну а тогда это был писк технологической моды — ведь можно было точно сказать, где была сделана фотка.\n\nЕсть здесь и возможность записи видео с максимальным качеством в HD 1080p: 1920x1080/прибл. 30 кадров в секунду. Что касается фото — то есть даже функция 3D-фотосъемки. Шик!\n\nА еще у нее дисплей-трансформер, который можно раскладывать, как душе угодно.\n\nОписывать здесь можно много чего, но не хочется утомлять лишними деталями, которые сейчас, к тому же, не особо актуальны. Подробный обзор камеры можно найти\nвот по этой ссылке\n.\n\nКамера проблемная — на линзе то ли глубокая царапина, то ли трещина, плюс отсутствуют резиновые накладки в нескольких местах. Если с последним можно еще как-то мириться, то вот проблемный объектив — это приговор. Он несъемный, починить, вероятно, можно, но это будет стоить столько, сколько несколько таких камер (может, я и не прав, тогда отпишитесь в комментариях, пожалуйста). Кстати, нашел вот ссылочку на iFixit,\nмануал по ремонту камеры\n.\n\nПроверить ее работоспособность я тоже не смог, поскольку отсутствует батарея. Чуть позже, вероятно, куплю, чтобы проверить, работает фотоаппарат или нет. Но пока что не вижу в этом смысла — наверное, просто продам на детали.\n\nНа онлайн-барахолке цена рабочей камеры от 80 до 200 евро, так что за 30-40 евро, вероятно, кто-то и купит\n\nРезюме: камеру придется продать на детали, мне она ни к чему, чинить нет ни времени, ни желания.\n\n### Nikon Coolpix P600 ###\n\nЭто более новая модель, от 2014 года. У камеры есть поворотный экран, есть WiFi, плюс есть возможность ручной фокусировки, которой не было у предыдущей модели. WiFi здесь является заменой упомянутого выше GPS-модуля, который, видимо, не получил особой популярности, а вот передача фоток из камеры в смартфон или на ПК — вполне себе полезная функция.\n\nРазработчики утверждают, что в камере отлично работает контрастный метод наведения резкости. Улучшена компенсация вибраций, фотографировать можно даже на ходу в автомобиле (конечно, не на месте водителя). Зум 60-кратный.\n\nК слову, в РФ этот фотоаппарат до сих пор продается на Яндекс.Маркете, причем цена его 25 000 рублей, что не так и мало.\n\nНа испанской онлайн-барахолке он тоже недешевый — стоимость от 150 до 320 евро, в зависимости от состояния и комплектации.\n\nСостояние у моего экземпляра так себе, отсутствуют некоторые резиновые накладки. Но повреждений нет, так что, думаю, к нему закажу аккумулятор и посмотрю, рабочий ли он.\n\nРезюме: куплю аккумулятор, поиграюсь сам и потом, наверное, продам, приведя фотоаппарат в нормальное состояние.\n\nКакой у статьи вывод? Вызывает удивление, что выбрасывают вполне рабочие фотоаппараты. Ладно бы пленочную «тушку», но ведь она с объективом, причем рабочим. Сказать, что испанцы — миллионеры, не скажу — живет большинство людей не особо шикарно. Почему не продают тому, кто может и хочет купить? Не знаю. Почему не дают своим вещам вторую жизнь, отдав или продав другим людям? Непонятно.\n\nНу а в следующий раз, вероятно, в субботу, расскажу о еще одной покупке, которая стала еще большим сюрпризом, чем фотоаппараты. Речь о ноутбуке от Huawei, а подробности в новой статье, так что не переключайтесь.","id":91}
{"Host":"https://habr.com","Path":"/ru/post/165669/?mobile=no","Text":"Имитируем адаптацию глаза к темноте в 3D, или HDR для чайников / Хабр                \n\n13-01-2013\nВсем знаком эффект временной слепоты, когда вы входите в темное помещение из светлого. Согласно распространенному заблуждению, чувствительность зрения регулируется размером зрачка. На самом деле, изменение площади зрачка регулирует количество поступающего света всего лишь в 25 раз, а основную роль в адаптации играют сами клетки сетчатки. Для имитации этого эффекта в играх используется механизм, называемый tonemapping. tonemapping — процесс проекции всего бесконечного интервала яркостей (HDR, high dynamic range, от 0 и до бесконечности) на конечный интервал восприятия глаза/камеры/монитора (LDR, low dynamic range, ограничен с обоих сторон). Для того, чтобы работать с HDR, нам понадобится соответствующий экранный буфер, поддерживающий значения больше единицы. Наша же задача будет состоять в правильной конвертации этих значений в диапазон [0..1]. Первым делом, мы должны как-то узнать общую яркость сцены. Для этого нужно вычислить среднее геометрическое значение яркости всех пикселей. Впрочем, для нашей ночной сцены это слегка неразумно, так как большая часть площади изображения темная, даже если присутствует яркий источник света, и поэтому средняя яркость практически не изменяется. Так что возьмем максимальную яркость, и поделим ее пополам. Ужмем нашу картинку до ближайшего квадрата со стороной, равной степени двойки и обесцветим ее. Затем будем каждый раз сжимать ее вдвое, пока не останется один пиксель: Для сжатия картинки, будем брать четыре соседних пикселя и выбирать из них средний (для нашего случая — вместо него максимальный). Для ускоренного вычисления среднего геометрического воспользуемся формулой Почему геометрическое? Потому что геометрическое среднее «тяготеет» к более высоким значениям, а значит будут выбираться более яркие пиксели (что нам и нужно, так как нас интересуют имеющиеся на картинке источники света). RenderTextureFormat rtFormat = RenderTextureFormat.ARGBFloat; if (lumBuffer == null) { lumBuffer = new RenderTexture (LuminanceGridSize, LuminanceGridSize, 0, rtFormat, RenderTextureReadWrite.Default); } RenderTexture currentTex = RenderTexture.GetTemporary (InitialSampling, InitialSampling, 0, rtFormat, RenderTextureReadWrite.Default); Graphics.Blit (source, currentTex, material, PASS_PREPARE); int currentSize = InitialSampling; while (currentSize > LuminanceGridSize) { RenderTexture next = RenderTexture.GetTemporary (currentSize / 2, currentSize / 2, 0, rtFormat, RenderTextureReadWrite.Default); Graphics.Blit (currentTex, next, material, PASS_DOWNSAMPLE); RenderTexture.ReleaseTemporary (currentTex); currentTex = next; currentSize /= 2; } // Downsample pass Pass { CGPROGRAM #pragma vertex vert #pragma fragment fragDownsample float4 fragDownsample(v2f i) : COLOR { float4 v1 = tex2D(_MainTex, i.uv + _MainTex_TexelSize.xy * float2(-1,-1)); float4 v2 = tex2D(_MainTex, i.uv + _MainTex_TexelSize.xy * float2(1,1)); float4 v3 = tex2D(_MainTex, i.uv + _MainTex_TexelSize.xy * float2(-1,1)); float4 v4 = tex2D(_MainTex, i.uv + _MainTex_TexelSize.xy * float2(1,-1)); float mn = min(min(v1.x,v2.x), min(v3.x,v4.x)); float mx = max(max(v1.y,v2.y), max(v3.y,v4.y)); float avg = (v1.z+v2.z+v3.z+v4.z) / 4; return float4(mn, mx, avg, 1); } ENDCG } // Prepare pass Pass { CGPROGRAM #pragma vertex vert #pragma fragment fragPrepare float4 fragPrepare(v2f i) : COLOR { float v = tex2D(_MainTex, i.uv); float l = log(v + 0.001); return half4(l, l, l, 1); } ENDCG } Заметьте, что при логарифмировании исходной картинки мы прибавляем небольшую константу, чтобы избежать коллапса вселенной в случае полностью черного (0) пикселя. На каждом шаге уменьшения в нашей текстуре хранится минимальное ( R ), максимальное ( G ) и среднелогарифмическое ( B ) значение яркости. Далее следует небольшой трюк, который позволит избежать чтения текстуры и производить «адаптацию» глаза полностью на GPU: заведем постоянную текстуру размером в 1 пиксель и на каждом кадре будем накладывать на нее новое значение яркости (тоже 1 пиксель) с небольшим alpha (прозрачностью). Таким образом сохраненное значение яркости будет постепенно приходить к текущему, что и требовалось. if (!lumBuffer.IsCreated ()) { Debug.Log (\"Luminance map recreated\"); lumBuffer.Create (); // если текстура только что создалась, явно установим ее значение Graphics.Blit (currentTex, lumBuffer); } else { material.SetFloat (\"_Adaptation\", AdaptationCoefficient); Graphics.Blit (currentTex, lumBuffer, material, PASS_UPDATE); } AdaptationCoefficient — коэффициент порядка 0.005, который определяет скорость адаптации к яркости. Осталось взять наши две текстуры (исходное изображение и яркость) и «подкрутить» экспозицию в первой, используя значение из второй. material.SetTexture (\"_LumTex\", lumBuffer); material.SetFloat (\"_Key\", Key); material.SetFloat (\"_White\", White); material.SetFloat (\"_Limit\", Limit); Graphics.Blit (source, destination, material, PASS_MAIN); // Main pass Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag float4 frag(v2f i) : COLOR { half4 cColor = tex2D(_MainTex, i.uv); float4 cLum = tex2D(_LumTex, i.uv); float lMin = exp(cLum.x); float lMax = exp(cLum.y); float lAvg = exp(cLum.z); lAvg = max(lMax / 2, _Limit); // force override for dark scene float lum = max(0.000001, Luminance(cColor.rgb)); float scaled = _Key / lAvg * lum; scaled *= (1 + scaled / _White / _White) / (1+scaled); return scaled * cColor; } ENDCG } Здесь мы восстанавливаем значение яркости из логарифма, вычисляем коэффициент масштабирования (scaled), и делаем поправку на уровень белого (_White). Используемые параметры: Key — регулирует общую яркость сцены, которая считается «нормальной» Limit — ограничивает максимальную светочувствительность глаза, не позволяя видеть, как Хищник White — регулирует ширину диапазона, указывая, какая яркость будет считаться «белой» на изображении Результат: Можно получить интересный результат, уменьшая текстуру яркости не до одного пикселя, а останавливаясь за несколько шагов (увеличив LuminanceGridSize). Тогда отдельные области экрана будут «привыкать» независимо. Кроме того, получится эффект «темного пятная», когда одна область сетчаки засвечивается, если смотреть прямо на лампу. Однако в большинстве случаев мозг автоматически прячет эффект засветки, и на мониторе он смотрится неестественно и непривычно. Подробнее о дневном tonemapping'e читаем у Рейнхарда Код Шейдер\n","id":92}
{"Host":"https://habr.com","Path":"/ru/companies/icl_services/articles/478480/","Text":"Искусственный интеллект, ITSM и в общем-то причём тут LEAN? / Хабр                                                          \n\n### Вместо предисловия или откуда щупальца LEAN ###\n\nПару лет назад мой коллега\n[рассказывал](https://icl-services.com/company/news/vnedrenie-instrumentov-lean-v-komande-servis-desk/)\n, как работает LEAN в нашем подразделении Сервис Деска. Но как-то умолчал, что LEAN работает у нас во всех сервисных проектах, а не только в Сервис Деске. В целом LEAN очень полезный инструмент поиска зон улучшений в процессах работы, и что важно – это хороший командообразующий инструмент.\n\n## Введение ##\n\nОднажды давным-давно, в далёкой-далёкой галактике… И да, пребудет с тобой Lean Thinking!\n\nВ общем, разбираясь в потерях работы ITSM-процессов, команда пришла к заключению, что они зачем-то тратят время людей на задачу с сильным таким monkey-job. А точнее, на координацию входящих запросов в стек команды со всех источников. И вроде бы всё тут понятно и можно сделать. Но в чём подвох? Создай классификаторы и маршрутизацию настрой на их основе, и будет тебе счастье… Вот тут-то мы и столкнулись с проблемой: страдает «точность» и человека-координатора мы полностью убрать не можем, прям «бяда».\n\n## На пути к правильному решению или подготовка ##\n\nНу, точность и точность…. Идём по принципу Кано и решаем, что можем с максимально разумным эффектом: матрицы классификаций – решение о выставление класса через поиск опорных слов в описании и т.д. И аллилуйя!\n\n70% скопа заявок перекрыто роботом! — Все довольны: «Мы круты, мы боги...». Мы реально это внедрили и нормально так жили уже несколько лет. Но время идёт, а потеря то, вот она. Мы теперь хотим, чтобы и классификация, и даёшь нам точность человека.\n\nНачинаем решать задачу перекрытия оставшегося скопа заявок. Помним, что это примерно 30%.\n\nИтак, их основные проблемы:\n\nЗапросы прямых пользователей без структуры описания.\n\nНовые типы запросов требуют времени на описание.\n\nЗапросы, похожие на другие, в классификаторе едут не в ту команду…\n\nУже становится понятно, куда клонится наш рассказ? Итак, время идёт, а LEAN не терпит издержек…\n\n## Итак, суть проблемы ##\n\nЧто такое запрос — это текст, который надо обрабатывать, и на результатах нужно выносить решение о его классе. К примеру, использование уже немного описанного классификатора по определённым фразам и словам требует достаточно долгой подготовки матрицы классификации и её постоянную актуализацию.\n\nНачали думать, как быть. Команда поняла, что у неё не хватает возможности решить эту задачу. Тогда обратились к коллегам из отдела оптимизации. Есть у нас такая команда, как на заводах Тойота, они помогают всей компании в оптимизации процессов: ищут, копают и т.д.\n\n```\n «Бесконечно можно смотреть на то, как течёт вода, горит огонь, и другие работают…» - неизвестный мне автор.\n```\n\nНачинаем штурмовать новые высоты с использование шторма. Brainstorm очень полезный инструмент, метод 5W усиливает шторм до бури! И что же мы нарешали:\n\nНаши исходные проблемы:\n\nПроблема точности, а точнее технологическая слабость имеющегося решения и нет возможности его улучшать.\n\nПроблема стоимости поддержки – надо постоянно актуализировать матрицу классификации, следить за отклонениями.\n\nКакие предложения по решению:\n\nНадо, чтобы машина могла принять решение по качеству предложенного варианта.\n\nРешение должно самообучаться с минимальными затратами.\n\nПоддержка решения в остальном не отличается по стоимости от предыдущего решения.\n\nНачинаем перебирать варианты.\n\nИз технологий можно подумать о статистической аналитике с элементами BI. Дорого, да и зачем тут монстра с элементами ERP? Проблемы то похожи на задачи, решаемые «искусственным интеллектом» и механизмами «машинного обучения». Ну что же, наш отдел оптимизации, не сомневаясь в успехе, на следующую встречу позвал ребят из отдела диджитал-решений.\n\n## Решаем проблему ##\n\nДата-архитекторы и дата-инженеры за пару недель перебрав немалое количество фреймворков, выкатили решение – первую оценку и модель:\n\nЕщё через месяц мы состыковали нашу ITSM и «Искусственный интеллект» и закончили тестирование.\n\nВ итоге: нам совсем не нужны координаторы запросов, так как робот сейчас обрабатывает 99% всех инцидентов и для оставшихся 10-15 инцидентов в день не создают негативное ощущение рутины. Команда довольна, не отвлекаясь от основных задач, сотрудники получили устранение рутины, просто заявив, что этот «архаичный инструмент» уже устарел и мешает работать.\n\n## Вывод ##\n\nСовместное с командой постоянное наблюдение за своими процессами несёт неоценимую пользу. Не только позволяя находить издержки и устранять их, а также формировать понимание и потребность в использовании новых технологий. Решая задачи устранения даже минимальных по значимости, но полностью рутинных проблем, мы действительно создаём ценность. И ценность не только для заказчика, но и для сотрудников и компании.","id":93}
{"Host":"https://habr.com","Path":"/en/post/191442/?mobile=no","Text":"Android — живые обои на OpenGL ES 2.0 с упрощенным эффектом DOF / Habr           \n\n26-08-2013\nИдеей для создания приложения послужило видео, в свою очередь вдохновленное дизайном Deus Ex: Human Revolution, а именно его элементами с осколками стекла. За пару дней созрело конечное видение сцены и в итоге за выходные была создана первая версия готового приложения. Построение сцены Сцена предельно простая. Она состоит из собственно плавно вращающихся осколков стекла, зависших в воздухе. Осколки разделены на 2 группы — находящиеся вблизи и вдалеке. Объекты вдалеке размыты для придания сцене дополнительного объема. Также есть размытый задний фон произвольного цвета. Камера движется влево-вправо согласно движения пальца по экрану, и таким образом осколки двигаются. Технически это реализовано так: Осколки стекла размещены по цилиндру вокруг камеры. Осколки, расположенные дальше определенного расстояния, отрисовываются в отдельную текстуру и размываются. Вместе с ними же рисуется и фон. Рисуются сперва размытые объекты вдали и поверх них уже рисуются осколки, находящиеся ближе к камере. Вот таким нехитрым способом мы и реализовали упрощенный эффект depth-of-field (DOF), которого вполне достаточно для даной сцены. Полноценная же реализация DOF несколько сложнее и более ресурсоемкая. Для этого потребовалось бы рендерить в отдельные текстуры карту глубины сцены, готовую сцену, и ее же еще раз но с размытием. И затем уже рисовать на экран одновременно размытую и четкую сцену, смешивая их согласно карты глубины и параметров фокуса камеры. Реализация Так как все объекты в сцене прозрачные, то весь рендеринг производится с различными blending mode. При этом запись в depth buffer не ведется для того, чтобы прозрачные стекла не отсекали объекты за ними. Так как самих стекол немного, то это не вызывает слишком большой повторной отрисовки пикселей. Для создания бликов и отражений объекты осколков рисуются с cubemap размером 128х128. Порядок отрисовки заднего плана: 1. Очитска FBO с glClearColor нужным цветом. 2. Поверх рисуется маска на весь размер фрейм-буфера. Таким образом получаем декоративные цветные размытые пятна для фона вместо сплошного цвета. 3. Затем отрисовываются стекла для заднего плана. Разрешение 256х256, изображение довольно сильно пикселировано. 4. Размытие всего заднего плана. Низкое разрешение заднего фона практически незаметно. Отрисовка основной сцены и компоновка двух планов: 1. Очистка экрана. 2. Отрисовка заднего плана. 3. Рендеринг стекол переднего плана. Эта отрисовка производится без записи в depth buffer, так как все объекты прозрачные. Размытие объектов на заднем плане Размытие реализовано последовательной отрисовкой изображения между двумя фрейм-буферами специальным шейдером. Шейдер может делать горизонтальное либо вертикальное размытие, это задается параметрами. Этот способ размытия называется ping-pong rendering. Суть его заключается в том, что сперва текстура из фрейм-буфера А отрисовывается с горизонтальным размытием в фрейм-буфер B, а затем наоборот из В в А, но с вертикальным размытием. Эту процедуру можно повторять необходимое количество итераций для достижения необходимого качества размытия исходного изображения. Пример реализации этого эффекта постпроцессинга был взят давно из какого-то примера bloom, ссылку к сожалению не могу найти. Примечательно, что современные телефоны и планшеты (и даже весьма старенькие устройства, тоже) могут успевать проводить даже не одну, а несколько итераций размытия достаточно быстро. На практике оказалось, что Nexus 10 выдает стабильные 50-40 fps даже при 6-8 проходах размытия текстуры 256х256, причем одним проходом является полное — горизонтальное + вертикальное размытие. Подбирая достаточно компромиссное соотношение разрешения текстуры, количества проходов и качества размытия, остановились на трех итерациях и разрешении 256х256. Mali В предыдущей статье я закинул камень в огород nVidia. Это не потому, что я просто так недолюбливаю nVidia — я точно также недолюбливаю любых других производителей железа, которые предоставляют глючные драйвера к своим GPU. Вот, например, при разработке описываемых живых обоев столкнулись с проблемой на Nexus 10. Проблема заключается в некорректном рендеринге в текстуру, причем проявляется это только при изменении ориентации устройства. Каким образом ориентация планшета может влиять на рендеринг в текстуру, для нас остается загадкой, но это факт. Сперва, для того чтобы убедиться что я просто упустил какой-то нюанс при инициализации контекста, написал вопрос на Stack Overflow: stackoverflow.com/questions/17403197/nexus-10-render-to-external-rendertarget-works-only-in-landscape И вот тут стоит похвалить сотрудников ARM за работу их тех. поддержки. Через пару-тройку дней я получил письмо от инженера ARM в котором он предложил дать запрос об этом баге на форум Mali Developer Center. Я подготовил простенькое тестовое приложение и описал шаги для воспроизведения ошибки: forums.arm.com/index.php?/topic/16894-nexus-10-render-to-external-rendertarget-works-only-in-landscape/page__gopid__41612. И через всего лишь 4(!) дня получил ответ о том, что действительно есть баг в текущей версии видео-драйвера для Nexus 10. Самое интересное, что ARM предложил workaround для решения моей проблемы, который чудесным образом помог — надо просто вызывать glViewport() после glBindFramebuffer(). За такую работу тех. поддержки ARM им памятник при жизни надо поставить — сотрудник ARM не поленился найти мой e-mail (а он на Stack Overflow не указан), и инженеры тех. поддержки ARM нашли и решили проблему быстрее чем я даже ожидал. Всех интересующихся качеством Android на Nexus 10 прошу голосовать за соответствующий баг в трекере Гугла: code.google.com/p/android/issues/detail?id=57391 Результат Скачать программку можно с Google Play по ссылке: play.google.com/store/apps/details?id=org.androidworks.livewallpaperglass Описанный метод упрощенного эффекта DOF можно применить не только для сцены с одинаковыми объектами, как в нашем приложении, но и в любых других случаях, где можно отделить основную сцену от фона.\n","id":94}
{"Host":"https://habr.com","Path":"/en/company/sonyxperia/blog/397791/?mobile=no","Text":"Обзор Sony Xperia XA Ultra. Большой смартфон по доступной цене / Habr              \n\n27-09-2016\nИз всей линейки Sony Xperia X модель XA Ultra – самая новая. Вернее она была представлена чуть позже, уже после официальной премьеры Xperia X, Xperia XA и Xperia X Performace. Как и все предыдущие модели со словом Ultra в названии, новый XA Ultra имеет крупный экран и усовершенствованную фронтальную камеру – с автофокусом и даже с отдельной светодиодной вспышкой. Внешний вид По своей концепции Xperia XA Ultra очень похож на прошлогодний C5 Ultra, а внешне напоминает увеличенный в размерах обычный Xperia XA. Впрочем, различаются модели не только габаритами. Одно из самых существенных отличий кроется во фронтальной камере. Об этом я расскажу ниже. Здесь огромный шестидюймовый экран закругляется по краям, стекло немного выпуклое, так называемое 2,5D. Кромка экрана практически незаметно сливается с боковыми гранями, смотрится очень здорово. Боковых рамок практически нет, так что по габаритам Xperia XA Ultra оказывается почти таким же, как iPhone 6s Plus, который обладает 5,5-дюймовым экраном. Разрешение составляет 1920х1080 (стандартное Full HD), плотность пикселей – 367 ppi. У флагманского смартфона Xperia X Performance этот показатель составлял 441 ppi, но разницы с дисплеем XA Ultra вы ни за что не разглядите. На задней крышке находится только глазок основной камеры и вспышка. На нижнем торце – динамик, микрофон и разъем microUSB. Слева вверху под заглушкой скрываются разъемы для nanoSIM и карты памяти microSD. Максимальная емкость накопителя составляет 256 ГБ. На правой грани находится качелька регулировки громкости, кнопка спуска камеры и клавиша включения. Последняя – круглая, прямо как в моделях Xperia Z. Другие аппараты линейки Xperia X обзавелись прямоугольными кнопками блокировки, а здесь своего рода дань традиции. В отличие от Xperia X и X Performance кнопка управления громкостью у XA Ultra находится прямо по центру правого торца. И если держать аппарат правой рукой, то большой палец интуитивно ложится прямо на нее и на расположенную чуть выше кнопку включения. На передней панели находится большой глазок камеры, вспышка и датчик освещенности. По фотографиям казалось, что передняя камера будто выпуклая. В реальности же она чуть-чуть утоплена в корпус, поэтому не будет ни цепляться за карман, ни царапаться, если класть гаджет «лицом вниз». Диод, который мигает при новых уведомлениях или зарядке устройства, прячется в левом углу и абсолютно неразличим. Цвета предлагается три: графитовый черный, белый и «золотой лайм». У нас, как видите, оказался как раз последний. Вживую выглядит интересно, при определенном освещении отдает едва заметным салатовым оттенком. Держать Xperia XA Ultra в одной руке можно, но дотянуться до углов экрана большим пальцем будет тяжеловато. Здесь на помощь придет специальный одноручный режим. Включается он легко: нужно провести от нижнего угла диагональ, и экран сразу уменьшится. Впрочем, размер активной области можно настроить вручную. В таком виде уже ничего не мешает использованию гаджета на ходу, палец спокойно касается даже противоположных углов. Начинка Сердце XA Ultra – восьмиядерный процессор MediaTek MT6755. Оперативной памяти ровно три гигабайта, прямо как в Xperia X Performance. Объем встроенной памяти составляет 16 ГБ, из них пользователю доступно около десяти. Если не увлекаться большими играми и фильмами, а скачать необходимый набор приложений (мессенджеры, банковские сервисы и прочее), то пространства легко хватит еще и под пару сотен песен. Тем же, кто хранит на устройстве много данных, лучше сразу купить карту памяти. Аккумулятор емкостью 2700 мАч обеспечивает до двух дней работы без подзарядки. Устройство может продержаться даже больше, если использовать его только при необходимости, а не бесцельно бродить по соцсетям, играть или смотреть видео. Тест PC Mark на автономность выдал время работы семь часов и две минуты. Это при использовании «на все деньги»: редактирование текстов и фотографий, просмотр HD-видео на Youtube, запуск тяжелых игр и так далее. В реальности как минимум один день, с утра до позднего вечера, аккумулятора будет хватать, даже если не экономить батарею. Смартфон поддерживает быструю зарядку Pump Express и использует адаптивную технологию Qnovo, которая призвана уменьшить естественный износ аккумулятора при зарядке, управляя процессом подачи тока. Тестирование аппарата в бенчмарках показало достойные результаты: 11 028 баллов в Ice Storm Unlimited и 423 очка в Sling Shot. Тест GFXBench GL продемонстрировал суммарно 294,8 fps, показатель в сравнении с обычной Xperia XA практически такой же. Из-за большой диагонали нагрузка на XA Ultra должна быть выше, но железо отлично справляется с любыми задачами. AnTuTu Benchmark выдал ровно 49 тысяч баллов. Операционная система XA Ultra из коробки работает под управлением Android 6.0 Marshmallow, но уже официально объявлено о том, что устройство получит обновление до следующей версии Android – Nougat (7.0). В этой версии ОС много внимания было уделено энергопотреблению и скорости работы системы, что сразу и заметно. Планшетофон великолепно уживается с Android, к тому же система вполне себе красивая: мягкая и при этом быстрая анимация, плавные переходы. Sony устанавливает свои фирменные приложения: PlayStation позволяет дистанционно управлять игровой приставкой и совершать покупки в магазине, What’s New собирает информацию о новых приложениях, трейлерах фильмов и музыке, также есть ряд других сервисов. Камера В фаблете стоят очень приличные модули: передняя камера обладает разрешением 16 Мп, задняя – 21,5 Мп. Когда-то многие зеркальные фотоаппараты позавидовали бы таким показателям. Камера мгновенно открывается, гибридный автофокус моментально наводится на объекты и быстро переключается с одного на другой при смене ракурса. Плюс предлагается множество режимов для тех, кто хочет почувствовать себя фотографом: сглаживание размытости, режим для съемки еды и животных, установки для зимних кадров при обилии снега и другие настройки. Отдельно стоит ручной ползунок для регулировки яркости – полезная вещь, когда снимаете на ярком солнце. С фронтальной камерой тоже полный порядок. Разрешение сенсора Exmor RS составляет 16 мегапикселей, есть автофокус и оптическая стабилизация. Есть даже вспышка, которая работает в умном режиме. При съемке со слабой освещенностью она дает два импульса, один из них освещает лицо, другой – фон, так что весь кадр получается равномерно подсвеченным. Есть и поддержка режиме расширенного динамического диапазона, HDR, в котором можно получить равномерно экспонированный кадр даже при большом перепаде яркости в разных участках кадра, а также есть вспышка. Видеозвонки, если вдруг ими пользуетесь, получаются великолепного качества, а селфи тем более. Выводы После типичного смартфона с диагональю 5 дюймов Xperia XA Ultra может показаться крупноватым, но в этом нет никаких минусов. Аппарат оказывается довольно компактным для своих габаритов, а большой экран фактически позволяет отказаться от покупки планшета. Он здорово смотрится без боковых рамок, но куда важнее то, что на шестидюймовом дисплее удобно смотреть видео, удобно просматривать веб-страницы, не адаптированные под мобильные устройства, здорово играть в игры и читать книги. Словом, Sony Xperia XA Ultra – прекрасный мультимедийный фаблет с хорошо сбалансированными характеристиками, приятным дизайном и выдающейся фронтальной камерой. Единственным недостатком можно считать 16 ГБ встроенной памяти, но при наличии слота расширения это не станет большой проблемой. Основные технические характеристики Sony Xperia XA Ultra Диагональ дисплея: 6 дюймов Разрешение дисплея: Full HD (1920x1080 пикселей, плотность ~367 ppi) Чипсет: Mediatek MT6755 Helio P10 (4 ядра по 2 ГГц + 4 ядра по 1 ГГц) Память: 3 ГБ оперативной, 16 ГБ энергонезависимой, слот для карт памяти microSD объемом до 256 ГБ Основная камера: 21,5 Мп, светодиодная вспышка, запись видео в разрешении Full HD Фронтальная камера: 16 Мп с широкоугольной и светосильной оптикой Аккумулятор: Li-ion, 2700 мАч, несъемный Формат SIM-карты: nano-SIM Версия операционной системы: Android 6.0 Marshmallow Габариты и вес: 164 x 79 x 8,4 мм; 202 г Цена в России: 27 990 рублей / 28 990 (Dual SIM)\n","id":95}
{"Host":"https://habr.com","Path":"/ru/articles/146150/","Text":"Как превратить медиаплеер в неттоп? / Хабр                 \n\nПриятного времени суток.\n\nFullHD медиаплееры уже перестали быть новинкой среди устройств воспроизведения медиаконтента. На рынке можно найти огромное количество устройств и моделей с различными характеристиками аппаратной части и разными прошивками ПО.\n\nВ данной статье речь пойдет о том, как научить медиаплеер не только воспроизводить видео и музыку, качать торренты и быть сетевой «шарой», но и поддерживать сервисы, различной степени надобности.\n\nВ качестве подопытного кролика будет медиаплеер на базе чипсета Realtek\n**RTD1186DD**\n. Разновидностей плееров на данном чипсете просто дикое количество. В магазинах представлены модели таких фирм как iNeXT, iconBIT, BlueTimes, Evaaa, Dune, Xtreamer, Digma, 3Q, Egreat и множество других. У каждого есть обзоры в интернете.\n\nМой домашний медиаплеер, который успешно прошел\nэкзекуцию\n, называется BlueTimes Eva Vision. Спустя какое-то время после покупки медиаплеера, желание крутить и вертеть все таки взяло верх.\n\nОсновные характеристики устройства, которые интересны в рамках данной статьи:\n\nПроцессор: Realtek 1186DD SoC, 1200 DMIPS MIPS Processor 750MHz\n\nПамять: 512MB DDR3\n\nFlash: 4GByte NAND Flash\n\nОперационная система: Linux + Android 2.2\n\nHDD: SATA 2Тб WD 5400rpm 64Mb Caviar Green ( _в стандартную комплектацию не входит_ )\n\nПоддержка файловых систем: EXT3, FAT32, NTFS\n\nLAN: RJ-45 Gigabit LAN 10/100/1000\n\nWiFi: a/g/n до 300 Мбит/с\n\nЖутко неудобным оказалось управление плеером по telnet, и я стал искать как бы запилить на медиаплеер ssh. Проведя в поисках какое-то время, мне на глаза попала система управления пакетами\n**Ipkg**\n. Благодаря ipkg можно в привычном виде устанавливать пакеты, собранные под архитектуру mipsel.\n\nФайловая система медиаплеера состоит из двух файловых систем:\nyaffs\n(/data) и\nsquashfs\n(/system).\n\nВсе пакеты Ipkg устанавливает в /opt, который изначально смотрит на /system/rtl\\_rootfs/bin/opt (упомянутый read only squashfs). Поэтому нужно перенести /opt на файловую систему yaffs. Сделать это можно путем внесения изменений в\nпрошивку\n.\n\nДля работы с прошивкой нужно установить несколько утилит:\n\n```\n~ #sudo apt-get install subversion cvs\n~ #svn checkout http://unyaffs.googlecode.com/svn/trunk/ unyaffs-read-only\n~ #cd unyaffs-read-only\n~/unyaffs-read-only #gcc -o unyaffs unyaffs.c\n~/unyaffs-read-only #sudo cp unyaffs /usr/local/sbin\n~/unyaffs-read-only #sudo apt-get install mtd-utils\n~/unyaffs-read-only #export CVSROOT=:pserver:anonymous@cvs.aleph1.co.uk:/home/aleph1/cvs cvs logon\n~/unyaffs-read-only #cvs checkout yaffs2\n~/unyaffs-read-only #cd yaffs2/utils\n~/unyaffs-read-only/yaffs2/utils #make\n~/unyaffs-read-only/yaffs2/utils #sudo cp mkyaffs2image mkyaffsimage /usr/local/sbin\n\n~ #sudo apt-get install zlib1g-dev\n~ #wget http://internode.dl.sourceforge.net/project/squashfs/squashfs/squashfs4.0/squashfs4.0.tar.gz\n~ #tar xzvf squashfs4.0.tar.gz\n~ #cd squashfs4.0/squashfs-tools\n~/squashfs4.0/squashfs-tools #make install\n\n```\n\nДля удобства я сделал два скрипта (положил в /tmp)\n**unpack.sh**\n\n```\n#!/bin/bash\nrm -rf install\nrm -rf squashfs1\nrm -rf yaffs2_2\nmkdir install\ncd  install\ntar -xf ../install.img\ncd  ..\nunsquashfs -dest squashfs1 ./install/package5/squashfs1.img\nrm ./install/package5/squashfs1.img\ncd  ..\n\n```\n\n**pack.sh**\n\n```\n#!/bin/bash\nrm ./install/package5/squashfs1.img\nmksquashfs squashfs1 ./install/package5/squashfs1.img\ncd  install\nrm ../install_new.img\ntar -cf ../install_new.img *\ncd  ..\n\n```\n\nКопируем загруженную прошивку в /tmp install.img и запускаем ./unpack\n\nДалее нужно перенести /opt на read-write раздел. Открываем файл vim squashfs1/etc/init.venus.sh и делаем следующие изменения:\n\n```\n24,25c24,27\n<\n< ln -s /system/rtk_rootfs/bin/opt /opt\n---\n> if  [ ! -d /data/opt ]; then\n> cp -R /system/rtk_rootfs/bin/opt /data/\n> fi\n> ln -s /data/opt /opt\n\n```\n\nЗапаковываем прошивку обратно, запустив скрипт ./pack.sh\n\nУстанавливаем прошивку на плеер обычным для него способом.\n\nПример установки пакета выглядит так:\n\n```\n/opt/bin #./ipkg update\n/opt/bin #./ipkg list\n/opt/bin #./ipkg install openssh\n\n```\n\nНу, а после того как механизм налажен, были добавлены bash, svn.\n\nДалее варианты использования расходятся. Можно хостить небольшой сайт, можно устроить svn\\git репозиторий. В общем в ipkg достаточно много пакетов и каждый наверняка найдет для себя что-то полезное. Я прокинул на своем dir-615 несколько портов до медиаплеера и сейчас имею свой закрытый мини репозиторий SVN.\n\nЕстественно не стоит забывать что памяти на медиаплеере не сильно много, да и процессор не i7. Всегда стоит использовать в меру, но мера у каждого своя.","id":96}
{"Host":"https://habr.com","Path":"/en/post/343774/?mobile=no","Text":"Дыра у хостинг провайдера или как получить доступ к удаленному(не активному) аккаунту / Habr               \n\n## Спасибо Elasticweb ##\n\n### Предыстория ###\n\nИмеется у меня один сайт, перенесенный с данного хостинга задолго до обнаружения уязвимости, разработанный двумя разными компаниями, названий которых не назову по\n~~непонятным~~\nпонятным причинам. Сайт претерпел значительные изменения, как по структуре интерфейса, так и по переезду на другую систему управления (\nзначимый момент в данной истории\n).\n\nЖивет себе сайт своей жизнью, никому не мешает, как вдруг получаю я запросы на определенную директорию, с периодичностью ровно в 1 минуту, с одного и того же ip. Плюс редкие запросы с разных ip, направленные на поиск административной панели сайта. Разумеется ip я заблокировал, но он продолжал настаивать на своем.\n\nПросмотрев логи ошибок и визитов, решил проверить, что же это за ip такой и кому принадлежит. Недолгими манипуляциями узнаю, что ip принадлежит компании Elasticweb, предоставляющей услуги хостинг провайдера, у которой и была расположена предыдущая версия сайта, но аккаунт был неактивен и заброшен.\n\nНедолго думая решил позвонить в их техподдержку и узнать в чем собственно дело, кто автор запросов и как долго это будет продолжаться (\n_продолжалось это больше 12 часов_\n). К сожалению номера телефона я не обнаружил ни на сайте данной компании, ни в открытых источниках, что и сподвигло меня написать тикет в техподдержку.\n\n### Завязка ###\n\nПервое, что мне бросилось в глаза, это нежелание поддержки подробно разобраться в ситуации и просто закрыть тикет, после буквально первого же сообщения.\n\nТак как у меня были подозрения на то, кто может стоять за данным деянием, я решил уточнить у техподдержки про конкретный аккаунт (\n_тот самый, на котором был когда то расположен сайт_\n). Немного подождав,\n~~на самом деле довольно долго~~\n, получил ответ, что действительно, с данного аккаунта проводились все манипуляции, но доступ к аккаунту можно получить по Email который указан в учетной записи.\n\nЯ решил связаться с предыдущим разработчиком и получить от него информацию по доступам к учетной записи, на что получил однозначный ответ о том, что учетная запись удалена по причине неуплаты хостинга.\n\nПосле небольших разъяснений с техподдержкой на тему “Доступа нет потому, что”, выяснилось что аккаунт вроде как удален за неуплату и доступа к нему нет, но он все таки существует и продолжает работу в качестве бекап версии (что полность противоречит предыдущим ответам).\n\nРешил уточнить как может быть такое, что учетная запись удалена, доступа к ней из Web версии нет, но все же кто то смог поставить задачу на Cron.\n\nПорыскав в историях переписок, нашел доступы к FTP этого хостинга, к данной учетной записи.\n\nЧем черт не шутит подумал я и запустил FTP клиент.\n\nКаково же было мое удивление, когда на экране я увидел содержимое корневого каталога и файлы с бэкапом? Сказать, что я был удивлен, ничего не сказать. При якобы удаленной учетной записи и отсутствии доступа к ней, каким то чудесным образом, я получил все содержимое аккаунта и смог спокойно его перенести на свой компьютер. Проанализировав последние записанные логи, выяснилось. что не только FTP работает, но и SSH открыто и через данную уязвимость и был запущен Cron.\n\nНаписал еще один запрос в техподдержку, я получил весьма интересный ответ. По словам хостера, предыдущий разработчик установил Cron задачу уже после того как они все удалили, никакой проблемы в этом они не видят и тикет закрывают.\n\nЕсли перефразировать, то получится следующее:\n\n_Мы знаем, что у нас есть дыра, которая позволяет подключиться к заблокированному, неоплаченному и неактивному аккаунту, произвести манипуляции с ним и получить вполне себе рабочий результат. Но делать мы с этим ничего не будем, потому, что потому. Разбирайтесь сами, а мы закрываем тикет и не хотим больше Вас слушать._\n\nСобственно на этом общение с техподдержкой было завершено, а тикет был принудительно завершен.\n\n### Причина и следствие ###\n\nИсходя из данной истории, я выяснил для себя следующее:\n\nПри переносе сайта, смене владельца, разработчика или просто смене хостера — обязательно меняйте все доступы, пароли, названия директорий, ставьте доступ в панель управления по IP или подобное, а еще лучше сменить систему управления если есть возможность. Именно смена системы управление и обеспечила мне защиту моих данных, потому как по пути к которому обращался злоумышленник, на прошлом хостинге лежал файл настроек, а возможно и вшитый shell для удаленного доступа, который оставил прошлый разработчик.\n\nНе доверяйте хостеру наслово. Проверяйте уязвимости сами, читайте отзывы и делайте бэкапы! Последний пункт очевиден, но не все его выполняют.\n\nВыбирайте хостера, у которого доступ к SSH как минимум открывается на определенное время, когда вам нужно. Ставьте разные данные для FTP, SSH, sFTP и прочих учетных записей, поверьте именно такой подход поможет не только спасти данные, но и определить с какой учетной записи производились манипуляции, если таковые были.\n\nP.S. Данная статья никоем случаем не нацелена на принижение чьих бы то нибыло прав. Все описанное выше имело место быть, о чем я хотел бы предостеречь всех кто читает данную статью. Будьте внимательны, да прибудет с Вами бэкап!","id":97}
{"Host":"https://habr.com","Path":"/en/post/405907/?mobile=yes","Text":"Децентрализованные цифровые валюты. Часть 2. Блокчейн / Habr                         \n\n11-08-2017\nВ предыдущей части мы описали основную идею децентрализованной цифровой валюты и её практическую реализацию в виде Биткойна. Как и любая новая концепция, Биткойн на практике столкнулся с множеством проблем связанных протоколом работы сети и защиты целостности базы. В этой статье мы обсудим текущие ограничения сети, альтернативные валюты с решениями тех, или и иных проблем Биткойна, и почему блокчейном заинтересовался большой бизнес. Часть 1. Биткойн Часть 2. Другие (не)популярные разновидности Биткойна, блокчейн Часть 3. Ethereum Блоки Биткойн показал не только состоятельность идеи p2p платёжной системы, но еще предложил решение проблемы публичного(децентрализованного) консенсуса. Читателю, на начальном этапе, архитектура Биткойна может показаться чрезмерно усложненной. Например, если есть общие правила и мы можем просто игнорировать некорректные/мошеннические транзакции, то зачем нужны блоки и майнинг? Для этого нам нужно понять как реализованы транзакция и предотвращение двойного расходования(double-spending) средств. Транзакция В Биткойне, при формировании транзакции на входе объединяется множество предыдущих транзакций, и на выходе осуществляются переводы на другие счета(см рисунок). Переводы, что получаются на выходе называются непотраченными транзакциями(UTXO), пока не войдут в другую транзакцию. При создании новой транзакции, UTXO расходуется целиком, но если нужно потратить только часть, то просто добавляется еще один перевод с остатком на свой же собственный счёт. Если у нас не будет уверенности в валидности предыдущей транзакции, то нам придется каждый раз перепроверять всю цепочку транзакций, вплоть до момента эмиссии. Во избежание этого, транзакции пакуются в блок, который, в свою очередь, запечатывается красивым хэшем и линкуется другими блоками с двух сторон. Это дает нам некую гарантию того, что транзакция, попавшая в блок, является валидной, и на неё уже можно ссылаться. Просуммировав все UTXO адресованных на некий счёт мы можем узнать его баланс, следовательно, в любой момент времени, сумма всех UTXO равна общему количеству денег в системе. Двойное расходование В децентрализованных системах сложно синхронизировать записи всех пользователей, возникают проблемы с очередностью транзакций и двойным расходованием. Представим ситуацию, когда некий пользователь, запускает (почти) одновременно несколько транзакций используя один и тот же UTXO. Из-за задержек сети, участники получат эти сообщения с разной очередностью, а то и вовсе могут пропустить часть(см схему). Конечно, если бы у нас была система предоставляющее всем участникам универсальное-точное время, мы могли бы избежать эту путаницу, но это обратно централизация от который мы изначально отказались. Для решения этой проблемы были придуманы блоки и майнинг, которые определяют и очередность, и подтверждение этих транзакций. Одновременно майнятся несколько ветвей, но сложность майнинга корректируется так, что в течение 3-4 блоков вперёд вырывается одна ветка, которая закрепляет все транзакции в истории навсегда. Proof-of-Work Как мы помним из предыдущей статьи, PoW защищает целостность базы. Но что такое атака 51% и какие действия в теории могут быть выполнены если у кого-то(назовем его злоумышленником) появятся внушительные ресурсы. Злоумышленник сможет наращивать цепь быстрее остальных(и забирать львиную долю вознаграждения), но только честным способом. Корректность блока проверить легко, и как только обнаружится такая деятельность добросовестные участники будут игнорировать такие блоки и транзакции. Злоумышленник может отменить транзакцию и вернуть свои деньги(см схему). В целом, такое положение дел на коротком промежутке времени особо не навредит, кроме как откат собственных транзакций и блокирования чужих. Но из-за того, что большую часть вознаграждения будет забирать один участник другие майнеры разбегутся, а вот это и означает конец сети. В 2014 году пул Ghash.io аккумулировал 51% мощности на несколько часов, и часть майнеров самостоятельно вышли из пула чтобы не подрывать доверие к Биткойну. Другие механизмы децентрализованного консенсуса Одним из главных недостатков PoW является энергозатратность. В качестве энергоэффективной альтернативы был разработан протокол консенсуса Proof-of-Stake, где вероятность создания следующего блока выше у участника с большей долей. Здесь очень хорошо описан этот механизм и другие виды консенсуса. Текущие проблемы Биткойна По правилам сети, в среднем, 1 блок создается за 10 минут. Советуют ждать около шести блоков чтобы транзакция считалась явно закрепленной в блокчейне, а это уже час времени. По сравнению с межбанковским переводом это конечно очень быстро, но все равно не подходит для мелкой коммерции. Размер блока ограничен одним мегабайтом, учитывая время создания блока, майнерам выгоднее обслуживать транзакции с высокой комиссией. Недавно, была попытка обновить протокол изменив структуру и размер блока, что в итоге привела к разветвлению сети на классический Биткойн(BTC) и на БиткойнКэш(BCH). Еще одним недостатком является высокий порог вхождения для майнеров, уже нет смысла заниматься майнингом без ASIC машин. Вдобавок к этому, майнеры объединяются в пулы ради стабильной прибыли, а это обратно некая централизация. Разновидности Namecoin — самый первый форк Биткойна, который позволяет регистрировать имена вписывая их в блокчейн. Есть пространства d/ — для доменных имен в зоне .bit, и id/ для регистрации какого-нибудь названия с сопутствующей информацией. Благодаря публичному консенсусу, в рамках сети, вы гарантированы владением конкретного имени, пока за нее регулярно платите. Такая модель является достойной альтернативой текущей системы ICANN, и в теории избавит от проблем с регистраторами. Litecoin — один из популярных форков. Транзакция проходит в 4 раза быстрее и низкая комиссия по сравнению с Биткойн. Вдобавок, для PoW был выбран алгоритм, которая должна была усложнить майнинг на GPU картах и ASIC машинах. PeerCoin — первая валюта с гибридной моделью PoW и PoS. Zerocoin — полностью анонимная валюта позволяющая отслеживать движение денег. Ethereum — платформа которая служит виртуальной машиной для децентрализованных приложений. Например, разрабатывается какой-нибудь смарт-контракт, заправляют его деньгами(наподобие комиссии, здесь это называется топливо) и отправляют в сеть. Майнеры, в свою очередь, обрабатывая блоки выполняют байткод этих приложений, и снимают себе топливо с этих приложений в качестве платы за работу. А что же с блокчейном? Способ, которым Биткойн хранит данные вкупе с методами консенсуса развился в самостоятельную тему. Перспективы внедрения могут быть в любой сфере где есть необходимость в консенсусе и прозрачной базе: Финансы, торговля; Страхование, букмекерский бизнес; Публичные базы, реестры; Голосование, электронное правительство. В каждой сфере предъявляются свои требования к модели безопасности, приватности данных, механизмам консенсуса в зависимости от участников и природы объектов циркулирующих в блокчейне. Примером может служить частный межбанковский блокчейн, где исключена анонимность и участники изначально доверяют друг другу. Соответственно, упрощаются механизмы защиты сети, а у банков просто будет инструмент для синхронизации базы и совместного управления некими активами. Другими необходимыми требованиями могут быть обмен приватными данными между участниками, подключение надзорных органов и т. д. Изначально, блокчейн решал вопрос двойного расходования цифровых активов, но в таких сферах как отслеживание товаров и страхование появляется необходимость в эффективном методе сериализации физических объектов, предотвращающий разные роды мошенничества. Здесь вы можете подробнее ознакомиться с обзором блокчейн платформ. В следующей части будет описана платформа Ethereum и разобраны примеры смарт-контрактов.\n","id":98}
{"Host":"https://habr.com","Path":"/en/company/croc/blog/174505/?hl=ru_RU&fl=ru,en","Text":"Китайцы, светодиоды и гигантские экраны – короткий фотоотчёт с LED China / Habr            \n\n28-03-2013\nУ нас в Москве начинают потихоньку запрещать наружную рекламу типа баннеров. И многие владельцы крупных зданий уже оживляются на предмет заказа здоровенных светодиодных экранов на фасады и крышу. Поэтому на выставку LED China 2013 в Гуанчжоу я ехал с чётким пониманием того, что мне интересно. Это самая крупная в мире по количеству участников выставка, посвященная светодиодам. Она проводится в Китае уже много лет. Вот место проведения — один из выставочных павильонов Гуанчжоу, рядом ещё 6 таких с другими выставками: Вот так в китайском городе (побольше Москвы) выглядит местная ВДНХ. Основной язык выставки — английский, но что радует, уже много русских. Когда заходишь в крупный павильон, спрашивают, откуда ты, и, услышав «Рашша», говорят, что есть человек, который знает язык. Через минуту появляется специально обученный китаец, который действительно говорит по-русски, но так, что лучше бы он этого не делал. В результате всё равно общение на английском. Внимание, трафик: ниже фотоотчёт. Выставка проводится именно в Китае потому, что там сконцентрированы все основные производители. Встречаются производители железа: самих экранов, светодиодов, систем коммутации, ламп и так далее. Приезжают интеграторы со всего мира. Есть ещё специальный павильон оборудования для производства экранов – станки по производству чипов и самих светодиодов. Ближайшая по размеру – ISE, с которой уже был отчёт, но она общая, и именно светодиодных технологий там не очень много. Наибольший интерес для меня представляли производители экранов и контрольного оборудования. По большому счету, все участники имели «стандартный набор» популярных версий экранов, и то, что было новинкой в пролом году, в этом присутствовало на многих стендах. Как, например светодиодные шары, разных размеров и в очень большом количестве. Напомню, изначально сама идея светодиодного экрана появилась в тот момент, когда понадобилось выносить информацию на улицу, например, делать навигацию на дорогах, таблички снаружи аэропортов и так далее. Наверное, вы все отлично помните старые добрые ламповые таблички со счётом на стадионах – это как раз прообразы современных экранов. Основное требование было очень простым – высокая яркость при больших размерах экранов минимальной стоимости. У технологии с лучевой трубкой (у плазм и LCD) были проблемы ограничения максимального размера: LCD и плазма получались слишком дорогими и неконтрастными, а проекторы могли адекватно работать только ночью, поэтому стали использоваться именно светодиоды. С годами шаг пикселя уменьшался – сейчас это один из основных трендов. На ISE несколько месяцев назад, например, показывали экран с шагом 1,9 миллиметра, на этой выставке уже есть с шагом 1,5 мм. Сейчас экраны разделены на два основных типа – для улицы с влагозащитой и стойкостью к русской зиме и тропическому лету, и интерьерные – они похлипче и менее яркие, но зато с минимальным шагом пикселя. Представленные в большом количестве светодиодные шары на разных стендах. В этом году было представлено большое количество различных вариантов изогнутых экранов. Варианты с жесткими экранами, с фиксируемым выставляемым радиусом и без, плюс сетки. Применять можно как для создания экрана определенной формы на фиксированных или арендных инсталляциях, так и для покрытия существующих периметров. В нашей практике они часто используются для разной статичной наружной рекламы и при организации концертов. Слева – экран с жесткими модулями без фиксации. Справа – элементы выставления углов на жесткой сетке. Новинкой среди изогнутых экранов стали полностью гнущиеся модули с платами на резиновой основе. Больше всего понравилась реализация таких модулей у компаний Bada и EsdLumen. Такие модули позволяют делать из экранов окружность с очень маленьким радиусом. Хотя по прежнему останется вопрос с разводкой сигналов и питания к таким маленьким модулям. Новый экран на магнитах можно просто взять и обернуть вокруг столба: очень удобно. Слева – резиновый модуль EsdLumen. Справа – резиновые модули Absen. Среди производителей экранов с малым шагом пикселя хедлайнером выступал знакомый уже Silicon Core, который представил небольшой модуль с шагом 1,5 мм, на котором уверенно крутился мультфильм. Остальные компании держались в рамках 2 мм. Слева – отдельно стоящий модуль P1,5мм. Справа – осмотр стыковки кабинетов экрана Р2,5. В принципе, модуль 1,5мм можно назвать прототипом, и считаю, что пока все что меньше 2,5 мм не совсем доработано, собранные экраны смотрятся не выведенными. С другой стороны 2,5 мм уже выглядят очень качественно. Изогнутый экран Leyard с шагом Р2,5 – P2,6, разрешение 3840х1080р. Сделан из небольших жестких модулей, то есть поверхность монтажа может быть самой разной. На примере – дуга. Несколько компаний представили светодиодные киоски. Неплохо выглядят киоски от Liantronics. К сожалению пока только indoor-исполнения, но обещали в ближайшем будущем выпустить уличные. Высокая яркость и разрешение делают использование уличных киосков очень привлекательным для рекламы и для ритейла. Учитывая, что они могут быть интерактивными, скорее всего, стоит ждать момента, когда наши розничные сети увидят в них потенциал. Киоски Liantronics. У производителей систем управления появились 3D-процессоры. Поддерживают обычное активное стерео от карт NVidia (например) — своевременно, с учетом удешевления indoor экранов высокого разрешения. Теоретически контроллеры поддерживают развертку 400Гц и выше, но принципиальных отличий от обычного стерео не чувствуется. Справа — коммутационное оборудование RGBLink, по центру – 3D контроллер, справа – процессоры LEDWALL. Несколько компаний предлагают кастомные решения на любой вкус, в частности, понравились standalone спортивные табло — минимум управления и функционала, неплохо подошло бы для небольших спортивных объектов. Маленькое спортивное табло, с набором кнопок для 4 командных игр. Эти решения интересны тем, что делаются предельно простыми и под индивидуальный заказ. Если нужна табличка для небольшого стадиона без всяких заморочек – стучим к китайцам, заказываем, получаем само табло и пульт с кнопками типа «увеличить счёт, уменьшить счёт, выключить». Можно привинтить ручку. Также очень понравились светодиодные таблички. Работают на аккумуляторах, заряжаются от mini-USB, через него же и загоняется контент. Время автономной работы порядка 16 часов, хватит на любое совещание. Пока широкого практического их применения не вижу, но китайцам они почему-то очень нравятся. Слева – табличка для индивидуального использования, справа – таблички для транспорта. Еще одно интересное применение светодиодов — рамка на тонкошовные панели для видеостен. Прозрачная силиконовая пленка со светодиодами крепится на шов между панелями и досвечивает недостающую картинку. Есть рамки под стандартный набор тонкошовных панелей, но думаю, кастомные варианты также возможны. На мой взгляд, выглядит не совсем правдоподобно, но это лучше чем закрашивать швы в основные цвета презентации, как меня несколько раз просили заказчики. Интерфейс подключения пока не очень простой (явно не пара кликов и кабелей), но если вдруг очень надо – решение есть. Видеостены NEC 47'' со светодиодной пленкой на швах. Из основных российских поставщиков больше всего понравился стенд Absen. Самая качественная инсталляция экранов, большая территория и очень много людей. Экран уголком смотрелся лучшим на выставке, потому что был очень ровно выведен. На фото все в оранжевом – это сотрудники, которые готовы помочь. Вдалеке есть бар и Мерседес с женщиной, даже один раз включили русскую музыку. В общем, позаботились. Стенд компании Absen, вверху расположен экран с шагом 3,9мм. А вот то, что было для меня самым важным – сетки. Среди производителей сеток большого разнообразия замечено не было, примерно одинаковый модельный ряд по шагу пикселя и яркости. Выделился один стенд компании GTEK, у которых есть очень интересное решение медиафасада на офисном здании. Модули сетки собираются в рулоны в дневное время, тем самым обеспечивая обзор в окнах. В вечернее время получается полноценный экран. Удобно. Насколько эта технология применима в условиях русской зимы, остается только догадываться, будем тестировать. Сетки с большим шагом были представлены только в виде отдельных модулей, что, собственно, логично, так как оценить качество работы в пределах помещения невозможно. Слева – реализованый моторизованный экран Gtek. Справа – арендная сетка с шагом 12мм. Я точно знаю, что уже в этом году спрос на статичные сетки будет сильно расти, поэтому было очень интересно посмотреть на все новинки вживую. Ну и напоследок про перспективы всего этого. Вот, например: в Китае очень много светодиодов на улицах, и один из самых ярких объектов ночью – телебашня в Гуанчжоу: Возможно, у нас тоже так будет довольно скоро.\n","id":99}
{"Host":"https://habr.com","Path":"/ru/post/270081/?mobile=yes","Text":"Где находиться типу: справа или слева? / Хабр                 \n\n05-11-2015\nКак-то увидев очередную статью на Хабре, посвященную для меня совершенно новому и неизведанному языку Go, решил попробовать, что это за зверь и с чем его едят (В основном, конечно, понравился логотип) . Конечно, язык имеет много возможностей и достаточно удобен. Но что меня сразу удивило, это отличный от C-подобных языков принцип объявления переменных, а именно тип переменных описывается справа от имени переменной. У меня как человека, практически выросшего на С, это вызывало удивление. Потом я конечно вспомнил Pascal, что там тоже тип переменной был справа. Заинтересовавшись этим вопросом, я попытался разобраться, почему используется тот или иной синтаксис описания типа переменных в этих 2-х языках. Начнем с описания синтаксиса объявления переменных в С-подобных языках. В С было решено отказаться от отдельного синтаксиса описания переменных и позволить объявлять переменные как выражения: int x; Как мы видим, тип переменной стоит слева, затем имя переменной. Благодаря этому мы максимально приближаем объявление переменной к обычному выражению. Допустим, к такому: int x = 5; Или такому: int x = y*z; В принципе, все просто и понятно, и вполне логично, посмотрим на определение функций в C. Изначально в C использовался вот такой синтаксис определения функции: int main(argc, argv) int argc; char *argv[]; { /* ... */ } Типы переменных описывались не вместе с именами аргументов, но потом синтаксис заменили на другой: int main(int argc, char *argv[]) { /* ... */ } Здесь все тоже достаточно просто и понятно. Но это удобство начинает испаряться, когда в дело вступают указатели на функции и функции, которые могут принимать указатели на них. int (*fp)(int a, int b); Здесь fp — ссылка на функцию, принимающую 2 аргумента и возвращающая int. В принципе, не сложно, но вот что будет если одним из аргументов будет ссылка на функцию: int (*fp)(int (*ff)(int x, int y), int b) Уже как-то сложновато или вот такой пример: int (*(*fp)(int (*)(int, int), int))(int, int) В нем, если честно, я заблудился. Как видно из описания, при декларировании указателей на функции в языках С есть существенный недостаток в читаемости кода. Теперь посмотрим, какой метод предлагает использовать для чтения определения переменных в С Дэвид Андерсон(David Anderson). Чтение происходит по методу Clockwise/Spiral Rule (часовой стрелке/спирали). Данный метод имеет 3 правила: Чтение начинается с неизвестного элемента движением по спирали; Обработка выражения по спирали продолжается пока не покроются все символы; Чтение необходимо начинать с круглых скобок. Пример 1: Следуя правилу, начинаем с неизвестной str: Двигаемся по спирали и первое, что мы видим, это символ ‘[’. Значит, мы имеем дело с массивом — str массив 10-и элементов; Продолжаем движение по спирали и следующий символ это '*'. Значит, это указатель — str массив 10-и указателей; Продолжая движения по спирали приходим к символу ';', означающий конец выражения. Двигаемся по спирали и находим тип данных char — str массив 10-и указателей на тип char. Возьмем пример посложнее Пример 2: Первая неизвестная, которая нам встречается, это signal. Начинаем движение по спирали от нее и видим скобку. Это означает, что: — signal – это функция которая принимает int и… Здесь мы видим вторую неизвестную и пытаемся проанализировать ее. По тому же правилу двигаемся от нее по спирали и видим, что это указатель. — fp указатель на … Продолжаем движение и видим символ ‘(’. Значит: — fp указатель на функцию, принимающую int и возвращающую… Идем по спирали и видим 'void'. Из этого следует, что: — fp указатель на функцию, принимающую int и ничего не возвращающую; Анализ fp закончен и мы возвращаемся к signal — signal – это функция, которая принимает int и указатель на функцию, принимающую int и ничего не возвращающую; Продолжая движение видим символ ‘*’, что означает — signal – это функция, которая принимает int и указатель на функцию, принимающую int и ничего не возвращающую, и возвращает указатель на… Идем по спирали и мы видим ‘(’, что означает — signal – это функция, которая принимает int и указатель на функцию, принимающую int и ничего не возвращающую, и возвращает указатель на функцию, принимающую int… Делаем последний виток и получаем следующее — signal – это функция, которая принимает int и указатель на функцию, принимающую int и ничего не возвращающую, и возвращает указатель на функцию, принимающую int и возвращающую void. Вот так, без особых усилий, предлагает нам читать определение переменных Дэвид Андерсон. Теперь рассмотрим диаметрально противоположный синтаксис, когда тип переменной находится справа от имени переменной, на примере Go. В Go переменные читаются слева направо и выглядят вот так: var x int var p *int var a [3]int Здесь не нужно применять никаких спиральных методов, читается просто — переменная a — это массив, состоящий из 3-х элементов типа int. С функциями тоже все достаточно просто: func main(argc int, argv []string) int И данное объявление тоже читается с легкостью слева направо. Даже сложные функции, принимающие другие функции, вполне читаются слева направо: f func(func(int,int) int, int) int f — функция, принимающая функцию, которая, в свою очередь, принимает в параметрах 2 целых числа и возвращает целое число, и целое число, и возвращающая целое число. Вот такие имеет отличия определение переменных в языках семейства C и Go. Очевидно, Go явно в этом выигрывает. Но если теперь вспомнить, какие языки выросли из старого доброго С – это С++, C#, Java — все они используют определение переменных такого типа. И они построены на парадигмах ООП и не используют (или практически не используют) передачу указателей на функции, все это нам заменили классы. Недостатки, которые выявляются у определения типа переменной слева, улетучиваются при использовании ООП.\n","id":100}
