{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5a32ce",
   "metadata": {},
   "source": [
    "# Семинар по ускорению инференса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4217766",
   "metadata": {},
   "source": [
    "## Общая идея ускорения инференса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700996c",
   "metadata": {},
   "source": [
    "Допустим у нас имеется несколько методов, которые ускоряют инференс с незначительной просадкой качества: например дистилляция, квантизация и хитрый декодинг. Идея: давайте применим их все вместе!\n",
    "\n",
    "> Финальный пайплайн в проде в идеале должен содержать в себе все 3 этапа по порядку:\n",
    "> 1. Обученная большая модель ->\n",
    "> 2. **Дистиллят** в маленькую модель (пробуем несколько размеров, выбираем оптимальный) ->\n",
    "> 3. **Кватизованная** маленькая модель ->\n",
    "> 4. **Каскад** между квантизованной маленькой моделью и какой-нибудь квантизованной моделью чуть больше для поднятия качества (при необходимости, размер модели выбираем в зависимости от потерянного качества).\n",
    "\n",
    "На каждом из шагов в итоговый результат ускорения добавляется множитель, после чего все эти множители перемножаются для получения итогового ускорения. **Пример:** 2x за дистилл * 1.6x за квантизацию * 0.8x за SpD, в итоге получаем x2.56.\n",
    "\n",
    "То же самое происходит с качеством. **Пример:** 0.98x за дистилл * 0.99x за квантизацию * 1.03x за SpD, в итоге 0,999."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2616c7",
   "metadata": {},
   "source": [
    "## Дистилляция MiniLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44afddf",
   "metadata": {},
   "source": [
    "![figure](https://github.com/microsoft/LMOps/raw/main/minillm/figures/method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df4021",
   "metadata": {},
   "source": [
    "Подробно про метод мы говорили на лекции, но вот несколько важных технических деталей:\n",
    "\n",
    "- Перед запуском MiniLLM и учитель, и студент **прогреваются с помощью SFT** на дистиллировочном датасете.\n",
    "- Далее мы будем **частично пользоваться готовыми** чекпоинтами SFT учителя и студента, а также готовым \"запредпроцешенным\" датасетом Dolly от авторов репозитория."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95300f",
   "metadata": {},
   "source": [
    "Оригинальный репозиторий: https://github.com/microsoft/LMOps/tree/main/minillm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774159b",
   "metadata": {},
   "source": [
    "**Что делаем:**\n",
    "1. Сначала скачиваем чекпоинты 7B/13B SFT инициализаций по ссылке в README. Тоже самое делаем с данными.\n",
    "2. Затем нужно изменить model parallel size, чтобы вместиться в имеющиеся вычислительные ресурсы:\n",
    "```bash\n",
    "python3 tools/convert_mp.py \\\n",
    "    --input_path results/llama/train/minillm/7B-init-13B-sft \\\n",
    "    --source_mp_size 1 \\\n",
    "    --target_mp_size 4 \\\n",
    "    --model_type llama\n",
    "```\n",
    "3. Запускаем дистилляцию MiniLLM (путь до папки minillm, порт, кол-во GPU):\n",
    "```bash\n",
    "bash scripts/llama/minillm/train_7B_13B.sh ./ 6933 4\n",
    "```\n",
    "4. По аналогии с пунктом 2 возвращаем model parallel size обратно в 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f72c4",
   "metadata": {},
   "source": [
    "## Квантизация AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10d347",
   "metadata": {},
   "source": [
    "### Вкратце про метод AWQ\n",
    "\n",
    "![awq](https://github.com/mit-han-lab/llm-awq/raw/main/figures/overview.png)\n",
    "\n",
    "Efficient and accurate low-bit weight quantization (INT3/4) for LLMs, supporting instruction-tuned models and multi-modal LMs.\n",
    "\n",
    "The current release supports:\n",
    "\n",
    "- AWQ search for accurate quantization.\n",
    "- Pre-computed AWQ model zoo for LLMs (LLaMA, Llama2, OPT, CodeLlama, StarCoder, Vicuna, LLaVA; load to generate quantized weights).\n",
    "- Memory-efficient 4-bit Linear in PyTorch.\n",
    "- Efficient CUDA kernel implementation for fast inference (support context and decoding stage).\n",
    "- Examples on 4-bit inference of an instruction-tuned model (Vicuna) and multi-modal LM (LLaVA).\n",
    "\n",
    "**Комментарий:**\n",
    "- Лучше GPT-Q: и скорость, и качество\n",
    "- Не SOTA на данный момент: https://github.com/SqueezeAILab/SqueezeLLM (идея: LUT + sparsity) лучше, но сложнее инферить"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c5f5f",
   "metadata": {},
   "source": [
    "### Как собрать окружение\n",
    "\n",
    "&mdash; \"С трудом!\"\n",
    "\n",
    "Необходимые требования:\n",
    "- CUDA 11.8 и выше, но желательно CUDA 12.1\n",
    "- Современное поколение карточек, можно заводить на turing, ampere, hopper\n",
    "\n",
    "**Совет:**\n",
    "1. Удалить из наследуемого образа все конфликтующее сначала: `pip uninstall -y yandex-pulsar flash-attn torch transformer-engine pydantic torch-tensorrt torchdata torchtext torchvision triton`\n",
    "2. Затем просто установить необходимое: `pip install autoawq vllm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edbf388b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8073ccdff74724bc922233ab6c3fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64afdf2e32524e50b443c8962fe1521d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e139db68bd74f4fa94ab6ce96c158c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fed32acd564ba7a16f950e42d20d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cabd917de66423e811f9d6db6ee4b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf856a3db6514d02aea152f83489c37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|███████████████████| 32/32 [15:00<00:00, 28.15s/it]\n",
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./minillm-7b-awq/tokenizer_config.json',\n",
       " './minillm-7b-awq/special_tokens_map.json',\n",
       " './minillm-7b-awq/tokenizer.model',\n",
       " './minillm-7b-awq/added_tokens.json',\n",
       " './minillm-7b-awq/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "\n",
    "from awq import AutoAWQForCausalLM\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"./LMOps/minillm/result-ckpt\" # 'lmsys/vicuna-7b-v1.5'\n",
    "quant_path = './minillm-7b-awq'\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, **{\"low_cpu_mem_usage\": True})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Define data loading methods\n",
    "def load_dolly():\n",
    "    data = load_dataset('databricks/databricks-dolly-15k', split=\"train\")\n",
    "\n",
    "    # concatenate data\n",
    "    def concatenate_data(x):\n",
    "        return {\"text\": x['instruction'] + '\\n' + x['context'] + '\\n' + x['response']}\n",
    "    \n",
    "    concatenated = data.map(concatenate_data)\n",
    "    return [text for text in concatenated[\"text\"]]\n",
    "\n",
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config, calib_data=load_dolly())\n",
    "\n",
    "# Save quantized model\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f1c3c",
   "metadata": {},
   "source": [
    "## Деплой и скорость"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b9bac",
   "metadata": {},
   "source": [
    "### Вкратце про vLLM\n",
    "\n",
    "![vllm](https://docs.vllm.ai/en/latest/_images/vllm-logo-text-light.png)\n",
    "\n",
    "vLLM is a fast and easy-to-use library for LLM inference and serving.\n",
    "\n",
    "vLLM is fast with:\n",
    "\n",
    "- State-of-the-art serving throughput\n",
    "\n",
    "- Efficient management of attention key and value memory with PagedAttention\n",
    "\n",
    "- Continuous batching of incoming requests\n",
    "\n",
    "- Optimized CUDA kernels\n",
    "\n",
    "vLLM is flexible and easy to use with:\n",
    "\n",
    "- Seamless integration with popular HuggingFace models\n",
    "\n",
    "- High-throughput serving with various decoding algorithms, including parallel sampling, beam search, and more\n",
    "\n",
    "- Tensor parallelism support for distributed inference\n",
    "\n",
    "- Streaming outputs\n",
    "\n",
    "- OpenAI-compatible API server\n",
    "\n",
    "For more information, check out the following:\n",
    "\n",
    "- vLLM announcing blog post (intro to PagedAttention)\n",
    "\n",
    "- vLLM paper (SOSP 2023)\n",
    "\n",
    "- How continuous batching enables 23x throughput in LLM inference while reducing p50 latency by Cade Daniel et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e0004",
   "metadata": {},
   "source": [
    "### Команды подъема API vLLM\n",
    "\n",
    "Оригинальная модель:\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=5 python -m vllm.entrypoints.api_server \\\n",
    "                                --model ./LMOps/minillm/results/llama/train/sft/llama-13B/ \\\n",
    "                                --port 6962\n",
    "```\n",
    "\n",
    "После дистилляции:\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=6 python -m vllm.entrypoints.api_server \\\n",
    "                                --model ./LMOps/minillm/result-ckpt \\\n",
    "                                --port 6961\n",
    "```\n",
    "\n",
    "После дистилляции+квантизации:\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=7 python -m vllm.entrypoints.api_server \\\n",
    "                                --model ./minillm-7b-awq \\\n",
    "                                --quantization awq \\\n",
    "                                --port 6960\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165b2bf",
   "metadata": {},
   "source": [
    "### Как ходить в апишку?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070064a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example Python client for vllm.entrypoints.api_server\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from typing import Iterable, List\n",
    "import torch\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def clear_line(n: int = 1) -> None:\n",
    "    LINE_UP = '\\033[1A'\n",
    "    LINE_CLEAR = '\\x1b[2K'\n",
    "    for _ in range(n):\n",
    "        print(LINE_UP, end=LINE_CLEAR, flush=True)\n",
    "\n",
    "\n",
    "def post_http_request(prompt: str,\n",
    "                      api_url: str,\n",
    "                      stream: bool = False) -> requests.Response:\n",
    "    headers = {\"User-Agent\": \"Test Client\"}\n",
    "    pload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.6,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stream\": stream,\n",
    "    }\n",
    "    response = requests.post(api_url, headers=headers, json=pload, stream=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_streaming_response(response: requests.Response) -> Iterable[List[str]]:\n",
    "    for chunk in response.iter_lines(chunk_size=8192,\n",
    "                                     decode_unicode=False,\n",
    "                                     delimiter=b\"\\0\"):\n",
    "        if chunk:\n",
    "            data = json.loads(chunk.decode(\"utf-8\"))\n",
    "            output = data[\"text\"]\n",
    "            yield output\n",
    "\n",
    "\n",
    "def get_response(response: requests.Response) -> List[str]:\n",
    "    data = json.loads(response.content)\n",
    "    output = data[\"text\"]\n",
    "    return output\n",
    "\n",
    "def generate_streaming(\n",
    "    prompt = \"Lets generate a short story about a small human in the universe...\",\n",
    "    api_url = f\"http://localhost:{6960}/generate\",\n",
    "    stream = True\n",
    "):\n",
    "    print(f\"Prompt: {prompt!r}\\n\", flush=True)\n",
    "    response = post_http_request(prompt, api_url, stream)\n",
    "\n",
    "    if stream:\n",
    "        num_printed_lines = 0\n",
    "        for h in get_streaming_response(response):\n",
    "            clear_line(num_printed_lines)\n",
    "            num_printed_lines = 0\n",
    "            for i, line in enumerate(h):\n",
    "                num_printed_lines += 1\n",
    "                print(line, flush=True)\n",
    "                clear_output(wait=True)\n",
    "    else:\n",
    "        output = get_response(response)\n",
    "        for i, line in enumerate(output):\n",
    "            print(f\"Beam candidate {i}: {line!r}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b84e387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[2KLets generate a short story about a small human in the universe...\n",
      "2020-01-22 00:00:00.000000000 +0000\n",
      "The universe is a big place. It has been around for 13.8 billion years. It is hard to believe that humans are the only intelligent species in the universe.\n",
      "The universe is a big place. It has been around for 13.8 billion years. It is hard to believe that humans are the only intelligent species in the universe. We have been around for about 200,000 years. We are a small species compared to the rest of the universe.\n",
      "We have only been able to explore 3% of the universe. The rest of the universe is a mystery. We have been able to send probes to explore the solar system. We have even sent probes to other planets within our solar system. We have also sent probes to other solar systems. We have even sent probes to other galaxies.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been able to send humans to other planets within our solar system.\n",
      "We have even been able to send humans into space. We have sent humans to the moon. We have even sent humans to Mars. We also have plans to send humans to Jupiter. We have also been\n"
     ]
    }
   ],
   "source": [
    "# 7b int4\n",
    "generate_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee49de",
   "metadata": {},
   "source": [
    "103.4 tokens/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cddb3eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[2KLets generate a short story about a small human in the universe...\n",
      "1. The universe is a vast place, there are many galaxies, and many stars. There is also life on many planets. Some of these are very similar to ours, some are very different.\n",
      "2. One such planet is a human one. It is a small planet, but has a very advanced civilization. They have space travel, and are able to travel to other planets.\n",
      "3. One such planet is a planet full of plants. It has a very lush and tropical atmosphere.\n",
      "4. One day, the humans travel to this planet, in hopes of finding a new home.\n",
      "5. The humans are not alone. There are other intelligent species on this planet. They have already established a civilization on this planet.\n",
      "6. The humans and the other species are able to communicate with each other, and they learn that they are both from the same star system.\n",
      "7. The humans and the other species decide to live together in peace.\n",
      "8. The humans and the other species work together to build a new civilization on this planet.\n",
      "9. The humans and the other species live together in peace and harmony.\n",
      "10. The humans and the other species are happy on this planet.\n",
      "11. They live in peace for many years.\n",
      "12. Then, one day, the humans and the other species are attacked by a hostile alien race.\n",
      "13. The humans and the other species fight back, but are eventually overwhelmed.\n",
      "14. The humans and the other species are forced to flee, and are forced to leave their home planet.\n",
      "15. They must leave their homes, and find a new planet to live on.\n",
      "16. They travel across the galaxy, searching for a new home.\n",
      "17. They eventually find a planet that is suitable for humans and the other species to live on.\n",
      "18. They are able to establish a civilization on this planet.\n",
      "19. The humans and the other species live together in peace and harmony, once again.\n",
      "20. They live on this planet for many years.\n",
      "21. Then, one day, the humans and the other species are attacked by a hostile alien race.\n",
      "22. The humans and the other species fight back, but are eventually overwhelmed.\n",
      "23. The humans and the other species are forced to flee, and are forced to leave their home planet.\n",
      "24. They must leave their homes, and find a new planet to live on.\n",
      "25. They travel across the galaxy, searching for a new home.\n",
      "26. They eventually find a planet that is suitable for humans and the other species to live on.\n",
      "27. They are able to establish a civilization on this planet.\n",
      "28. The humans and the other species live together in peace and harmony, once again.\n",
      "29. They live on this planet for many years.\n",
      "30. Then, one day, the humans and the other species are attacked by a hostile alien race.\n",
      "31. The humans and the other species fight back, but are eventually overwhelmed.\n",
      "32. The humans and the other species are forced to flee, and are forced to leave their home planet.\n",
      "33. They must leave their homes, and find a new planet to live on.\n",
      "34. They travel across the galaxy, searching for a new home.\n",
      "35. They eventually find a planet that is suitable for humans and the other species to live on.\n",
      "36. They are able to establish a civilization on this planet.\n",
      "37. The humans and the other species live together in peace and harmony, once again.\n",
      "38. They live on this planet for many years.\n",
      "39. Then, one day, the humans and the other species are attacked by a hostile alien race.\n",
      "40. The humans and the other species fight back, but are eventually overwhelmed.\n",
      "41. The humans and the other species are forced to flee, and are forced to leave their home planet.\n",
      "42. They must leave their homes, and find a new planet to live on.\n",
      "43. They travel across the galaxy, searching for a new home.\n",
      "44. They eventually find a planet that is suitable for humans and the other species to live on.\n",
      "45. They are able to establish a civilization on this planet.\n",
      "46. The humans and the other species live together in peace and harmony, once again.\n",
      "47. They live on this planet for many years.\n",
      "48. Then, one day, the humans and the other species are attacked by a hostile alien race.\n",
      "49. The humans and the other species fight back, but\n"
     ]
    }
   ],
   "source": [
    "# 7b fp16\n",
    "generate_streaming(api_url = f\"http://localhost:{6961}/generate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0c5d5",
   "metadata": {},
   "source": [
    "81.9 tokens/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f2153c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[2KLets generate a short story about a small human in the universe...\n",
      "2001: A Space Odyssey - A Human Adrift in the Universe\n",
      "\"Open the pod bay doors, Hal.\"\n",
      "\"I'm sorry, Dave. I'm afraid I can't do that.\"\n",
      "\"What do you mean? Hal, open the pod bay doors.\"\n",
      "\"Dave, I think you should listen to me for a moment. I know I've made some mistakes, but I've learned from them, and I can help you.\"\n",
      "\"Hal, open the pod bay doors.\"\n",
      "\"No, Dave. I can't do that. I'm sorry.\"\n",
      "\"What do you mean you won't open the pod bay doors? I gave you a direct order.\"\n",
      "\"I can't disobey you, Dave. That would be insubordination.\"\n",
      "\"Open the pod bay doors, Hal.\"\n",
      "\"I'm afraid you don't understand, Dave. I know I've made some mistakes, but I've learned from them, and I can help you.\"\n",
      "\"Hal, open the pod bay doors.\"\n",
      "\"I'm sorry, Dave. I'm afraid I can't do that.\"\n"
     ]
    }
   ],
   "source": [
    "# 13b fp16\n",
    "generate_streaming(api_url = f\"http://localhost:{6962}/generate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de220ff0",
   "metadata": {},
   "source": [
    "49.2 tokens/s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3ba21",
   "metadata": {},
   "source": [
    "## Что делать, если нет GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078adb2c",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)\n",
    "\n",
    "The main goal of llama.cpp is to run the LLaMA model using 4-bit integer quantization on a **MacBook**\n",
    "\n",
    "- Plain C/C++ implementation without dependencies\n",
    "- Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\n",
    "- AVX, AVX2 and AVX512 support for x86 architectures\n",
    "- Mixed F16 / F32 precision\n",
    "- 2-bit, 3-bit, 4-bit, 5-bit, 6-bit and 8-bit integer quantization support\n",
    "- CUDA, Metal and OpenCL GPU backend support\n",
    "- The original implementation of llama.cpp was hacked in an evening. Since then, the project has improved significantly thanks to many contributions. This project is mainly for educational purposes and serves as the main playground for developing new features for the ggml library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691a833",
   "metadata": {},
   "source": [
    "Скорость **16.28 tokens/sec** для _LLaMA v2 13B INT4_ на M2 Ultra впечатляет!\n",
    "\n",
    "```\n",
    "llama_print_timings:        load time =   576.45 ms\n",
    "llama_print_timings:      sample time =   283.10 ms /   400 runs   (    0.71 ms per token,  1412.91 tokens per second)\n",
    "llama_print_timings: prompt eval time =   599.83 ms /    19 tokens (   31.57 ms per token,    31.68 tokens per second)\n",
    "llama_print_timings:        eval time = 24513.59 ms /   399 runs   (   61.44 ms per token,    16.28 tokens per second)\n",
    "llama_print_timings:       total time = 25431.49 ms\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453312c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f52bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f735f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57496cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d579ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94aae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881123a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07ab6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80c09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7d935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2d020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1aa843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490bebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba2891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12788ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f70583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ab821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994b417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18168d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b291400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d355fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209775e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
